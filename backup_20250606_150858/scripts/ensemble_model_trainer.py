#!/usr/bin/env python3
"""
Treinamento de Modelo Ensemble para Previs√£o de 4 Dias
Projeto: Sistema de Alertas de Cheias - Rio Gua√≠ba

Este script implementa a estrat√©gia de ensemble multi-escala:
1. Modelo Meteorol√≥gico B√°sico (LSTM + Attention)
2. Modelo Sin√≥tico-Din√¢mico (Transformer + CNN)
3. Modelo de Teleconex√µes (Graph Neural Network)
4. Meta-Modelo Ensemble
"""

import json
import logging
import warnings
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import joblib
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import TimeSeriesSplit
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from tensorflow import keras
from tensorflow.keras import layers

warnings.filterwarnings("ignore")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class EnsembleForecastTrainer:
    """Treinador de modelo ensemble para previs√£o de 4 dias"""

    def __init__(self):
        self.models_dir = Path("models/ensemble")
        self.models_dir.mkdir(parents=True, exist_ok=True)
        self.results_dir = Path("results/training")
        self.results_dir.mkdir(parents=True, exist_ok=True)

        # Configura√ß√µes do modelo
        self.sequence_length = 24  # 24 horas de hist√≥rico
        self.forecast_horizon = 96  # 96 horas (4 dias)
        self.ensemble_size = 3

        # Scalers para diferentes tipos de features
        self.scalers = {
            "meteorological": StandardScaler(),
            "synoptic": StandardScaler(),
            "teleconnections": MinMaxScaler(),
        }

    def prepare_training_data(self, data_file: str = None) -> Dict[str, np.ndarray]:
        """
        Prepara dados de treinamento simulados ou carrega dados reais
        """
        logger.info("Preparando dados de treinamento...")

        if data_file and Path(data_file).exists():
            # Carrega dados reais se dispon√≠vel
            return self._load_real_data(data_file)
        else:
            # Simula dados de treinamento
            return self._simulate_training_data()

    def _simulate_training_data(self) -> Dict[str, np.ndarray]:
        """Simula dados de treinamento realistas"""
        np.random.seed(42)

        # Simula 2 anos de dados hor√°rios
        n_hours = 24 * 365 * 2
        dates = pd.date_range(start="2022-01-01", periods=n_hours, freq="H")

        # Features meteorol√≥gicas b√°sicas
        meteorological_features = self._generate_meteorological_features(n_hours, dates)

        # Features sin√≥ticas
        synoptic_features = self._generate_synoptic_features(n_hours, dates)

        # Features de teleconex√µes
        teleconnection_features = self._generate_teleconnection_features(n_hours, dates)

        # Target: n√≠vel do rio (simplificado)
        river_level = self._generate_river_level_target(
            meteorological_features, synoptic_features
        )

        return {
            "meteorological": meteorological_features,
            "synoptic": synoptic_features,
            "teleconnections": teleconnection_features,
            "target": river_level,
            "dates": dates,
        }

    def _generate_meteorological_features(
        self, n_hours: int, dates: pd.DatetimeIndex
    ) -> np.ndarray:
        """Gera features meteorol√≥gicas realistas"""
        # Sazonalidade e tend√™ncias
        day_of_year = dates.dayofyear
        hour_of_day = dates.hour

        # Temperatura com sazonalidade
        temp_seasonal = 20 + 10 * np.sin(2 * np.pi * day_of_year / 365.25)
        temp_daily = 5 * np.sin(2 * np.pi * hour_of_day / 24)
        temperature = temp_seasonal + temp_daily + np.random.normal(0, 2, n_hours)

        # Precipita√ß√£o com eventos extremos
        precipitation = np.random.exponential(0.5, n_hours)
        # Adiciona eventos de chuva intensa ocasionais
        extreme_events = np.random.choice(
            n_hours, size=int(n_hours * 0.02), replace=False
        )
        precipitation[extreme_events] *= np.random.exponential(10, len(extreme_events))

        # Press√£o atmosf√©rica
        pressure = (
            1013
            + 10 * np.sin(2 * np.pi * day_of_year / 365.25)
            + np.random.normal(0, 5, n_hours)
        )

        # Vento
        wind_speed = (
            5
            + 3 * np.sin(2 * np.pi * day_of_year / 365.25)
            + np.random.exponential(2, n_hours)
        )
        wind_direction = np.random.uniform(0, 360, n_hours)

        # Umidade
        humidity = (
            60
            + 20 * np.sin(2 * np.pi * day_of_year / 365.25)
            + np.random.normal(0, 10, n_hours)
        )
        humidity = np.clip(humidity, 0, 100)

        return np.column_stack(
            [temperature, precipitation, pressure, wind_speed, wind_direction, humidity]
        )

    def _generate_synoptic_features(
        self, n_hours: int, dates: pd.DatetimeIndex
    ) -> np.ndarray:
        """Gera features sin√≥ticas (frentes, sistemas de press√£o)"""
        # √çndices de frentes frias (mais frequentes no inverno)
        day_of_year = dates.dayofyear
        winter_intensity = 0.5 + 0.5 * np.cos(
            2 * np.pi * (day_of_year - 172) / 365.25
        )  # Pico no inverno

        front_intensity = np.random.exponential(winter_intensity, n_hours)
        front_speed = 20 + np.random.normal(0, 10, n_hours)

        # Sistemas de alta/baixa press√£o
        pressure_system_type = np.random.choice(
            [-1, 0, 1], n_hours, p=[0.3, 0.4, 0.3]
        )  # -1: baixa, 0: neutro, 1: alta
        system_intensity = np.random.exponential(1, n_hours)

        # Cisalhamento do vento
        wind_shear = np.random.exponential(5, n_hours)

        # CAPE (energia convectiva)
        cape = np.random.exponential(500, n_hours)
        cape[dates.month.isin([12, 1, 2, 6, 7, 8])] *= 2  # Maior no ver√£o

        return np.column_stack(
            [
                front_intensity,
                front_speed,
                pressure_system_type,
                system_intensity,
                wind_shear,
                cape,
            ]
        )

    def _generate_teleconnection_features(
        self, n_hours: int, dates: pd.DatetimeIndex
    ) -> np.ndarray:
        """Gera features de teleconex√µes (El Ni√±o, oscila√ß√µes)"""
        # El Ni√±o/La Ni√±a (varia lentamente)
        enso_index = np.sin(
            2 * np.pi * dates.dayofyear / (365.25 * 3)
        ) + np.random.normal(0, 0.1, n_hours)

        # Oscila√ß√£o do Atl√¢ntico Sul
        sao_index = np.cos(
            2 * np.pi * dates.dayofyear / (365.25 * 2)
        ) + np.random.normal(0, 0.2, n_hours)

        # Temperatura da superf√≠cie do mar
        sst_anomaly = 0.5 * enso_index + np.random.normal(0, 0.3, n_hours)

        # √çndices de dipolo
        dipole_index = np.random.normal(0, 0.5, n_hours)

        return np.column_stack([enso_index, sao_index, sst_anomaly, dipole_index])

    def _generate_river_level_target(
        self, met_features: np.ndarray, syn_features: np.ndarray
    ) -> np.ndarray:
        """Gera n√≠vel do rio baseado nas features meteorol√≥gicas e sin√≥ticas"""
        n_hours = len(met_features)

        # N√≠vel base do rio
        base_level = 2.0

        # Contribui√ß√£o da precipita√ß√£o (com delay)
        precipitation = met_features[:, 1]
        runoff = np.convolve(
            precipitation, np.exp(-np.arange(24) / 6), mode="same"
        )  # Decay exponencial

        # Contribui√ß√£o de frentes frias
        front_contribution = syn_features[:, 0] * 0.5

        # Efeito de mar√© e sazonalidade
        seasonal_effect = 0.3 * np.sin(2 * np.pi * np.arange(n_hours) / (24 * 365.25))
        tidal_effect = 0.1 * np.sin(
            2 * np.pi * np.arange(n_hours) / 12.42
        )  # Mar√© semi-diurna

        # Ru√≠do
        noise = np.random.normal(0, 0.1, n_hours)

        river_level = (
            base_level
            + runoff * 0.01
            + front_contribution
            + seasonal_effect
            + tidal_effect
            + noise
        )

        return np.maximum(river_level, 0.5)  # N√≠vel m√≠nimo

    def create_sequences(
        self, data: Dict[str, np.ndarray]
    ) -> Tuple[Dict[str, np.ndarray], np.ndarray]:
        """Cria sequ√™ncias de entrada e sa√≠da para o modelo"""
        logger.info("Criando sequ√™ncias de treinamento...")

        n_samples = (
            len(data["target"]) - self.sequence_length - self.forecast_horizon + 1
        )

        # Preparar X (entradas)
        X = {}
        for feature_type in ["meteorological", "synoptic", "teleconnections"]:
            features = data[feature_type]
            # Normalizar
            features_scaled = self.scalers[feature_type].fit_transform(features)

            # Criar sequ√™ncias
            X[feature_type] = np.array(
                [
                    features_scaled[i : i + self.sequence_length]
                    for i in range(n_samples)
                ]
            )

        # Preparar y (sa√≠das) - m√∫ltiplos horizontes
        y = np.array(
            [
                data["target"][
                    i
                    + self.sequence_length : i
                    + self.sequence_length
                    + self.forecast_horizon
                ]
                for i in range(n_samples)
            ]
        )

        return X, y

    def build_meteorological_model(self, input_shapes: Dict[str, Tuple]) -> keras.Model:
        """
        Modelo 1: LSTM + Attention para dados meteorol√≥gicos b√°sicos
        Foco: 1-2 dias
        """
        logger.info("Construindo modelo meteorol√≥gico (LSTM + Attention)...")

        # Input para features meteorol√≥gicas
        met_input = keras.Input(
            shape=input_shapes["meteorological"], name="meteorological_input"
        )

        # LSTM layers com dropout
        lstm1 = layers.LSTM(128, return_sequences=True, dropout=0.2)(met_input)
        lstm2 = layers.LSTM(64, return_sequences=True, dropout=0.2)(lstm1)

        # Attention mechanism
        attention = layers.MultiHeadAttention(num_heads=4, key_dim=64)(lstm2, lstm2)
        attention = layers.LayerNormalization()(attention + lstm2)

        # Global pooling
        pooled = layers.GlobalAveragePooling1D()(attention)

        # Dense layers
        dense1 = layers.Dense(128, activation="relu")(pooled)
        dense1 = layers.Dropout(0.3)(dense1)
        dense2 = layers.Dense(64, activation="relu")(dense1)

        # Output para m√∫ltiplos horizontes
        output = layers.Dense(
            self.forecast_horizon, activation="linear", name="meteorological_output"
        )(dense2)

        model = keras.Model(
            inputs=met_input, outputs=output, name="meteorological_model"
        )
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss="mse",
            metrics=["mae"],
        )

        return model

    def build_synoptic_model(self, input_shapes: Dict[str, Tuple]) -> keras.Model:
        """
        Modelo 2: Transformer + CNN para an√°lise sin√≥tica
        Foco: 2-4 dias
        """
        logger.info("Construindo modelo sin√≥tico (Transformer + CNN)...")

        # Inputs
        met_input = keras.Input(shape=input_shapes["meteorological"], name="met_input")
        syn_input = keras.Input(shape=input_shapes["synoptic"], name="syn_input")

        # CNN para features meteorol√≥gicas (padr√µes locais)
        conv1 = layers.Conv1D(64, 3, activation="relu", padding="same")(met_input)
        conv2 = layers.Conv1D(32, 3, activation="relu", padding="same")(conv1)
        pool = layers.MaxPooling1D(2)(conv2)

        # Transformer para features sin√≥ticas (padr√µes de longo prazo)
        # Positional encoding
        positions = tf.range(start=0, limit=input_shapes["synoptic"][0], delta=1)
        positions = tf.cast(positions, tf.float32)
        pos_encoding = layers.Dense(input_shapes["synoptic"][1])(
            positions[:, tf.newaxis]
        )
        syn_with_pos = syn_input + pos_encoding

        # Multi-head attention
        attention = layers.MultiHeadAttention(num_heads=8, key_dim=64)(
            syn_with_pos, syn_with_pos
        )
        attention = layers.LayerNormalization()(attention + syn_with_pos)

        # Feed forward
        ff = layers.Dense(128, activation="relu")(attention)
        ff = layers.Dense(input_shapes["synoptic"][1])(ff)
        transformer_out = layers.LayerNormalization()(ff + attention)

        # Combine CNN and Transformer outputs
        cnn_flat = layers.GlobalAveragePooling1D()(pool)
        transformer_flat = layers.GlobalAveragePooling1D()(transformer_out)
        combined = layers.Concatenate()([cnn_flat, transformer_flat])

        # Dense layers
        dense1 = layers.Dense(256, activation="relu")(combined)
        dense1 = layers.Dropout(0.3)(dense1)
        dense2 = layers.Dense(128, activation="relu")(dense1)
        dense2 = layers.Dropout(0.2)(dense2)

        # Output
        output = layers.Dense(
            self.forecast_horizon, activation="linear", name="synoptic_output"
        )(dense2)

        model = keras.Model(
            inputs=[met_input, syn_input], outputs=output, name="synoptic_model"
        )
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0005),
            loss="mse",
            metrics=["mae"],
        )

        return model

    def build_teleconnection_model(self, input_shapes: Dict[str, Tuple]) -> keras.Model:
        """
        Modelo 3: Graph Neural Network para teleconex√µes
        Foco: 3-7 dias
        """
        logger.info("Construindo modelo de teleconex√µes (GNN simplificado)...")

        # Simplifica√ß√£o: usar Dense layers para simular GNN
        tel_input = keras.Input(
            shape=input_shapes["teleconnections"], name="teleconnection_input"
        )

        # "Graph" processing (simulado com Dense layers)
        # Na implementa√ß√£o real, usaria bibliotecas como DGL ou PyTorch Geometric

        # Temporal processing
        lstm = layers.LSTM(32, return_sequences=True)(tel_input)

        # "Node" processing
        node1 = layers.Dense(64, activation="tanh")(lstm)
        node2 = layers.Dense(32, activation="tanh")(node1)

        # Global features
        global_pool = layers.GlobalAveragePooling1D()(node2)

        # Dense layers
        dense1 = layers.Dense(64, activation="relu")(global_pool)
        dense1 = layers.Dropout(0.2)(dense1)
        dense2 = layers.Dense(32, activation="relu")(dense1)

        # Output (pesos menores para horizontes longos)
        output = layers.Dense(
            self.forecast_horizon, activation="linear", name="teleconnection_output"
        )(dense2)

        model = keras.Model(
            inputs=tel_input, outputs=output, name="teleconnection_model"
        )
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.0001),
            loss="mse",
            metrics=["mae"],
        )

        return model

    def build_meta_ensemble_model(self, n_base_models: int = 3) -> keras.Model:
        """
        Meta-modelo que combina as previs√µes dos modelos base
        """
        logger.info("Construindo meta-modelo ensemble...")

        # Inputs das previs√µes dos modelos base
        model_inputs = []
        for i in range(n_base_models):
            model_inputs.append(
                keras.Input(
                    shape=(self.forecast_horizon,), name=f"model_{i}_prediction"
                )
            )

        # Concatena as previs√µes
        combined = layers.Concatenate()(model_inputs)

        # Rede para aprender pesos adaptativos
        dense1 = layers.Dense(128, activation="relu")(combined)
        dense1 = layers.Dropout(0.2)(dense1)
        dense2 = layers.Dense(64, activation="relu")(dense1)

        # Pesos para cada modelo (softmax para garantir soma = 1)
        weights = layers.Dense(
            n_base_models, activation="softmax", name="model_weights"
        )(dense2)

        # Previs√£o final ponderada
        weighted_predictions = []
        for i in range(n_base_models):
            weight = layers.Lambda(lambda x: x[:, i : i + 1])(weights)
            weighted_pred = layers.Multiply()([model_inputs[i], weight])
            weighted_predictions.append(weighted_pred)

        final_prediction = layers.Add()(weighted_predictions)

        model = keras.Model(
            inputs=model_inputs,
            outputs=[final_prediction, weights],
            name="meta_ensemble",
        )
        model.compile(
            optimizer=keras.optimizers.Adam(learning_rate=0.001),
            loss={"add": "mse", "model_weights": "categorical_crossentropy"},
            loss_weights={"add": 1.0, "model_weights": 0.1},
            metrics={"add": ["mae"], "model_weights": ["accuracy"]},
        )

        return model

    def train_ensemble(self, data: Dict[str, np.ndarray]) -> Dict[str, keras.Model]:
        """Treina o ensemble completo"""
        logger.info("Iniciando treinamento do ensemble...")

        # Preparar dados
        X, y = self.create_sequences(data)

        # Split temporal
        split_point = int(len(y) * 0.8)
        X_train = {k: v[:split_point] for k, v in X.items()}
        X_val = {k: v[split_point:] for k, v in X.items()}
        y_train = y[:split_point]
        y_val = y[split_point:]

        input_shapes = {k: v.shape[1:] for k, v in X.items()}

        # Treinar modelos base
        models = {}
        base_predictions = {"train": [], "val": []}

        # Modelo 1: Meteorol√≥gico
        logger.info("Treinando modelo meteorol√≥gico...")
        met_model = self.build_meteorological_model(input_shapes)
        met_history = met_model.fit(
            X_train["meteorological"],
            y_train,
            validation_data=(X_val["meteorological"], y_val),
            epochs=50,
            batch_size=32,
            verbose=0,
            callbacks=[
                keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
            ],
        )
        models["meteorological"] = met_model
        base_predictions["train"].append(
            met_model.predict(X_train["meteorological"], verbose=0)
        )
        base_predictions["val"].append(
            met_model.predict(X_val["meteorological"], verbose=0)
        )

        # Modelo 2: Sin√≥tico
        logger.info("Treinando modelo sin√≥tico...")
        syn_model = self.build_synoptic_model(input_shapes)
        syn_history = syn_model.fit(
            [X_train["meteorological"], X_train["synoptic"]],
            y_train,
            validation_data=([X_val["meteorological"], X_val["synoptic"]], y_val),
            epochs=50,
            batch_size=32,
            verbose=0,
            callbacks=[
                keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
            ],
        )
        models["synoptic"] = syn_model
        base_predictions["train"].append(
            syn_model.predict(
                [X_train["meteorological"], X_train["synoptic"]], verbose=0
            )
        )
        base_predictions["val"].append(
            syn_model.predict([X_val["meteorological"], X_val["synoptic"]], verbose=0)
        )

        # Modelo 3: Teleconex√µes
        logger.info("Treinando modelo de teleconex√µes...")
        tel_model = self.build_teleconnection_model(input_shapes)
        tel_history = tel_model.fit(
            X_train["teleconnections"],
            y_train,
            validation_data=(X_val["teleconnections"], y_val),
            epochs=50,
            batch_size=32,
            verbose=0,
            callbacks=[
                keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)
            ],
        )
        models["teleconnections"] = tel_model
        base_predictions["train"].append(
            tel_model.predict(X_train["teleconnections"], verbose=0)
        )
        base_predictions["val"].append(
            tel_model.predict(X_val["teleconnections"], verbose=0)
        )

        # Meta-modelo
        logger.info("Treinando meta-modelo ensemble...")
        meta_model = self.build_meta_ensemble_model()

        # Preparar dados para meta-modelo (dummy weights para treino)
        dummy_weights = np.ones((len(y_train), 3)) / 3  # Pesos uniformes iniciais

        meta_history = meta_model.fit(
            base_predictions["train"],
            [y_train, dummy_weights],
            validation_data=(
                base_predictions["val"],
                [y_val, np.ones((len(y_val), 3)) / 3],
            ),
            epochs=30,
            batch_size=32,
            verbose=0,
            callbacks=[
                keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)
            ],
        )
        models["meta_ensemble"] = meta_model

        # Salvar modelos
        self._save_models(models)

        # Avaliar ensemble
        self._evaluate_ensemble(models, X_val, y_val, base_predictions["val"])

        return models

    def _save_models(self, models: Dict[str, keras.Model]) -> None:
        """Salva os modelos treinados"""
        logger.info("Salvando modelos...")

        for name, model in models.items():
            model_path = self.models_dir / f"{name}_model.h5"
            model.save(model_path)

        # Salvar scalers
        for name, scaler in self.scalers.items():
            scaler_path = self.models_dir / f"{name}_scaler.joblib"
            joblib.dump(scaler, scaler_path)

    def _evaluate_ensemble(
        self,
        models: Dict[str, keras.Model],
        X_val: Dict,
        y_val: np.ndarray,
        base_preds: List[np.ndarray],
    ) -> None:
        """Avalia o desempenho do ensemble"""
        logger.info("Avaliando ensemble...")

        # Previs√£o do meta-modelo
        ensemble_pred, weights = models["meta_ensemble"].predict(base_preds, verbose=0)

        # M√©tricas por horizonte de tempo
        horizons = [24, 48, 72, 96]  # horas
        results = {}

        for horizon in horizons:
            if horizon <= self.forecast_horizon:
                idx = horizon - 1

                # M√©tricas individuais
                for i, (name, _) in enumerate(
                    [(k, v) for k, v in models.items() if k != "meta_ensemble"]
                ):
                    pred = base_preds[i][:, idx]
                    true = y_val[:, idx]

                    results[f"{name}_{horizon}h"] = {
                        "mae": mean_absolute_error(true, pred),
                        "rmse": np.sqrt(mean_squared_error(true, pred)),
                        "r2": r2_score(true, pred),
                    }

                # M√©tricas do ensemble
                ensemble_pred_h = ensemble_pred[:, idx]
                true_h = y_val[:, idx]

                results[f"ensemble_{horizon}h"] = {
                    "mae": mean_absolute_error(true_h, ensemble_pred_h),
                    "rmse": np.sqrt(mean_squared_error(true_h, ensemble_pred_h)),
                    "r2": r2_score(true_h, ensemble_pred_h),
                }

        # Salvar resultados
        results_file = (
            self.results_dir
            / f"ensemble_evaluation_{datetime.now().strftime('%Y%m%d_%H%M')}.json"
        )
        with open(results_file, "w") as f:
            json.dump(results, f, indent=2)

        # Imprimir resumo
        print("\n" + "=" * 60)
        print("üìä AVALIA√á√ÉO DO ENSEMBLE")
        print("=" * 60)

        for horizon in horizons:
            if f"ensemble_{horizon}h" in results:
                print(f"\n‚è∞ {horizon} horas:")
                ensemble_metrics = results[f"ensemble_{horizon}h"]
                print(f"  üìè MAE: {ensemble_metrics['mae']:.3f}")
                print(f"  üìê RMSE: {ensemble_metrics['rmse']:.3f}")
                print(f"  üìà R¬≤: {ensemble_metrics['r2']:.3f}")

        # An√°lise dos pesos
        avg_weights = np.mean(weights, axis=0)
        print(f"\nüèãÔ∏è Pesos m√©dios do ensemble:")
        model_names = ["Meteorol√≥gico", "Sin√≥tico", "Teleconex√µes"]
        for i, (name, weight) in enumerate(zip(model_names, avg_weights)):
            print(f"  {name}: {weight:.1%}")

        print("=" * 60)


def main():
    """Demonstra√ß√£o do treinamento do ensemble"""

    print("üöÄ INICIANDO TREINAMENTO DO ENSEMBLE")
    print("=" * 50)

    trainer = EnsembleForecastTrainer()

    # Preparar dados
    print("üìä Preparando dados de treinamento...")
    training_data = trainer.prepare_training_data()

    print(f"‚úÖ Dados preparados:")
    print(f"  üìÖ Per√≠odo: {len(training_data['dates'])} horas")
    print(f"  üå°Ô∏è Features meteorol√≥gicas: {training_data['meteorological'].shape[1]}")
    print(f"  üåÄ Features sin√≥ticas: {training_data['synoptic'].shape[1]}")
    print(f"  üåç Features teleconex√µes: {training_data['teleconnections'].shape[1]}")

    # Treinar ensemble
    print("\nüß† Treinando modelos...")
    models = trainer.train_ensemble(training_data)

    print(f"\n‚úÖ Treinamento conclu√≠do!")
    print(f"üìÅ Modelos salvos em: {trainer.models_dir}")
    print(f"üìà Resultados salvos em: {trainer.results_dir}")

    print("\nüéØ RECOMENDA√á√ïES PARA PRODU√á√ÉO:")
    print("1. üì° Integrar APIs reais (NOAA, ECMWF)")
    print("2. üîÑ Implementar retreinamento autom√°tico")
    print("3. üìä Adicionar monitoramento de drift")
    print("4. ‚ö° Otimizar para infer√™ncia em tempo real")
    print("5. üéõÔ∏è Implementar ajuste fino por regi√£o")


if __name__ == "__main__":
    main()
