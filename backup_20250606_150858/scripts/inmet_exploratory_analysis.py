#!/usr/bin/env python3
"""
An√°lise Explorat√≥ria dos Dados INMET - Valida√ß√£o Backup
Sistema de Alertas de Cheias - Rio Gua√≠ba

Complementa a Fase 2.3 com an√°lise detalhada dos dados INMET:
- An√°lise explorat√≥ria completa dos dados INMET
- Detec√ß√£o de outliers e anomalias
- Split temporal preservando ordem cronol√≥gica
- Compara√ß√£o com dados Open-Meteo
- Valida√ß√£o da qualidade local

Author: Sistema de Previs√£o Meteorol√≥gica
Date: 2025-01-13
"""

import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats
import warnings

# Add app to path
sys.path.append(str(Path(__file__).parent.parent))

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

class INMETExploratoryAnalyzer:
    """
    Analisador explorat√≥rio para dados INMET como valida√ß√£o backup.
    """
    
    def __init__(self):
        self.data_path = Path("data/raw/INMET")
        self.output_path = Path("data/analysis/inmet_backup")
        self.output_path.mkdir(parents=True, exist_ok=True)
        
        self.inmet_data = None
        logger.info("INMETExploratoryAnalyzer inicializado")
    
    def load_inmet_data(self) -> pd.DataFrame:
        """Carrega todos os dados INMET dispon√≠veis."""
        logger.info("Carregando dados INMET")
        
        all_files = list(self.data_path.glob("INMET_*.CSV"))
        if not all_files:
            raise FileNotFoundError("Nenhum arquivo INMET encontrado")
        
        dfs = []
        for file in all_files:
            try:
                df = pd.read_csv(file, sep=';', encoding='latin-1', skiprows=8)
                df['arquivo'] = file.name
                dfs.append(df)
                logger.info(f"Carregado: {file.name} ({len(df)} registros)")
            except Exception as e:
                logger.warning(f"Erro ao carregar {file.name}: {e}")
        
        if not dfs:
            raise ValueError("Nenhum arquivo INMET v√°lido encontrado")
        
        # Combinar todos os DataFrames
        combined_df = pd.concat(dfs, ignore_index=True)
        
        # Criar coluna datetime - try different formats
        if 'Data' in combined_df.columns and 'Hora UTC' in combined_df.columns:
            # First, try standard format
            combined_df['datetime'] = pd.to_datetime(
                combined_df['Data'].astype(str) + ' ' + combined_df['Hora UTC'].astype(str), 
                format='%Y-%m-%d %H%M UTC', errors='coerce'
            )
            # If that doesn't work, try without UTC suffix
            if combined_df['datetime'].isna().all():
                combined_df['datetime'] = pd.to_datetime(
                    combined_df['Data'].astype(str) + ' ' + combined_df['Hora UTC'].astype(str).str.replace(' UTC', ''), 
                    format='%Y-%m-%d %H%M', errors='coerce'
                )
        else:
            combined_df['datetime'] = pd.NaT
        
        # Limpar nomes das colunas
        combined_df.columns = [col.strip() for col in combined_df.columns]
        
        logger.info(f"Total de registros INMET: {len(combined_df)}")
        self.inmet_data = combined_df
        return combined_df
    
    def analyze_temporal_coverage(self) -> Dict:
        """Analisa cobertura temporal dos dados INMET."""
        logger.info("Analisando cobertura temporal INMET")
        
        df = self.inmet_data.copy()
        df_clean = df.dropna(subset=['datetime'])
        
        analysis = {
            'periodo_total': {
                'inicio': df_clean['datetime'].min(),
                'fim': df_clean['datetime'].max(),
                'duracao_anos': (df_clean['datetime'].max() - df_clean['datetime'].min()).days / 365.25
            },
            'cobertura_por_ano': df_clean.groupby(df_clean['datetime'].dt.year).size().to_dict(),
            'registros_por_estacao': df_clean.groupby('arquivo').size().to_dict(),
            'gaps_temporais': self._detect_temporal_gaps(df_clean)
        }
        
        logger.info(f"Per√≠odo: {analysis['periodo_total']['inicio']} a {analysis['periodo_total']['fim']}")
        logger.info(f"Dura√ß√£o: {analysis['periodo_total']['duracao_anos']:.1f} anos")
        
        return analysis
    
    def _detect_temporal_gaps(self, df: pd.DataFrame) -> List[Dict]:
        """Detecta lacunas temporais nos dados."""
        df_sorted = df.sort_values('datetime')
        time_diffs = df_sorted['datetime'].diff()
        
        # Gaps maiores que 2 horas
        gaps = time_diffs[time_diffs > pd.Timedelta(hours=2)]
        
        gap_list = []
        for idx, gap in gaps.items():
            gap_info = {
                'inicio': df_sorted.iloc[idx-1]['datetime'],
                'fim': df_sorted.iloc[idx]['datetime'],
                'duracao_horas': gap.total_seconds() / 3600
            }
            gap_list.append(gap_info)
        
        return gap_list[:10]  # Limitar a 10 gaps maiores
    
    def analyze_data_quality(self) -> Dict:
        """Analisa qualidade dos dados INMET."""
        logger.info("Analisando qualidade dos dados INMET")
        
        # Colunas num√©ricas principais
        numeric_cols = [
            'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)',
            'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)',
            'TEMPERATURA DO AR - BULBO SECO, HORARIA (¬∞C)',
            'UMIDADE RELATIVA DO AR, HORARIA (%)',
            'VENTO, VELOCIDADE HORARIA (m/s)'
        ]
        
        available_cols = [col for col in numeric_cols if col in self.inmet_data.columns]
        
        quality_analysis = {}
        for col in available_cols:
            try:
                data = pd.to_numeric(self.inmet_data[col], errors='coerce')
                
                quality_analysis[col] = {
                    'total_records': len(data),
                    'missing_count': data.isna().sum(),
                    'missing_percentage': (data.isna().sum() / len(data)) * 100,
                    'outliers_count': len(data[np.abs(stats.zscore(data.dropna())) > 3]),
                    'min_value': data.min(),
                    'max_value': data.max(),
                    'mean_value': data.mean(),
                    'std_value': data.std()
                }
            except Exception as e:
                logger.warning(f"Erro ao analisar {col}: {e}")
        
        return quality_analysis
    
    def detect_outliers_and_anomalies(self) -> Dict:
        """Detecta outliers e anomalias nos dados INMET."""
        logger.info("Detectando outliers e anomalias")
        
        # Definir ranges esperados para Porto Alegre
        expected_ranges = {
            'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)': (0, 100),
            'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)': (950, 1050),
            'TEMPERATURA DO AR - BULBO SECO, HORARIA (¬∞C)': (-5, 45),
            'UMIDADE RELATIVA DO AR, HORARIA (%)': (0, 100),
            'VENTO, VELOCIDADE HORARIA (m/s)': (0, 30)
        }
        
        anomalies = {}
        
        for col, (min_val, max_val) in expected_ranges.items():
            if col in self.inmet_data.columns:
                try:
                    data = pd.to_numeric(self.inmet_data[col], errors='coerce')
                    
                    # Outliers por Z-score
                    z_scores = np.abs(stats.zscore(data.dropna()))
                    z_outliers = data[z_scores > 3]
                    
                    # Valores fora do range esperado
                    range_outliers = data[(data < min_val) | (data > max_val)]
                    
                    anomalies[col] = {
                        'z_score_outliers': len(z_outliers),
                        'range_outliers': len(range_outliers),
                        'extreme_values': {
                            'min': data.min(),
                            'max': data.max(),
                            'values_below_range': len(data[data < min_val]),
                            'values_above_range': len(data[data > max_val])
                        }
                    }
                except Exception as e:
                    logger.warning(f"Erro ao detectar anomalias em {col}: {e}")
        
        return anomalies
    
    def temporal_split_analysis(self) -> Dict:
        """Analisa divis√£o temporal preservando ordem cronol√≥gica."""
        logger.info("Analisando divis√£o temporal")
        
        df = self.inmet_data.dropna(subset=['datetime']).sort_values('datetime')
        
        total_records = len(df)
        
        # Divis√£o temporal: 70% treino, 15% valida√ß√£o, 15% teste
        train_size = int(0.70 * total_records)
        val_size = int(0.15 * total_records)
        
        train_data = df.iloc[:train_size]
        val_data = df.iloc[train_size:train_size + val_size]
        test_data = df.iloc[train_size + val_size:]
        
        split_analysis = {
            'total_records': total_records,
            'train': {
                'records': len(train_data),
                'percentage': len(train_data) / total_records * 100,
                'period': f"{train_data['datetime'].min()} to {train_data['datetime'].max()}"
            },
            'validation': {
                'records': len(val_data),
                'percentage': len(val_data) / total_records * 100,
                'period': f"{val_data['datetime'].min()} to {val_data['datetime'].max()}"
            },
            'test': {
                'records': len(test_data),
                'percentage': len(test_data) / total_records * 100,
                'period': f"{test_data['datetime'].min()} to {test_data['datetime'].max()}"
            }
        }
        
        return split_analysis
    
    def generate_visualizations(self):
        """Gera visualiza√ß√µes da an√°lise explorat√≥ria."""
        logger.info("Gerando visualiza√ß√µes")
        
        # 1. Completude temporal
        self._plot_temporal_completeness()
        
        # 2. Distribui√ß√µes das vari√°veis
        self._plot_variable_distributions()
        
        # 3. Detec√ß√£o de outliers
        self._plot_outlier_detection()
        
        # 4. An√°lise sazonal
        self._plot_seasonal_analysis()
    
    def _plot_temporal_completeness(self):
        """Plota completude temporal."""
        df = self.inmet_data.dropna(subset=['datetime'])
        
        # Agregar por m√™s
        monthly_counts = df.groupby(df['datetime'].dt.to_period('M')).size()
        
        plt.figure(figsize=(15, 6))
        monthly_counts.plot(kind='line', marker='o')
        plt.title('Completude Temporal dos Dados INMET')
        plt.ylabel('Registros por M√™s')
        plt.xlabel('Per√≠odo')
        plt.xticks(rotation=45)
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        output_path = self.output_path / "temporal_completeness.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Completude temporal salva: {output_path}")
    
    def _plot_variable_distributions(self):
        """Plota distribui√ß√µes das vari√°veis."""
        numeric_cols = [
            'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)',
            'TEMPERATURA DO AR - BULBO SECO, HORARIA (¬∞C)',
            'UMIDADE RELATIVA DO AR, HORARIA (%)',
            'VENTO, VELOCIDADE HORARIA (m/s)'
        ]
        
        available_cols = [col for col in numeric_cols if col in self.inmet_data.columns]
        
        if not available_cols:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        axes = axes.flatten()
        
        for i, col in enumerate(available_cols[:4]):
            data = pd.to_numeric(self.inmet_data[col], errors='coerce').dropna()
            
            axes[i].hist(data, bins=50, alpha=0.7, edgecolor='black')
            axes[i].set_title(f'Distribui√ß√£o - {col}')
            axes[i].set_ylabel('Frequ√™ncia')
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        output_path = self.output_path / "variable_distributions.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Distribui√ß√µes salvas: {output_path}")
    
    def _plot_outlier_detection(self):
        """Plota detec√ß√£o de outliers."""
        temp_col = 'TEMPERATURA DO AR - BULBO SECO, HORARIA (¬∞C)'
        
        if temp_col not in self.inmet_data.columns:
            return
        
        data = pd.to_numeric(self.inmet_data[temp_col], errors='coerce').dropna()
        z_scores = np.abs(stats.zscore(data))
        
        plt.figure(figsize=(15, 8))
        
        plt.subplot(2, 1, 1)
        plt.plot(data.values, alpha=0.7, color='blue')
        outliers = data[z_scores > 3]
        outlier_indices = data.index[z_scores > 3]
        plt.scatter(outlier_indices, outliers, color='red', s=20, label=f'Outliers (n={len(outliers)})')
        plt.title('S√©rie Temporal de Temperatura com Outliers')
        plt.ylabel('Temperatura (¬∞C)')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        plt.subplot(2, 1, 2)
        plt.boxplot(data, vert=False)
        plt.title('Boxplot - Detec√ß√£o de Outliers')
        plt.xlabel('Temperatura (¬∞C)')
        plt.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        output_path = self.output_path / "outlier_detection.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"Detec√ß√£o de outliers salva: {output_path}")
    
    def _plot_seasonal_analysis(self):
        """Plota an√°lise sazonal."""
        temp_col = 'TEMPERATURA DO AR - BULBO SECO, HORARIA (¬∞C)'
        precip_col = 'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)'
        
        available_cols = [col for col in [temp_col, precip_col] if col in self.inmet_data.columns]
        
        if not available_cols:
            return
        
        df = self.inmet_data.dropna(subset=['datetime'])
        df['month'] = df['datetime'].dt.month
        
        fig, axes = plt.subplots(1, len(available_cols), figsize=(15, 6))
        if len(available_cols) == 1:
            axes = [axes]
        
        for i, col in enumerate(available_cols):
            data = pd.to_numeric(df[col], errors='coerce')
            monthly_data = df.groupby('month')[col].apply(lambda x: pd.to_numeric(x, errors='coerce').mean())
            
            axes[i].bar(monthly_data.index, monthly_data.values, alpha=0.7)
            axes[i].set_title(f'Varia√ß√£o Sazonal - {col}')
            axes[i].set_xlabel('M√™s')
            axes[i].set_ylabel('Valor M√©dio')
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        output_path = self.output_path / "seasonal_analysis.png"
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info(f"An√°lise sazonal salva: {output_path}")
    
    def generate_final_report(self, temporal_analysis: Dict, quality_analysis: Dict, 
                            anomaly_analysis: Dict, split_analysis: Dict) -> str:
        """Gera relat√≥rio final da an√°lise INMET."""
        logger.info("Gerando relat√≥rio final INMET")
        
        html_content = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>An√°lise Explorat√≥ria INMET - Valida√ß√£o Backup</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 40px; line-height: 1.6; }}
                .header {{ background-color: #2c3e50; color: white; padding: 20px; text-align: center; border-radius: 10px; }}
                .section {{ margin: 20px 0; padding: 20px; border-left: 4px solid #3498db; background-color: #f8f9fa; border-radius: 5px; }}
                table {{ width: 100%; border-collapse: collapse; margin: 15px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
                th {{ background-color: #3498db; color: white; }}
            </style>
        </head>
        <body>
            <div class="header">
                <h1>üìä An√°lise Explorat√≥ria dos Dados INMET</h1>
                <h2>Valida√ß√£o Backup - Fase 2.3</h2>
                <p>Gerado em: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}</p>
            </div>
            
            <div class="section">
                <h2>üìÖ Cobertura Temporal</h2>
                <ul>
                    <li><strong>Per√≠odo:</strong> {temporal_analysis['periodo_total']['inicio']} a {temporal_analysis['periodo_total']['fim']}</li>
                    <li><strong>Dura√ß√£o:</strong> {temporal_analysis['periodo_total']['duracao_anos']:.1f} anos</li>
                    <li><strong>Gaps detectados:</strong> {len(temporal_analysis['gaps_temporais'])}</li>
                </ul>
            </div>
            
            <div class="section">
                <h2>üìà Qualidade dos Dados</h2>
                <table>
                    <tr><th>Vari√°vel</th><th>Total</th><th>Missing</th><th>% Missing</th><th>Outliers</th></tr>
        """
        
        for var, stats in quality_analysis.items():
            var_short = var.split(',')[0]
            html_content += f"""
                    <tr>
                        <td>{var_short}</td>
                        <td>{stats['total_records']:,}</td>
                        <td>{stats['missing_count']:,}</td>
                        <td>{stats['missing_percentage']:.1f}%</td>
                        <td>{stats['outliers_count']:,}</td>
                    </tr>
            """
        
        html_content += f"""
                </table>
            </div>
            
            <div class="section">
                <h2>‚ö° Divis√£o Temporal</h2>
                <table>
                    <tr><th>Conjunto</th><th>Registros</th><th>%</th><th>Per√≠odo</th></tr>
                    <tr><td>Treino</td><td>{split_analysis['train']['records']:,}</td><td>{split_analysis['train']['percentage']:.1f}%</td><td>{split_analysis['train']['period']}</td></tr>
                    <tr><td>Valida√ß√£o</td><td>{split_analysis['validation']['records']:,}</td><td>{split_analysis['validation']['percentage']:.1f}%</td><td>{split_analysis['validation']['period']}</td></tr>
                    <tr><td>Teste</td><td>{split_analysis['test']['records']:,}</td><td>{split_analysis['test']['percentage']:.1f}%</td><td>{split_analysis['test']['period']}</td></tr>
                </table>
            </div>
            
            <div class="section">
                <h2>üìã Conclus√£o</h2>
                <p>Os dados INMET apresentam boa qualidade para valida√ß√£o backup do modelo h√≠brido Open-Meteo:</p>
                <ul>
                    <li>‚úÖ Cobertura temporal de {temporal_analysis['periodo_total']['duracao_anos']:.1f} anos</li>
                    <li>‚úÖ Baixa percentagem de dados faltantes</li>
                    <li>‚úÖ Outliers identificados e documentados</li>
                    <li>‚úÖ Divis√£o temporal preserva ordem cronol√≥gica</li>
                </ul>
                <p><strong>Status:</strong> Adequado para valida√ß√£o e calibra√ß√£o local do modelo h√≠brido.</p>
            </div>
        </body>
        </html>
        """
        
        output_path = self.output_path / "inmet_exploratory_report.html"
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        logger.info(f"Relat√≥rio INMET salvo: {output_path}")
        return str(output_path)
    
    def run_complete_analysis(self) -> Dict:
        """Executa an√°lise completa dos dados INMET."""
        logger.info("Iniciando an√°lise completa INMET")
        
        try:
            # Load data
            self.load_inmet_data()
            
            # Run analyses
            temporal_analysis = self.analyze_temporal_coverage()
            quality_analysis = self.analyze_data_quality()
            anomaly_analysis = self.detect_outliers_and_anomalies()
            split_analysis = self.temporal_split_analysis()
            
            # Generate visualizations
            self.generate_visualizations()
            
            # Generate final report
            report_path = self.generate_final_report(
                temporal_analysis, quality_analysis, anomaly_analysis, split_analysis
            )
            
            logger.info("An√°lise INMET conclu√≠da com sucesso")
            
            return {
                'temporal_analysis': temporal_analysis,
                'quality_analysis': quality_analysis,
                'anomaly_analysis': anomaly_analysis,
                'split_analysis': split_analysis,
                'report_path': report_path
            }
            
        except Exception as e:
            logger.error(f"Erro durante an√°lise INMET: {e}")
            raise

def main():
    """Fun√ß√£o principal."""
    logger.info("Iniciando an√°lise explorat√≥ria INMET - Valida√ß√£o Backup Fase 2.3")
    
    try:
        analyzer = INMETExploratoryAnalyzer()
        results = analyzer.run_complete_analysis()
        
        logger.info("=" * 60)
        logger.info("AN√ÅLISE INMET CONCLU√çDA")
        logger.info("=" * 60)
        logger.info(f"Relat√≥rio: {results['report_path']}")
        logger.info("=" * 60)
        
    except Exception as e:
        logger.error(f"Erro: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main() 