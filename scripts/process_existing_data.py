#!/usr/bin/env python3
"""
Script para processar dados jÃ¡ existentes em data/raw/
Converte dados brutos em formato processado para os notebooks
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def setup_directories():
    """Criar diretÃ³rios necessÃ¡rios"""
    # Detectar se estamos executando do notebook ou da raiz
    current_dir = Path.cwd()
    if current_dir.name == "notebooks":
        base_path = current_dir.parent
    else:
        base_path = current_dir
    
    dirs = [
        base_path / "data/processed",
        base_path / "data/analysis", 
        base_path / "models"
    ]
    
    for dir_path in dirs:
        dir_path.mkdir(parents=True, exist_ok=True)
    
    print("âœ… DiretÃ³rios configurados")
    return base_path

def process_openmeteo_forecast(base_path):
    """Processar dados Open-Meteo Historical Forecast (JSON)"""
    print("\nðŸ”„ Processando Open-Meteo Historical Forecast...")
    
    forecast_dir = base_path / "data/raw/Open-Meteo Historical Forecast"
    if not forecast_dir.exists():
        print("âŒ DiretÃ³rio nÃ£o encontrado")
        return None
    
    # Encontrar arquivos JSON
    json_files = list(forecast_dir.glob("*.json"))
    
    if not json_files:
        print("âŒ Nenhum arquivo JSON encontrado")
        return None
    
    print(f"ðŸ“ Encontrados {len(json_files)} arquivos JSON")
    
    # Carregar e combinar arquivos JSON
    all_hourly_data = []
    
    for file in json_files[:5]:  # Limitar a 5 arquivos para performance
        try:
            with open(file, 'r') as f:
                data = json.load(f)
            
            # Extrair dados horÃ¡rias
            if 'hourly' in data:
                hourly = data['hourly']
                
                # Converter em DataFrame
                df_hourly = pd.DataFrame(hourly)
                
                # Garantir coluna datetime
                if 'time' in df_hourly.columns:
                    df_hourly['datetime'] = pd.to_datetime(df_hourly['time'])
                
                all_hourly_data.append(df_hourly)
                print(f"âœ… Carregado: {file.name} - {df_hourly.shape}")
                
        except Exception as e:
            print(f"âŒ Erro em {file.name}: {e}")
    
    if not all_hourly_data:
        print("âŒ Nenhum dado processado com sucesso")
        return None
    
    # Combinar todos os DataFrames
    df_combined = pd.concat(all_hourly_data, ignore_index=True)
    
    # Limpar e organizar
    if 'datetime' not in df_combined.columns and 'time' in df_combined.columns:
        df_combined['datetime'] = pd.to_datetime(df_combined['time'])
    
    # Remover duplicatas e ordenar
    if 'datetime' in df_combined.columns:
        df_combined = df_combined.drop_duplicates(subset=['datetime']).sort_values('datetime')
    
    # Salvar processado
    output_file = base_path / "data/processed/openmeteo_historical_forecast.csv"
    df_combined.to_csv(output_file, index=False)
    
    print(f"âœ… Salvo: {output_file}")
    print(f"ðŸ“Š Shape final: {df_combined.shape}")
    if 'datetime' in df_combined.columns:
        print(f"ðŸ—“ï¸ PerÃ­odo: {df_combined['datetime'].min()} atÃ© {df_combined['datetime'].max()}")
    
    return df_combined

def process_openmeteo_weather(base_path):
    """Processar dados Open-Meteo Historical Weather (CSV)"""
    print("\nðŸ”„ Processando Open-Meteo Historical Weather...")
    
    weather_dir = base_path / "data/raw/Open-Meteo Historical Weather"
    if not weather_dir.exists():
        print("âŒ DiretÃ³rio nÃ£o encontrado")
        return None
    
    # Priorizar arquivos horÃ¡rias (mais dados)
    hourly_files = list(weather_dir.glob("*hourly*.csv"))
    daily_files = list(weather_dir.glob("*daily*.csv"))
    
    files_to_process = hourly_files if hourly_files else daily_files
    
    if not files_to_process:
        print("âŒ Nenhum arquivo CSV encontrado")
        return None
    
    print(f"ðŸ“ Encontrados {len(files_to_process)} arquivos ({'horÃ¡rias' if hourly_files else 'diÃ¡rias'})")
    
    # Carregar e combinar mÃºltiplos arquivos
    dfs = []
    for file in files_to_process[:10]:  # Limitar a 10 arquivos para performance
        try:
            df = pd.read_csv(file)
            
            # Garantir coluna datetime
            datetime_cols = ['datetime', 'time', 'date', 'timestamp']
            datetime_col = None
            
            for col in datetime_cols:
                if col in df.columns:
                    datetime_col = col
                    break
            
            if datetime_col:
                df['datetime'] = pd.to_datetime(df[datetime_col])
            else:
                print(f"âš ï¸ {file.name}: Coluna de datetime nÃ£o encontrada")
                # Tentar criar baseado no nome do arquivo
                year_match = str(file.name)
                if any(year in year_match for year in ['2020', '2021', '2022', '2023', '2024']):
                    year = int([y for y in ['2020', '2021', '2022', '2023', '2024'] if y in year_match][0])
                    df['datetime'] = pd.date_range(f'{year}-01-01', periods=len(df), freq='H')
            
            dfs.append(df)
            print(f"âœ… Carregado: {file.name} - {df.shape}")
            
        except Exception as e:
            print(f"âŒ Erro em {file.name}: {e}")
    
    if not dfs:
        return None
    
    # Combinar DataFrames
    df_combined = pd.concat(dfs, ignore_index=True)
    
    # Remover duplicatas se temos datetime
    if 'datetime' in df_combined.columns:
        df_combined = df_combined.drop_duplicates(subset=['datetime']).sort_values('datetime')
    
    # Salvar processado
    output_file = base_path / "data/processed/openmeteo_historical_weather.csv"
    df_combined.to_csv(output_file, index=False)
    
    print(f"âœ… Salvo: {output_file}")
    print(f"ðŸ“Š Shape final: {df_combined.shape}")
    
    return df_combined

def process_inmet_data(base_path):
    """Processar dados INMET"""
    print("\nðŸ”„ Processando dados INMET...")
    
    inmet_dir = base_path / "data/raw/INMET"
    if not inmet_dir.exists():
        print("âŒ DiretÃ³rio INMET nÃ£o encontrado")
        return None
    
    # Procurar arquivos CSV (maiÃºsculo e minÃºsculo)
    csv_files = list(inmet_dir.glob("*.csv")) + list(inmet_dir.glob("*.CSV"))
    
    if not csv_files:
        print("âŒ Nenhum arquivo INMET encontrado")
        return None
    
    print(f"ðŸ“ Encontrados {len(csv_files)} arquivos INMET")
    
    # Combinar mÃºltiplos arquivos INMET
    all_dfs = []
    
    for file in csv_files[:15]:  # Aumentar para 15 arquivos
        try:
            # INMET usa encoding latin-1, separador ; e tem cabeÃ§alho de 8 linhas
            df = pd.read_csv(file, encoding='latin-1', sep=';', skiprows=8, 
                           decimal=',', na_values=['', ' ', 'null', 'NULL', '-9999', '-9999.0'])
            
            # Verificar se conseguiu carregar dados vÃ¡lidos
            if df.shape[0] > 0 and df.shape[1] > 3:
                print(f"âœ… Carregado: {file.name} - {df.shape}")
            else:
                print(f"âš ï¸ Dados insuficientes: {file.name} - {df.shape}")
                continue
            
            # Limpar o DataFrame
            df = df.dropna(how='all')  # Remover linhas completamente vazias
            df = df.dropna(axis=1, how='all')  # Remover colunas completamente vazias
            
            # Remover colunas unnamed
            df = df.loc[:, ~df.columns.str.contains('^Unnamed')]
            
            if df.empty:
                print(f"âš ï¸ DataFrame vazio apÃ³s limpeza: {file.name}")
                continue
            
            # Extrair ano do nome do arquivo
            file_name = str(file.name)
            year = 2020  # padrÃ£o
            for y in range(2000, 2026):
                if str(y) in file_name:
                    year = y
                    break
            
            # Identificar coluna de data
            date_col = None
            possible_date_cols = ['Data', 'DATA (YYYY-MM-DD)', 'data']
            
            for col in df.columns:
                if col in possible_date_cols or 'data' in col.lower():
                    date_col = col
                    break
            
            if not date_col and len(df.columns) > 0:
                # A primeira coluna normalmente Ã© a data
                date_col = df.columns[0]
            
            if date_col and date_col in df.columns:
                try:
                    # Tentar diferentes formatos de data
                    if 'YYYY-MM-DD' in date_col:
                        df['datetime'] = pd.to_datetime(df[date_col], format='%Y-%m-%d', errors='coerce')
                    else:
                        df['datetime'] = pd.to_datetime(df[date_col], format='%Y/%m/%d', errors='coerce')
                    
                    # Se tem coluna de hora, tentar combinar
                    hora_col = None
                    for col in df.columns:
                        if 'hora' in col.lower() and 'utc' in col.lower():
                            hora_col = col
                            break
                    
                    if hora_col and hora_col in df.columns:
                        # Converter hora UTC para datetime
                        df['hora_str'] = df[hora_col].astype(str).str.replace(' UTC', '').str.replace('UTC', '').str.zfill(4)
                        df['hora_time'] = pd.to_datetime(df['hora_str'], format='%H%M', errors='coerce').dt.time
                        
                        # Combinar data e hora
                        valid_dates = df['datetime'].notna()
                        valid_times = df['hora_time'].notna()
                        valid_both = valid_dates & valid_times
                        
                        if valid_both.sum() > 0:
                            df.loc[valid_both, 'datetime'] = pd.to_datetime(
                                df.loc[valid_both, 'datetime'].dt.date.astype(str) + ' ' + 
                                df.loc[valid_both, 'hora_time'].astype(str), 
                                errors='coerce'
                            )
                    
                    valid_count = df['datetime'].notna().sum()
                    print(f"âœ… Data processada - perÃ­odo vÃ¡lido: {valid_count}/{len(df)} registros")
                    
                except Exception as e:
                    print(f"âš ï¸ Erro convertendo data, criando sequencial: {str(e)[:50]}...")
                    df['datetime'] = pd.date_range(f'{year}-01-01', periods=len(df), freq='H')
            else:
                print("âš ï¸ Coluna de data nÃ£o encontrada, criando sequencial")
                df['datetime'] = pd.date_range(f'{year}-01-01', periods=len(df), freq='H')
            
            # Padronizar nomes de colunas principais antes de combinar
            standardized_cols = {}
            for col in df.columns:
                col_lower = col.lower()
                if 'temperatura do ar' in col_lower and 'bulbo seco' in col_lower:
                    standardized_cols[col] = 'temperatura'
                elif 'precipitaÃ§Ã£o total' in col_lower or 'precipita' in col_lower:
                    standardized_cols[col] = 'precipitacao'  
                elif 'umidade relativa do ar' in col_lower and 'horaria' in col_lower:
                    standardized_cols[col] = 'umidade'
                elif 'pressao atmosferica' in col_lower and 'estacao' in col_lower:
                    standardized_cols[col] = 'pressao'
                elif 'vento' in col_lower and 'velocidade' in col_lower:
                    standardized_cols[col] = 'vento_velocidade'
                elif 'vento' in col_lower and ('direÃ§Ã£o' in col_lower or 'direcao' in col_lower):
                    standardized_cols[col] = 'vento_direcao'
            
            df = df.rename(columns=standardized_cols)
            
            # Adicionar metadados
            df['ano_arquivo'] = year
            df['fonte_arquivo'] = file.name
            
            # Selecionar apenas colunas essenciais para evitar conflitos
            essential_cols = ['datetime', 'temperatura', 'precipitacao', 'umidade', 'pressao', 
                            'vento_velocidade', 'vento_direcao', 'ano_arquivo', 'fonte_arquivo']
            
            available_cols = [col for col in essential_cols if col in df.columns]
            df_clean = df[available_cols].copy()
            
            all_dfs.append(df_clean)
            
        except Exception as e:
            print(f"âŒ Erro processando {file.name}: {str(e)[:100]}...")
    
    if not all_dfs:
        print("âŒ Nenhum arquivo INMET processado com sucesso")
        return None
    
    print(f"ðŸ“Š Processados {len(all_dfs)} arquivos com sucesso")
    
    # Combinar todos os DataFrames
    df_combined = pd.concat(all_dfs, ignore_index=True, sort=False)
    
    # Remover duplicatas se temos datetime
    if 'datetime' in df_combined.columns:
        initial_size = len(df_combined)
        df_combined = df_combined.drop_duplicates(subset=['datetime']).sort_values('datetime')
        removed = initial_size - len(df_combined)
        if removed > 0:
            print(f"ðŸ”„ Removidas {removed} duplicatas")
    
    # Converter valores -9999 restantes para NaN
    numeric_cols = ['temperatura', 'precipitacao', 'umidade', 'pressao', 'vento_velocidade', 'vento_direcao']
    for col in numeric_cols:
        if col in df_combined.columns:
            df_combined[col] = pd.to_numeric(df_combined[col], errors='coerce')
            # Converter -9999 para NaN
            df_combined.loc[df_combined[col] == -9999, col] = pd.NA
            df_combined.loc[df_combined[col] < -999, col] = pd.NA  # Outros valores extremos
    
    # Salvar processado
    output_file = base_path / "data/processed/dados_inmet_processados.csv"
    df_combined.to_csv(output_file, index=False, encoding='utf-8')
    
    print(f"âœ… Salvo: {output_file}")
    print(f"ðŸ“Š Shape final: {df_combined.shape}")
    print(f"ðŸ“‹ Colunas finais: {list(df_combined.columns)}")
    
    if 'datetime' in df_combined.columns:
        valid_dates = df_combined['datetime'].notna()
        if valid_dates.sum() > 0:
            print(f"ðŸ—“ï¸ PerÃ­odo: {df_combined.loc[valid_dates, 'datetime'].min()} atÃ© {df_combined.loc[valid_dates, 'datetime'].max()}")
        print(f"ðŸ“… Registros com data vÃ¡lida: {valid_dates.sum()}/{len(df_combined)}")
    
    # EstatÃ­sticas de qualidade dos dados
    print("\nðŸ“ˆ Qualidade dos dados:")
    for col in numeric_cols:
        if col in df_combined.columns:
            valid_count = df_combined[col].notna().sum()
            total_count = len(df_combined)
            pct = (valid_count / total_count) * 100
            print(f"  â€¢ {col}: {valid_count}/{total_count} ({pct:.1f}%) valores vÃ¡lidos")
    
    return df_combined

def create_atmospheric_features(df_forecast, base_path):
    """Criar features atmosfÃ©ricas bÃ¡sicas"""
    print("\nðŸ§® Criando features atmosfÃ©ricas...")
    
    if df_forecast is None:
        print("âŒ Dados forecast nÃ£o disponÃ­veis")
        return None
    
    df_features = df_forecast.copy()
    
    # Identificar colunas de nÃ­veis de pressÃ£o
    pressure_levels = ['850hPa', '500hPa', '700hPa', '1000hPa']
    temp_cols = []
    
    for level in pressure_levels:
        # Diferentes formatos possÃ­veis
        possible_cols = [
            f"temperature_{level}",
            f"temp_{level}",
            f"T_{level}",
            f"temperature{level}"
        ]
        
        for temp_col in possible_cols:
            if temp_col in df_features.columns:
                temp_cols.append(temp_col)
                break
    
    print(f"ðŸŒ¡ï¸ Colunas de temperatura encontradas: {temp_cols}")
    
    # Criar gradiente tÃ©rmico se temos pelo menos 2 nÃ­veis
    if len(temp_cols) >= 2:
        # Procurar especificamente 850hPa e 500hPa
        temp_850 = None
        temp_500 = None
        
        for col in temp_cols:
            if '850' in col:
                temp_850 = col
            elif '500' in col:
                temp_500 = col
        
        if temp_850 and temp_500:
            df_features['thermal_gradient_850_500'] = (
                df_features[temp_850] - df_features[temp_500]
            )
            print("âœ… Gradiente tÃ©rmico 850-500hPa criado")
    
    # Features temporais bÃ¡sicas
    if 'datetime' in df_features.columns:
        df_features['hour'] = pd.to_datetime(df_features['datetime']).dt.hour
        df_features['day_of_year'] = pd.to_datetime(df_features['datetime']).dt.dayofyear
        df_features['month'] = pd.to_datetime(df_features['datetime']).dt.month
        df_features['weekday'] = pd.to_datetime(df_features['datetime']).dt.weekday
        print("âœ… Features temporais criadas")
    
    # Salvar features
    output_file = base_path / "data/processed/atmospheric_features_149vars.csv"
    df_features.to_csv(output_file, index=False)
    
    print(f"âœ… Features salvas: {output_file}")
    print(f"ðŸ“Š Total de features: {df_features.shape[1]}")
    
    return df_features

def create_surface_features(df_weather, base_path):
    """Criar features de superfÃ­cie"""
    print("\nðŸŒ Criando features de superfÃ­cie...")
    
    if df_weather is None:
        print("âŒ Dados weather nÃ£o disponÃ­veis")
        return None
    
    df_surface = df_weather.copy()
    
    # Features temporais
    if 'datetime' in df_surface.columns:
        df_surface['hour'] = pd.to_datetime(df_surface['datetime']).dt.hour
        df_surface['day_of_year'] = pd.to_datetime(df_surface['datetime']).dt.dayofyear
        df_surface['month'] = pd.to_datetime(df_surface['datetime']).dt.month
        df_surface['weekday'] = pd.to_datetime(df_surface['datetime']).dt.weekday
        print("âœ… Features temporais de superfÃ­cie criadas")
    
    # Features meteorolÃ³gicas derivadas se existem as colunas
    if 'temperature_2m' in df_surface.columns and 'relative_humidity_2m' in df_surface.columns:
        # Ãndice de calor simplificado
        df_surface['heat_index'] = df_surface['temperature_2m'] * (1 + df_surface['relative_humidity_2m'] / 100)
        print("âœ… Ãndice de calor criado")
    
    # Salvar features de superfÃ­cie
    output_file = base_path / "data/processed/surface_features_25vars.csv"
    df_surface.to_csv(output_file, index=False)
    
    print(f"âœ… Features de superfÃ­cie salvas: {output_file}")
    print(f"ðŸ“Š Total de features: {df_surface.shape[1]}")
    
    return df_surface

def create_summary_report(df_forecast, df_weather, df_inmet, base_path):
    """Criar relatÃ³rio resumo"""
    print("\nðŸ“‹ Criando relatÃ³rio resumo...")
    
    report = {
        "timestamp": datetime.now().isoformat(),
        "data_sources": {},
        "total_features": 0,
        "date_ranges": {},
        "processing_status": "completed"
    }
    
    if df_forecast is not None:
        report["data_sources"]["openmeteo_forecast"] = {
            "shape": list(df_forecast.shape),
            "features": df_forecast.shape[1],
            "date_range": [
                str(df_forecast['datetime'].min()) if 'datetime' in df_forecast.columns else None,
                str(df_forecast['datetime'].max()) if 'datetime' in df_forecast.columns else None
            ],
            "columns_sample": list(df_forecast.columns[:10])
        }
        report["total_features"] += df_forecast.shape[1]
    
    if df_weather is not None:
        report["data_sources"]["openmeteo_weather"] = {
            "shape": list(df_weather.shape),
            "features": df_weather.shape[1],
            "date_range": [
                str(df_weather['datetime'].min()) if 'datetime' in df_weather.columns else None,
                str(df_weather['datetime'].max()) if 'datetime' in df_weather.columns else None
            ],
            "columns_sample": list(df_weather.columns[:10])
        }
    
    if df_inmet is not None:
        report["data_sources"]["inmet"] = {
            "shape": list(df_inmet.shape),
            "features": df_inmet.shape[1],
            "date_range": [
                str(df_inmet['datetime'].min()) if 'datetime' in df_inmet.columns else None,
                str(df_inmet['datetime'].max()) if 'datetime' in df_inmet.columns else None
            ],
            "columns_sample": list(df_inmet.columns[:10])
        }
    
    # Salvar relatÃ³rio
    report_file = base_path / "data/analysis/processing_report.json"
    with open(report_file, "w") as f:
        json.dump(report, f, indent=2)
    
    print(f"âœ… RelatÃ³rio salvo: {report_file}")
    return report

def main():
    """Executar pipeline completo"""
    print("ðŸš€ INICIANDO PROCESSAMENTO DOS DADOS EXISTENTES")
    print("=" * 50)
    print(f"ðŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # 1. Setup
    base_path = setup_directories()
    
    # 2. Processar dados Open-Meteo
    df_forecast = process_openmeteo_forecast(base_path)
    df_weather = process_openmeteo_weather(base_path)
    
    # 3. Processar dados INMET
    df_inmet = process_inmet_data(base_path)
    
    # 4. Criar features
    df_features = create_atmospheric_features(df_forecast, base_path)
    df_surface = create_surface_features(df_weather, base_path)
    
    # 5. RelatÃ³rio
    report = create_summary_report(df_forecast, df_weather, df_inmet, base_path)
    
    print("\nðŸŽ¯ PROCESSAMENTO CONCLUÃDO!")
    print("=" * 30)
    print(f"âœ… Dados processados e salvos em data/processed/")
    print(f"ðŸ“Š Total de features disponÃ­veis: {report.get('total_features', 0)}")
    print(f"ðŸ“‹ RelatÃ³rio: data/analysis/processing_report.json")
    
    # Resumo dos dados processados
    processed_count = len([ds for ds in report['data_sources'].values() if ds])
    if processed_count > 0:
        print(f"\nðŸ“ˆ Fontes de dados processadas: {processed_count}")
        for source, data in report['data_sources'].items():
            if data:
                print(f"   â€¢ {source}: {data['shape'][0]:,} registros, {data['features']} features")
    
    print("\nðŸ”— PrÃ³ximos passos:")
    print("   1. Execute: notebooks/1_exploratory_analysis.ipynb")
    print("   2. Execute: notebooks/2_model_training.ipynb")

if __name__ == "__main__":
    main() 