{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38146b48",
   "metadata": {},
   "source": [
    "# Preprocessamento dos Dados Meteorol√≥gicos INMET\n",
    "\n",
    "Este notebook realiza o preprocessamento dos dados hist√≥ricos do INMET para treinamento do modelo LSTM.\n",
    "\n",
    "## Objetivos:\n",
    "- Limpeza e normaliza√ß√£o dos dados\n",
    "- Tratamento de valores missing\n",
    "- Feature engineering\n",
    "- Cria√ß√£o de datasets de treino/valida√ß√£o/teste\n",
    "- Salvamento dos dados processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== PREPROCESSAMENTO DOS DADOS INMET ===\")\n",
    "print(f\"Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df51fc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configura√ß√£o dos caminhos\n",
    "DATA_PATH = Path('../data')\n",
    "RAW_DATA_PATH = DATA_PATH / 'raw' / 'dados_historicos'\n",
    "PROCESSED_DATA_PATH = DATA_PATH / 'processed'\n",
    "SCALERS_PATH = DATA_PATH / 'scalers'\n",
    "\n",
    "# Criar diret√≥rios se n√£o existirem\n",
    "PROCESSED_DATA_PATH.mkdir(exist_ok=True, parents=True)\n",
    "SCALERS_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Dados brutos: {RAW_DATA_PATH}\")\n",
    "print(f\"Dados processados: {PROCESSED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c49411",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_and_clean_inmet_data():\n",
    "    \"\"\"\n",
    "    Carrega e limpa dados brutos do INMET\n",
    "    \"\"\"\n",
    "    print(\"=== CARREGAMENTO E LIMPEZA DOS DADOS ===\")\n",
    "    \n",
    "    # Verificar arquivos dispon√≠veis\n",
    "    csv_files = list(RAW_DATA_PATH.glob(\"*.CSV\")) if RAW_DATA_PATH.exists() else []\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"‚ùå Nenhum arquivo CSV encontrado!\")\n",
    "        # Criar dados sint√©ticos para demonstra√ß√£o\n",
    "        dates = pd.date_range('2000-01-01', '2023-12-31', freq='H')\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        data = pd.DataFrame({\n",
    "            'timestamp': dates,\n",
    "            'precipitacao_mm': np.random.exponential(0.3, len(dates)),\n",
    "            'temperatura_c': 20 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 3, len(dates)),\n",
    "            'umidade_relativa': np.clip(50 + 30 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 8, len(dates)), 0, 100),\n",
    "            'pressao_mb': 1013 + np.random.normal(0, 15, len(dates)),\n",
    "            'velocidade_vento_ms': np.random.gamma(2, 1.5, len(dates)),\n",
    "            'direcao_vento_gr': np.random.uniform(0, 360, len(dates)),\n",
    "            'radiacao_kjm2': np.maximum(0, 800 * np.sin(2 * np.pi * np.arange(len(dates)) / 24) + np.random.normal(0, 150, len(dates)))\n",
    "        })\n",
    "        return data\n",
    "    \n",
    "    # Carregar dados reais do INMET\n",
    "    dataframes = []\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            print(f\"Processando: {file_path.name}\")\n",
    "            df = pd.read_csv(file_path, sep=';', encoding='latin1', skiprows=8)\n",
    "            df['arquivo_origem'] = file_path.name\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao processar {file_path.name}: {e}\")\n",
    "    \n",
    "    if dataframes:\n",
    "        data = pd.concat(dataframes, ignore_index=True)\n",
    "        print(f\"‚úì {len(dataframes)} arquivos processados\")\n",
    "        return data\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Carregar dados\n",
    "raw_data = load_and_clean_inmet_data()\n",
    "\n",
    "if raw_data is not None:\n",
    "    print(f\"Dados carregados: {raw_data.shape}\")\n",
    "else:\n",
    "    print(\"‚ùå Falha no carregamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6d820",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def standardize_columns(df):\n",
    "    \"\"\"\n",
    "    Padroniza nomes das colunas\n",
    "    \"\"\"\n",
    "    print(\"=== PADRONIZA√á√ÉO DE COLUNAS ===\")\n",
    "    \n",
    "    # Mapeamento de colunas para nomes padronizados\n",
    "    column_mapping = {\n",
    "        'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)': 'precipitacao_mm',\n",
    "        'TEMPERATURA DO AR - BULBO SECO, HORARIA (¬∞C)': 'temperatura_c',\n",
    "        'UMIDADE RELATIVA DO AR, HORARIA (%)': 'umidade_relativa',\n",
    "        'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)': 'pressao_mb',\n",
    "        'VENTO, VELOCIDADE HORARIA (m/s)': 'velocidade_vento_ms',\n",
    "        'VENTO, DIRE√á√ÉO HORARIA (gr) (¬∞ (gr))': 'direcao_vento_gr',\n",
    "        'RADIACAO GLOBAL (Kj/m¬≤)': 'radiacao_kjm2',\n",
    "        'Data': 'data',\n",
    "        'Hora UTC': 'hora'\n",
    "    }\n",
    "    \n",
    "    # Renomear colunas existentes\n",
    "    df_processed = df.copy()\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_processed.columns:\n",
    "            df_processed = df_processed.rename(columns={old_name: new_name})\n",
    "            print(f\"‚úì {old_name} ‚Üí {new_name}\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Padronizar colunas\n",
    "if raw_data is not None:\n",
    "    processed_data = standardize_columns(raw_data)\n",
    "    print(f\"Colunas ap√≥s padroniza√ß√£o: {list(processed_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac138ca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_datetime_column(df):\n",
    "    \"\"\"\n",
    "    Cria coluna datetime unificada\n",
    "    \"\"\"\n",
    "    print(\"=== CRIA√á√ÉO DE COLUNA DATETIME ===\")\n",
    "    \n",
    "    if 'timestamp' in df.columns:\n",
    "        print(\"‚úì Coluna timestamp j√° existe\")\n",
    "        return df\n",
    "    \n",
    "    # Tentar criar timestamp a partir de Data + Hora\n",
    "    if 'data' in df.columns and 'hora' in df.columns:\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['data'] + ' ' + df['hora'].astype(str))\n",
    "            print(\"‚úì Timestamp criado a partir de Data + Hora\")\n",
    "        except:\n",
    "            print(\"‚ùå Erro ao criar timestamp\")\n",
    "            return df\n",
    "    else:\n",
    "        print(\"‚ùå Colunas de data/hora n√£o encontradas\")\n",
    "        return df\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Criar coluna datetime\n",
    "if 'processed_data' in locals():\n",
    "    processed_data = create_datetime_column(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c658774",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Trata valores missing nos dados\n",
    "    \"\"\"\n",
    "    print(\"=== TRATAMENTO DE VALORES MISSING ===\")\n",
    "    \n",
    "    # Identificar colunas num√©ricas\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Estat√≠sticas de missing\n",
    "    missing_stats = []\n",
    "    for col in numeric_cols:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_percent = (missing_count / len(df)) * 100\n",
    "        missing_stats.append({\n",
    "            'coluna': col,\n",
    "            'missing_count': missing_count,\n",
    "            'missing_percent': missing_percent\n",
    "        })\n",
    "    \n",
    "    missing_df = pd.DataFrame(missing_stats).sort_values('missing_percent', ascending=False)\n",
    "    print(\"Estat√≠sticas de valores missing:\")\n",
    "    print(missing_df[missing_df['missing_count'] > 0])\n",
    "    \n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # Estrat√©gias de imputa√ß√£o por coluna\n",
    "    imputation_strategies = {\n",
    "        'precipitacao_mm': 0,  # Precipita√ß√£o missing = 0\n",
    "        'temperatura_c': 'interpolate',\n",
    "        'umidade_relativa': 'interpolate',\n",
    "        'pressao_mb': 'mean',\n",
    "        'velocidade_vento_ms': 'median',\n",
    "        'direcao_vento_gr': 'forward_fill',\n",
    "        'radiacao_kjm2': 'interpolate'\n",
    "    }\n",
    "    \n",
    "    for col, strategy in imputation_strategies.items():\n",
    "        if col in df_cleaned.columns:\n",
    "            missing_before = df_cleaned[col].isnull().sum()\n",
    "            \n",
    "            if strategy == 'interpolate':\n",
    "                df_cleaned[col] = df_cleaned[col].interpolate(method='linear')\n",
    "            elif strategy == 'mean':\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())\n",
    "            elif strategy == 'median':\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "            elif strategy == 'forward_fill':\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(method='ffill')\n",
    "            elif isinstance(strategy, (int, float)):\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(strategy)\n",
    "            \n",
    "            missing_after = df_cleaned[col].isnull().sum()\n",
    "            print(f\"‚úì {col}: {missing_before} ‚Üí {missing_after} missing\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Tratar valores missing\n",
    "if 'processed_data' in locals():\n",
    "    processed_data = handle_missing_values(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376977c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Cria features derivadas\n",
    "    \"\"\"\n",
    "    print(\"=== FEATURE ENGINEERING ===\")\n",
    "    \n",
    "    if 'timestamp' not in df.columns:\n",
    "        print(\"‚ùå Coluna timestamp necess√°ria\")\n",
    "        return df\n",
    "    \n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Features temporais\n",
    "    df_features['ano'] = df_features['timestamp'].dt.year\n",
    "    df_features['mes'] = df_features['timestamp'].dt.month\n",
    "    df_features['dia'] = df_features['timestamp'].dt.day\n",
    "    df_features['hora'] = df_features['timestamp'].dt.hour\n",
    "    df_features['dia_semana'] = df_features['timestamp'].dt.dayofweek\n",
    "    df_features['dia_ano'] = df_features['timestamp'].dt.dayofyear\n",
    "    \n",
    "    # Features c√≠clicas\n",
    "    df_features['hora_sin'] = np.sin(2 * np.pi * df_features['hora'] / 24)\n",
    "    df_features['hora_cos'] = np.cos(2 * np.pi * df_features['hora'] / 24)\n",
    "    df_features['mes_sin'] = np.sin(2 * np.pi * df_features['mes'] / 12)\n",
    "    df_features['mes_cos'] = np.cos(2 * np.pi * df_features['mes'] / 12)\n",
    "    \n",
    "    # Features meteorol√≥gicas derivadas\n",
    "    if 'temperatura_c' in df_features.columns and 'umidade_relativa' in df_features.columns:\n",
    "        # √çndice de calor aproximado\n",
    "        df_features['indice_calor'] = df_features['temperatura_c'] + 0.1 * df_features['umidade_relativa']\n",
    "    \n",
    "    if 'velocidade_vento_ms' in df_features.columns and 'temperatura_c' in df_features.columns:\n",
    "        # Wind chill aproximado\n",
    "        df_features['sensacao_termica'] = df_features['temperatura_c'] - 2 * df_features['velocidade_vento_ms']\n",
    "    \n",
    "    # Features de agrega√ß√£o temporal (moving averages)\n",
    "    window_sizes = [3, 6, 12, 24]  # 3h, 6h, 12h, 24h\n",
    "    \n",
    "    for col in ['precipitacao_mm', 'temperatura_c', 'umidade_relativa']:\n",
    "        if col in df_features.columns:\n",
    "            for window in window_sizes:\n",
    "                df_features[f'{col}_ma_{window}h'] = df_features[col].rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    print(f\"‚úì Features criadas. Total de colunas: {len(df_features.columns)}\")\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Criar features\n",
    "if 'processed_data' in locals():\n",
    "    processed_data = create_features(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a05e85",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def split_temporal_data(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Divide dados em treino/valida√ß√£o/teste de forma temporal\n",
    "    \"\"\"\n",
    "    print(\"=== DIVIS√ÉO TEMPORAL DOS DADOS ===\")\n",
    "    \n",
    "    if 'timestamp' not in df.columns:\n",
    "        print(\"‚ùå Coluna timestamp necess√°ria\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Ordenar por timestamp\n",
    "    df_sorted = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Calcular pontos de divis√£o\n",
    "    n_total = len(df_sorted)\n",
    "    train_end = int(n_total * train_ratio)\n",
    "    val_end = int(n_total * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Dividir datasets\n",
    "    train_data = df_sorted.iloc[:train_end].copy()\n",
    "    val_data = df_sorted.iloc[train_end:val_end].copy()\n",
    "    test_data = df_sorted.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"Divis√£o dos dados:\")\n",
    "    print(f\"  Treino: {len(train_data)} registros ({len(train_data)/n_total*100:.1f}%)\")\n",
    "    print(f\"  Valida√ß√£o: {len(val_data)} registros ({len(val_data)/n_total*100:.1f}%)\")\n",
    "    print(f\"  Teste: {len(test_data)} registros ({len(test_data)/n_total*100:.1f}%)\")\n",
    "    \n",
    "    if 'timestamp' in train_data.columns:\n",
    "        print(f\"  Per√≠odo treino: {train_data['timestamp'].min()} - {train_data['timestamp'].max()}\")\n",
    "        print(f\"  Per√≠odo valida√ß√£o: {val_data['timestamp'].min()} - {val_data['timestamp'].max()}\")\n",
    "        print(f\"  Per√≠odo teste: {test_data['timestamp'].min()} - {test_data['timestamp'].max()}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Dividir dados\n",
    "if 'processed_data' in locals():\n",
    "    train_data, val_data, test_data = split_temporal_data(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(train_df, val_df, test_df):\n",
    "    \"\"\"\n",
    "    Salva dados processados\n",
    "    \"\"\"\n",
    "    print(\"=== SALVAMENTO DOS DADOS ===\")\n",
    "    \n",
    "    # Salvar em formato parquet (mais eficiente)\n",
    "    train_df.to_parquet(PROCESSED_DATA_PATH / 'train_data.parquet', index=False)\n",
    "    val_df.to_parquet(PROCESSED_DATA_PATH / 'validation_data.parquet', index=False)\n",
    "    test_df.to_parquet(PROCESSED_DATA_PATH / 'test_data.parquet', index=False)\n",
    "    \n",
    "    print(f\"‚úì Dados salvos em {PROCESSED_DATA_PATH}\")\n",
    "    print(f\"  - train_data.parquet: {len(train_df)} registros\")\n",
    "    print(f\"  - validation_data.parquet: {len(val_df)} registros\")\n",
    "    print(f\"  - test_data.parquet: {len(test_df)} registros\")\n",
    "    \n",
    "    # Salvar metadados\n",
    "    metadata = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'train_shape': train_df.shape,\n",
    "        'val_shape': val_df.shape,\n",
    "        'test_shape': test_df.shape,\n",
    "        'columns': list(train_df.columns),\n",
    "        'train_period': f\"{train_df['timestamp'].min()} - {train_df['timestamp'].max()}\" if 'timestamp' in train_df.columns else 'N/A',\n",
    "        'val_period': f\"{val_df['timestamp'].min()} - {val_df['timestamp'].max()}\" if 'timestamp' in val_df.columns else 'N/A',\n",
    "        'test_period': f\"{test_df['timestamp'].min()} - {test_df['timestamp'].max()}\" if 'timestamp' in test_df.columns else 'N/A'\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(PROCESSED_DATA_PATH / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"‚úì Metadados salvos\")\n",
    "\n",
    "# Salvar dados processados\n",
    "if all(v is not None for v in [train_data, val_data, test_data]):\n",
    "    save_processed_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"‚úÖ PREPROCESSAMENTO CONCLU√çDO!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Pr√≥ximos passos:\")\n",
    "print(\"1. üß† Treinar modelo LSTM\")\n",
    "print(\"2. üìä Avaliar performance\")\n",
    "print(\"3. üîß Ajustar hiperpar√¢metros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bba3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
