{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38146b48",
   "metadata": {},
   "source": [
    "# Preprocessamento dos Dados Meteorológicos INMET\n",
    "\n",
    "Este notebook realiza o preprocessamento dos dados históricos do INMET para treinamento do modelo LSTM.\n",
    "\n",
    "## Objetivos:\n",
    "- Limpeza e normalização dos dados\n",
    "- Tratamento de valores missing\n",
    "- Feature engineering\n",
    "- Criação de datasets de treino/validação/teste\n",
    "- Salvamento dos dados processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessários\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== PREPROCESSAMENTO DOS DADOS INMET ===\")\n",
    "print(f\"Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df51fc1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configuração dos caminhos\n",
    "DATA_PATH = Path('../data')\n",
    "RAW_DATA_PATH = DATA_PATH / 'raw' / 'dados_historicos'\n",
    "PROCESSED_DATA_PATH = DATA_PATH / 'processed'\n",
    "SCALERS_PATH = DATA_PATH / 'scalers'\n",
    "\n",
    "# Criar diretórios se não existirem\n",
    "PROCESSED_DATA_PATH.mkdir(exist_ok=True, parents=True)\n",
    "SCALERS_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Dados brutos: {RAW_DATA_PATH}\")\n",
    "print(f\"Dados processados: {PROCESSED_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c49411",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_and_clean_inmet_data():\n",
    "    \"\"\"\n",
    "    Carrega e limpa dados brutos do INMET\n",
    "    \"\"\"\n",
    "    print(\"=== CARREGAMENTO E LIMPEZA DOS DADOS ===\")\n",
    "    \n",
    "    # Verificar arquivos disponíveis\n",
    "    csv_files = list(RAW_DATA_PATH.glob(\"*.CSV\")) if RAW_DATA_PATH.exists() else []\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"❌ Nenhum arquivo CSV encontrado!\")\n",
    "        # Criar dados sintéticos para demonstração\n",
    "        dates = pd.date_range('2000-01-01', '2023-12-31', freq='H')\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        data = pd.DataFrame({\n",
    "            'timestamp': dates,\n",
    "            'precipitacao_mm': np.random.exponential(0.3, len(dates)),\n",
    "            'temperatura_c': 20 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 3, len(dates)),\n",
    "            'umidade_relativa': np.clip(50 + 30 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 8, len(dates)), 0, 100),\n",
    "            'pressao_mb': 1013 + np.random.normal(0, 15, len(dates)),\n",
    "            'velocidade_vento_ms': np.random.gamma(2, 1.5, len(dates)),\n",
    "            'direcao_vento_gr': np.random.uniform(0, 360, len(dates)),\n",
    "            'radiacao_kjm2': np.maximum(0, 800 * np.sin(2 * np.pi * np.arange(len(dates)) / 24) + np.random.normal(0, 150, len(dates)))\n",
    "        })\n",
    "        return data\n",
    "    \n",
    "    # Carregar dados reais do INMET\n",
    "    dataframes = []\n",
    "    \n",
    "    for file_path in csv_files:\n",
    "        try:\n",
    "            print(f\"Processando: {file_path.name}\")\n",
    "            df = pd.read_csv(file_path, sep=';', encoding='latin1', skiprows=8)\n",
    "            df['arquivo_origem'] = file_path.name\n",
    "            dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao processar {file_path.name}: {e}\")\n",
    "    \n",
    "    if dataframes:\n",
    "        data = pd.concat(dataframes, ignore_index=True)\n",
    "        print(f\"✓ {len(dataframes)} arquivos processados\")\n",
    "        return data\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Carregar dados\n",
    "raw_data = load_and_clean_inmet_data()\n",
    "\n",
    "if raw_data is not None:\n",
    "    print(f\"Dados carregados: {raw_data.shape}\")\n",
    "else:\n",
    "    print(\"❌ Falha no carregamento\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d6d820",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def standardize_columns(df):\n",
    "    \"\"\"\n",
    "    Padroniza nomes das colunas\n",
    "    \"\"\"\n",
    "    print(\"=== PADRONIZAÇÃO DE COLUNAS ===\")\n",
    "    \n",
    "    # Mapeamento de colunas para nomes padronizados\n",
    "    column_mapping = {\n",
    "        'PRECIPITAÇÃO TOTAL, HORÁRIO (mm)': 'precipitacao_mm',\n",
    "        'TEMPERATURA DO AR - BULBO SECO, HORARIA (°C)': 'temperatura_c',\n",
    "        'UMIDADE RELATIVA DO AR, HORARIA (%)': 'umidade_relativa',\n",
    "        'PRESSAO ATMOSFERICA AO NIVEL DA ESTACAO, HORARIA (mB)': 'pressao_mb',\n",
    "        'VENTO, VELOCIDADE HORARIA (m/s)': 'velocidade_vento_ms',\n",
    "        'VENTO, DIREÇÃO HORARIA (gr) (° (gr))': 'direcao_vento_gr',\n",
    "        'RADIACAO GLOBAL (Kj/m²)': 'radiacao_kjm2',\n",
    "        'Data': 'data',\n",
    "        'Hora UTC': 'hora'\n",
    "    }\n",
    "    \n",
    "    # Renomear colunas existentes\n",
    "    df_processed = df.copy()\n",
    "    for old_name, new_name in column_mapping.items():\n",
    "        if old_name in df_processed.columns:\n",
    "            df_processed = df_processed.rename(columns={old_name: new_name})\n",
    "            print(f\"✓ {old_name} → {new_name}\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "# Padronizar colunas\n",
    "if raw_data is not None:\n",
    "    processed_data = standardize_columns(raw_data)\n",
    "    print(f\"Colunas após padronização: {list(processed_data.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac138ca",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_datetime_column(df):\n",
    "    \"\"\"\n",
    "    Cria coluna datetime unificada\n",
    "    \"\"\"\n",
    "    print(\"=== CRIAÇÃO DE COLUNA DATETIME ===\")\n",
    "    \n",
    "    if 'timestamp' in df.columns:\n",
    "        print(\"✓ Coluna timestamp já existe\")\n",
    "        return df\n",
    "    \n",
    "    # Tentar criar timestamp a partir de Data + Hora\n",
    "    if 'data' in df.columns and 'hora' in df.columns:\n",
    "        try:\n",
    "            df['timestamp'] = pd.to_datetime(df['data'] + ' ' + df['hora'].astype(str))\n",
    "            print(\"✓ Timestamp criado a partir de Data + Hora\")\n",
    "        except:\n",
    "            print(\"❌ Erro ao criar timestamp\")\n",
    "            return df\n",
    "    else:\n",
    "        print(\"❌ Colunas de data/hora não encontradas\")\n",
    "        return df\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Criar coluna datetime\n",
    "if 'processed_data' in locals():\n",
    "    processed_data = create_datetime_column(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c658774",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Trata valores missing nos dados\n",
    "    \"\"\"\n",
    "    print(\"=== TRATAMENTO DE VALORES MISSING ===\")\n",
    "    \n",
    "    # Identificar colunas numéricas\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Estatísticas de missing\n",
    "    missing_stats = []\n",
    "    for col in numeric_cols:\n",
    "        missing_count = df[col].isnull().sum()\n",
    "        missing_percent = (missing_count / len(df)) * 100\n",
    "        missing_stats.append({\n",
    "            'coluna': col,\n",
    "            'missing_count': missing_count,\n",
    "            'missing_percent': missing_percent\n",
    "        })\n",
    "    \n",
    "    missing_df = pd.DataFrame(missing_stats).sort_values('missing_percent', ascending=False)\n",
    "    print(\"Estatísticas de valores missing:\")\n",
    "    print(missing_df[missing_df['missing_count'] > 0])\n",
    "    \n",
    "    df_cleaned = df.copy()\n",
    "    \n",
    "    # Estratégias de imputação por coluna\n",
    "    imputation_strategies = {\n",
    "        'precipitacao_mm': 0,  # Precipitação missing = 0\n",
    "        'temperatura_c': 'interpolate',\n",
    "        'umidade_relativa': 'interpolate',\n",
    "        'pressao_mb': 'mean',\n",
    "        'velocidade_vento_ms': 'median',\n",
    "        'direcao_vento_gr': 'forward_fill',\n",
    "        'radiacao_kjm2': 'interpolate'\n",
    "    }\n",
    "    \n",
    "    for col, strategy in imputation_strategies.items():\n",
    "        if col in df_cleaned.columns:\n",
    "            missing_before = df_cleaned[col].isnull().sum()\n",
    "            \n",
    "            if strategy == 'interpolate':\n",
    "                df_cleaned[col] = df_cleaned[col].interpolate(method='linear')\n",
    "            elif strategy == 'mean':\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())\n",
    "            elif strategy == 'median':\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].median())\n",
    "            elif strategy == 'forward_fill':\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(method='ffill')\n",
    "            elif isinstance(strategy, (int, float)):\n",
    "                df_cleaned[col] = df_cleaned[col].fillna(strategy)\n",
    "            \n",
    "            missing_after = df_cleaned[col].isnull().sum()\n",
    "            print(f\"✓ {col}: {missing_before} → {missing_after} missing\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# Tratar valores missing\n",
    "if 'processed_data' in locals():\n",
    "    processed_data = handle_missing_values(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376977c3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"\n",
    "    Cria features derivadas\n",
    "    \"\"\"\n",
    "    print(\"=== FEATURE ENGINEERING ===\")\n",
    "    \n",
    "    if 'timestamp' not in df.columns:\n",
    "        print(\"❌ Coluna timestamp necessária\")\n",
    "        return df\n",
    "    \n",
    "    df_features = df.copy()\n",
    "    \n",
    "    # Features temporais\n",
    "    df_features['ano'] = df_features['timestamp'].dt.year\n",
    "    df_features['mes'] = df_features['timestamp'].dt.month\n",
    "    df_features['dia'] = df_features['timestamp'].dt.day\n",
    "    df_features['hora'] = df_features['timestamp'].dt.hour\n",
    "    df_features['dia_semana'] = df_features['timestamp'].dt.dayofweek\n",
    "    df_features['dia_ano'] = df_features['timestamp'].dt.dayofyear\n",
    "    \n",
    "    # Features cíclicas\n",
    "    df_features['hora_sin'] = np.sin(2 * np.pi * df_features['hora'] / 24)\n",
    "    df_features['hora_cos'] = np.cos(2 * np.pi * df_features['hora'] / 24)\n",
    "    df_features['mes_sin'] = np.sin(2 * np.pi * df_features['mes'] / 12)\n",
    "    df_features['mes_cos'] = np.cos(2 * np.pi * df_features['mes'] / 12)\n",
    "    \n",
    "    # Features meteorológicas derivadas\n",
    "    if 'temperatura_c' in df_features.columns and 'umidade_relativa' in df_features.columns:\n",
    "        # Índice de calor aproximado\n",
    "        df_features['indice_calor'] = df_features['temperatura_c'] + 0.1 * df_features['umidade_relativa']\n",
    "    \n",
    "    if 'velocidade_vento_ms' in df_features.columns and 'temperatura_c' in df_features.columns:\n",
    "        # Wind chill aproximado\n",
    "        df_features['sensacao_termica'] = df_features['temperatura_c'] - 2 * df_features['velocidade_vento_ms']\n",
    "    \n",
    "    # Features de agregação temporal (moving averages)\n",
    "    window_sizes = [3, 6, 12, 24]  # 3h, 6h, 12h, 24h\n",
    "    \n",
    "    for col in ['precipitacao_mm', 'temperatura_c', 'umidade_relativa']:\n",
    "        if col in df_features.columns:\n",
    "            for window in window_sizes:\n",
    "                df_features[f'{col}_ma_{window}h'] = df_features[col].rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    print(f\"✓ Features criadas. Total de colunas: {len(df_features.columns)}\")\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Criar features\n",
    "if 'processed_data' in locals():\n",
    "    processed_data = create_features(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a05e85",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def split_temporal_data(df, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
    "    \"\"\"\n",
    "    Divide dados em treino/validação/teste de forma temporal\n",
    "    \"\"\"\n",
    "    print(\"=== DIVISÃO TEMPORAL DOS DADOS ===\")\n",
    "    \n",
    "    if 'timestamp' not in df.columns:\n",
    "        print(\"❌ Coluna timestamp necessária\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # Ordenar por timestamp\n",
    "    df_sorted = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    \n",
    "    # Calcular pontos de divisão\n",
    "    n_total = len(df_sorted)\n",
    "    train_end = int(n_total * train_ratio)\n",
    "    val_end = int(n_total * (train_ratio + val_ratio))\n",
    "    \n",
    "    # Dividir datasets\n",
    "    train_data = df_sorted.iloc[:train_end].copy()\n",
    "    val_data = df_sorted.iloc[train_end:val_end].copy()\n",
    "    test_data = df_sorted.iloc[val_end:].copy()\n",
    "    \n",
    "    print(f\"Divisão dos dados:\")\n",
    "    print(f\"  Treino: {len(train_data)} registros ({len(train_data)/n_total*100:.1f}%)\")\n",
    "    print(f\"  Validação: {len(val_data)} registros ({len(val_data)/n_total*100:.1f}%)\")\n",
    "    print(f\"  Teste: {len(test_data)} registros ({len(test_data)/n_total*100:.1f}%)\")\n",
    "    \n",
    "    if 'timestamp' in train_data.columns:\n",
    "        print(f\"  Período treino: {train_data['timestamp'].min()} - {train_data['timestamp'].max()}\")\n",
    "        print(f\"  Período validação: {val_data['timestamp'].min()} - {val_data['timestamp'].max()}\")\n",
    "        print(f\"  Período teste: {test_data['timestamp'].min()} - {test_data['timestamp'].max()}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "# Dividir dados\n",
    "if 'processed_data' in locals():\n",
    "    train_data, val_data, test_data = split_temporal_data(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_processed_data(train_df, val_df, test_df):\n",
    "    \"\"\"\n",
    "    Salva dados processados\n",
    "    \"\"\"\n",
    "    print(\"=== SALVAMENTO DOS DADOS ===\")\n",
    "    \n",
    "    # Salvar em formato parquet (mais eficiente)\n",
    "    train_df.to_parquet(PROCESSED_DATA_PATH / 'train_data.parquet', index=False)\n",
    "    val_df.to_parquet(PROCESSED_DATA_PATH / 'validation_data.parquet', index=False)\n",
    "    test_df.to_parquet(PROCESSED_DATA_PATH / 'test_data.parquet', index=False)\n",
    "    \n",
    "    print(f\"✓ Dados salvos em {PROCESSED_DATA_PATH}\")\n",
    "    print(f\"  - train_data.parquet: {len(train_df)} registros\")\n",
    "    print(f\"  - validation_data.parquet: {len(val_df)} registros\")\n",
    "    print(f\"  - test_data.parquet: {len(test_df)} registros\")\n",
    "    \n",
    "    # Salvar metadados\n",
    "    metadata = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'train_shape': train_df.shape,\n",
    "        'val_shape': val_df.shape,\n",
    "        'test_shape': test_df.shape,\n",
    "        'columns': list(train_df.columns),\n",
    "        'train_period': f\"{train_df['timestamp'].min()} - {train_df['timestamp'].max()}\" if 'timestamp' in train_df.columns else 'N/A',\n",
    "        'val_period': f\"{val_df['timestamp'].min()} - {val_df['timestamp'].max()}\" if 'timestamp' in val_df.columns else 'N/A',\n",
    "        'test_period': f\"{test_df['timestamp'].min()} - {test_df['timestamp'].max()}\" if 'timestamp' in test_df.columns else 'N/A'\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(PROCESSED_DATA_PATH / 'metadata.json', 'w') as f:\n",
    "        json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"✓ Metadados salvos\")\n",
    "\n",
    "# Salvar dados processados\n",
    "if all(v is not None for v in [train_data, val_data, test_data]):\n",
    "    save_processed_data(train_data, val_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3805d762",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✅ PREPROCESSAMENTO CONCLUÍDO!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Próximos passos:\")\n",
    "print(\"1. 🧠 Treinar modelo LSTM\")\n",
    "print(\"2. 📊 Avaliar performance\")\n",
    "print(\"3. 🔧 Ajustar hiperparâmetros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bba3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
