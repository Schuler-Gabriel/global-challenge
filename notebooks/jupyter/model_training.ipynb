{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e0da8dd",
   "metadata": {},
   "source": [
    "# Treinamento do Modelo LSTM - Previsão Meteorológica\n",
    "\n",
    "Este notebook implementa a arquitetura LSTM para previsão de chuva baseada nos dados históricos do INMET (2000-2025).\n",
    "\n",
    "## Objetivos:\n",
    "- Arquitetura LSTM multivariada (16+ features)\n",
    "- Sequence length otimizado para dados horários\n",
    "- Previsão de precipitação para 24h à frente\n",
    "- Accuracy > 75% em classificação de eventos de chuva\n",
    "- MAE < 2.0 mm/h para precipitação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81062b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessários\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, RepeatVector, TimeDistributed\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score, classification_report\n",
    "\n",
    "# Configurações\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0773666c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações do modelo baseadas na documentação\n",
    "MODEL_CONFIG = {\n",
    "    'sequence_length': 24,  # 24 horas de histórico\n",
    "    'forecast_horizon': 24,  # Previsão para 24h à frente\n",
    "    'features_count': 16,    # Variáveis meteorológicas disponíveis\n",
    "    'lstm_units': [128, 64, 32],  # Configuração de camadas\n",
    "    'dropout_rate': 0.2,\n",
    "    'learning_rate': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 100,\n",
    "    'patience': 15\n",
    "}\n",
    "\n",
    "# Features principais dos dados INMET\n",
    "FEATURE_COLUMNS = [\n",
    "    'precipitacao_mm',\n",
    "    'pressao_mb', \n",
    "    'temperatura_c',\n",
    "    'ponto_orvalho_c',\n",
    "    'umidade_relativa',\n",
    "    'velocidade_vento_ms',\n",
    "    'direcao_vento_gr',\n",
    "    'radiacao_kjm2',\n",
    "    'pressao_max_mb',\n",
    "    'pressao_min_mb',\n",
    "    'temperatura_max_c',\n",
    "    'temperatura_min_c',\n",
    "    'umidade_max',\n",
    "    'umidade_min',\n",
    "    'ponto_orvalho_max_c',\n",
    "    'ponto_orvalho_min_c'\n",
    "]\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = Path('../data/processed')\n",
    "MODEL_PATH = Path('../data/modelos_treinados')\n",
    "MODEL_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Configuração do modelo: {MODEL_CONFIG}\")\n",
    "print(f\"Features disponíveis: {len(FEATURE_COLUMNS)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f226ee23",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Preparação dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ba5c4c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Carregar dados processados\n",
    "print(\"Carregando dados de treinamento...\")\n",
    "train_data = pd.read_parquet(DATA_PATH / 'train_data.parquet')\n",
    "val_data = pd.read_parquet(DATA_PATH / 'validation_data.parquet')\n",
    "test_data = pd.read_parquet(DATA_PATH / 'test_data.parquet')\n",
    "\n",
    "print(f\"Train shape: {train_data.shape}\")\n",
    "print(f\"Validation shape: {val_data.shape}\")\n",
    "print(f\"Test shape: {test_data.shape}\")\n",
    "\n",
    "# Verificar colunas disponíveis\n",
    "print(f\"\\nColunas disponíveis: {train_data.columns.tolist()}\")\n",
    "\n",
    "# Ajustar FEATURE_COLUMNS baseado nas colunas reais\n",
    "available_features = [col for col in FEATURE_COLUMNS if col in train_data.columns]\n",
    "if len(available_features) < len(FEATURE_COLUMNS):\n",
    "    print(f\"\\nAjustando features disponíveis: {len(available_features)} de {len(FEATURE_COLUMNS)}\")\n",
    "    FEATURE_COLUMNS = available_features\n",
    "    MODEL_CONFIG['features_count'] = len(FEATURE_COLUMNS)\n",
    "\n",
    "print(f\"\\nFeatures finais para o modelo: {FEATURE_COLUMNS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2d0738",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences(data, feature_cols, target_col, sequence_length, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Prepara sequências temporais para treinamento do LSTM\n",
    "    \n",
    "    Args:\n",
    "        data: DataFrame com dados temporais\n",
    "        feature_cols: Lista de colunas de features\n",
    "        target_col: Nome da coluna target\n",
    "        sequence_length: Comprimento da sequência de entrada\n",
    "        forecast_horizon: Horizonte de previsão\n",
    "    \n",
    "    Returns:\n",
    "        X: Sequências de features\n",
    "        y: Targets correspondentes\n",
    "    \"\"\"\n",
    "    # Ordenar por timestamp se disponível\n",
    "    if 'timestamp' in data.columns:\n",
    "        data = data.sort_values('timestamp')\n",
    "    \n",
    "    # Extrair features e target\n",
    "    features = data[feature_cols].values\n",
    "    target = data[target_col].values\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length - forecast_horizon + 1):\n",
    "        # Sequência de entrada (últimas sequence_length horas)\n",
    "        X.append(features[i:(i + sequence_length)])\n",
    "        \n",
    "        # Target (precipitação daqui a forecast_horizon horas)\n",
    "        target_idx = i + sequence_length + forecast_horizon - 1\n",
    "        y.append(target[target_idx])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Determinar coluna target (precipitação)\n",
    "target_column = None\n",
    "for col in train_data.columns:\n",
    "    if 'precipitacao' in col.lower() or 'chuva' in col.lower():\n",
    "        target_column = col\n",
    "        break\n",
    "\n",
    "if target_column is None:\n",
    "    # Usar primeira coluna de feature como target temporário\n",
    "    target_column = FEATURE_COLUMNS[0]\n",
    "    print(f\"ATENÇÃO: Usando {target_column} como target temporário\")\n",
    "\n",
    "print(f\"Target column: {target_column}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7e73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar sequências para treinamento\n",
    "print(\"Preparando sequências temporais...\")\n",
    "\n",
    "X_train, y_train = prepare_sequences(\n",
    "    train_data, \n",
    "    FEATURE_COLUMNS, \n",
    "    target_column,\n",
    "    MODEL_CONFIG['sequence_length'],\n",
    "    MODEL_CONFIG['forecast_horizon']\n",
    ")\n",
    "\n",
    "X_val, y_val = prepare_sequences(\n",
    "    val_data, \n",
    "    FEATURE_COLUMNS, \n",
    "    target_column,\n",
    "    MODEL_CONFIG['sequence_length'],\n",
    "    MODEL_CONFIG['forecast_horizon']\n",
    ")\n",
    "\n",
    "X_test, y_test = prepare_sequences(\n",
    "    test_data, \n",
    "    FEATURE_COLUMNS, \n",
    "    target_column,\n",
    "    MODEL_CONFIG['sequence_length'],\n",
    "    MODEL_CONFIG['forecast_horizon']\n",
    ")\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"X_val shape: {X_val.shape}\")\n",
    "print(f\"y_val shape: {y_val.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Estatísticas do target\n",
    "print(f\"\\nEstatísticas do target (precipitação):\")\n",
    "print(f\"Train - Mean: {y_train.mean():.3f}, Std: {y_train.std():.3f}, Max: {y_train.max():.3f}\")\n",
    "print(f\"Val - Mean: {y_val.mean():.3f}, Std: {y_val.std():.3f}, Max: {y_val.max():.3f}\")\n",
    "print(f\"Test - Mean: {y_test.mean():.3f}, Std: {y_test.std():.3f}, Max: {y_test.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6439f2",
   "metadata": {},
   "source": [
    "## 2. Normalização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d7e86",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Normalização das features\n",
    "print(\"Normalizando features...\")\n",
    "\n",
    "# Reshape para normalização\n",
    "X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "\n",
    "# Normalizar features\n",
    "feature_scaler = StandardScaler()\n",
    "X_train_scaled = feature_scaler.fit_transform(X_train_reshaped)\n",
    "X_val_scaled = feature_scaler.transform(X_val_reshaped)\n",
    "X_test_scaled = feature_scaler.transform(X_test_reshaped)\n",
    "\n",
    "# Reshape de volta\n",
    "X_train_scaled = X_train_scaled.reshape(X_train.shape)\n",
    "X_val_scaled = X_val_scaled.reshape(X_val.shape)\n",
    "X_test_scaled = X_test_scaled.reshape(X_test.shape)\n",
    "\n",
    "# Normalizar target (precipitação)\n",
    "target_scaler = StandardScaler()\n",
    "y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "print(f\"Features normalizadas - Shape: {X_train_scaled.shape}\")\n",
    "print(f\"Target normalizado - Mean: {y_train_scaled.mean():.3f}, Std: {y_train_scaled.std():.3f}\")\n",
    "\n",
    "# Salvar scalers\n",
    "import joblib\n",
    "joblib.dump(feature_scaler, MODEL_PATH / 'feature_scaler.pkl')\n",
    "joblib.dump(target_scaler, MODEL_PATH / 'target_scaler.pkl')\n",
    "print(\"Scalers salvos!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759d932",
   "metadata": {},
   "source": [
    "## 3. Arquitetura do Modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd55bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_model(sequence_length, features_count, lstm_units, dropout_rate):\n",
    "    \"\"\"\n",
    "    Cria modelo LSTM para previsão de séries temporais\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: Comprimento da sequência de entrada\n",
    "        features_count: Número de features\n",
    "        lstm_units: Lista com número de unidades por camada LSTM\n",
    "        dropout_rate: Taxa de dropout\n",
    "    \n",
    "    Returns:\n",
    "        model: Modelo Keras compilado\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Primeira camada LSTM\n",
    "    model.add(LSTM(\n",
    "        lstm_units[0],\n",
    "        return_sequences=len(lstm_units) > 1,\n",
    "        input_shape=(sequence_length, features_count),\n",
    "        name='lstm_1'\n",
    "    ))\n",
    "    model.add(Dropout(dropout_rate, name='dropout_1'))\n",
    "    \n",
    "    # Camadas LSTM adicionais\n",
    "    for i, units in enumerate(lstm_units[1:], 2):\n",
    "        return_sequences = i < len(lstm_units)\n",
    "        model.add(LSTM(\n",
    "            units,\n",
    "            return_sequences=return_sequences,\n",
    "            name=f'lstm_{i}'\n",
    "        ))\n",
    "        model.add(Dropout(dropout_rate, name=f'dropout_{i}'))\n",
    "    \n",
    "    # Camada densa final\n",
    "    model.add(Dense(50, activation='relu', name='dense_1'))\n",
    "    model.add(Dropout(dropout_rate, name='dropout_final'))\n",
    "    model.add(Dense(1, name='output'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Criar modelo principal\n",
    "print(\"Criando modelo LSTM...\")\n",
    "model = create_lstm_model(\n",
    "    MODEL_CONFIG['sequence_length'],\n",
    "    MODEL_CONFIG['features_count'],\n",
    "    MODEL_CONFIG['lstm_units'],\n",
    "    MODEL_CONFIG['dropout_rate']\n",
    ")\n",
    "\n",
    "# Compilar modelo\n",
    "optimizer = Adam(learning_rate=MODEL_CONFIG['learning_rate'])\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Resumo do modelo\n",
    "model.summary()\n",
    "\n",
    "# Plotar arquitetura\n",
    "plot_model(\n",
    "    model, \n",
    "    to_file=MODEL_PATH / 'model_architecture.png',\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True\n",
    ")\n",
    "print(f\"\\nArquitetura salva em: {MODEL_PATH / 'model_architecture.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70458e9",
   "metadata": {},
   "source": [
    "## 4. Configuração de Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835815e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar callbacks\n",
    "callbacks = [\n",
    "    # Early Stopping\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=MODEL_CONFIG['patience'],\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce Learning Rate\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Model Checkpoint\n",
    "    ModelCheckpoint(\n",
    "        filepath=str(MODEL_PATH / 'best_model.h5'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard\n",
    "    TensorBoard(\n",
    "        log_dir=str(MODEL_PATH / 'tensorboard_logs'),\n",
    "        histogram_freq=1,\n",
    "        write_graph=True,\n",
    "        write_images=True\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configurados:\")\n",
    "for callback in callbacks:\n",
    "    print(f\"  - {callback.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95620137",
   "metadata": {},
   "source": [
    "## 5. Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846b3779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento\n",
    "print(\"Iniciando treinamento...\")\n",
    "print(f\"Epochs: {MODEL_CONFIG['epochs']}\")\n",
    "print(f\"Batch size: {MODEL_CONFIG['batch_size']}\")\n",
    "print(f\"Dados de treino: {X_train_scaled.shape[0]} samples\")\n",
    "print(f\"Dados de validação: {X_val_scaled.shape[0]} samples\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_val_scaled, y_val_scaled),\n",
    "    epochs=MODEL_CONFIG['epochs'],\n",
    "    batch_size=MODEL_CONFIG['batch_size'],\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTreinamento concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aebbfa",
   "metadata": {},
   "source": [
    "## 6. Visualização do Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2244f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar histórico de treinamento\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss\n",
    "axes[0, 0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0, 0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0, 0].set_title('Model Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# MAE\n",
    "axes[0, 1].plot(history.history['mae'], label='Training MAE')\n",
    "axes[0, 1].plot(history.history['val_mae'], label='Validation MAE')\n",
    "axes[0, 1].set_title('Model MAE')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('MAE')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Learning Rate (se disponível)\n",
    "if 'lr' in history.history:\n",
    "    axes[1, 0].plot(history.history['lr'])\n",
    "    axes[1, 0].set_title('Learning Rate')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    axes[1, 0].grid(True)\n",
    "else:\n",
    "    axes[1, 0].text(0.5, 0.5, 'Learning Rate\\nhistory not available', \n",
    "                    ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "\n",
    "# Comparação final\n",
    "final_train_loss = history.history['loss'][-1]\n",
    "final_val_loss = history.history['val_loss'][-1]\n",
    "final_train_mae = history.history['mae'][-1]\n",
    "final_val_mae = history.history['val_mae'][-1]\n",
    "\n",
    "metrics_text = f\"\"\"\n",
    "Métricas Finais:\n",
    "Train Loss: {final_train_loss:.4f}\n",
    "Val Loss: {final_val_loss:.4f}\n",
    "Train MAE: {final_train_mae:.4f}\n",
    "Val MAE: {final_val_mae:.4f}\n",
    "\n",
    "Épocas: {len(history.history['loss'])}\n",
    "\"\"\"\n",
    "\n",
    "axes[1, 1].text(0.1, 0.5, metrics_text, transform=axes[1, 1].transAxes, \n",
    "                fontsize=12, verticalalignment='center')\n",
    "axes[1, 1].set_title('Métricas Finais')\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(MODEL_PATH / 'training_history.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Gráficos salvos em: {MODEL_PATH / 'training_history.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ea53ae",
   "metadata": {},
   "source": [
    "## 7. Avaliação Inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7619f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar melhor modelo\n",
    "best_model = tf.keras.models.load_model(MODEL_PATH / 'best_model.h5')\n",
    "\n",
    "# Previsões\n",
    "print(\"Gerando previsões...\")\n",
    "y_pred_train = best_model.predict(X_train_scaled)\n",
    "y_pred_val = best_model.predict(X_val_scaled)\n",
    "y_pred_test = best_model.predict(X_test_scaled)\n",
    "\n",
    "# Desnormalizar previsões\n",
    "y_pred_train_denorm = target_scaler.inverse_transform(y_pred_train).flatten()\n",
    "y_pred_val_denorm = target_scaler.inverse_transform(y_pred_val).flatten()\n",
    "y_pred_test_denorm = target_scaler.inverse_transform(y_pred_test).flatten()\n",
    "\n",
    "y_train_denorm = target_scaler.inverse_transform(y_train_scaled.reshape(-1, 1)).flatten()\n",
    "y_val_denorm = target_scaler.inverse_transform(y_val_scaled.reshape(-1, 1)).flatten()\n",
    "y_test_denorm = target_scaler.inverse_transform(y_test_scaled.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Métricas de regressão\n",
    "print(\"\\n=== MÉTRICAS DE REGRESSÃO ===\")\n",
    "print(\"TRAIN:\")\n",
    "print(f\"  MAE: {mean_absolute_error(y_train_denorm, y_pred_train_denorm):.4f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(y_train_denorm, y_pred_train_denorm)):.4f}\")\n",
    "\n",
    "print(\"VALIDATION:\")\n",
    "val_mae = mean_absolute_error(y_val_denorm, y_pred_val_denorm)\n",
    "val_rmse = np.sqrt(mean_squared_error(y_val_denorm, y_pred_val_denorm))\n",
    "print(f\"  MAE: {val_mae:.4f}\")\n",
    "print(f\"  RMSE: {val_rmse:.4f}\")\n",
    "\n",
    "print(\"TEST:\")\n",
    "test_mae = mean_absolute_error(y_test_denorm, y_pred_test_denorm)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test_denorm, y_pred_test_denorm))\n",
    "print(f\"  MAE: {test_mae:.4f}\")\n",
    "print(f\"  RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Verificar critérios de sucesso\n",
    "print(\"\\n=== CRITÉRIOS DE SUCESSO ===\")\n",
    "print(f\"MAE < 2.0: {'✓' if test_mae < 2.0 else '✗'} (atual: {test_mae:.4f})\")\n",
    "print(f\"RMSE < 3.0: {'✓' if test_rmse < 3.0 else '✗'} (atual: {test_rmse:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5008af2e",
   "metadata": {},
   "source": [
    "## 8. Salvar Modelo e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578cceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar modelo em formato SavedModel\n",
    "model_save_path = MODEL_PATH / 'lstm_weather_model'\n",
    "best_model.save(model_save_path)\n",
    "print(f\"Modelo salvo em: {model_save_path}\")\n",
    "\n",
    "# Salvar configurações e metadados\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "model_metadata = {\n",
    "    'model_config': MODEL_CONFIG,\n",
    "    'feature_columns': FEATURE_COLUMNS,\n",
    "    'target_column': target_column,\n",
    "    'training_date': datetime.now().isoformat(),\n",
    "    'tensorflow_version': tf.__version__,\n",
    "    'model_metrics': {\n",
    "        'train_mae': float(mean_absolute_error(y_train_denorm, y_pred_train_denorm)),\n",
    "        'val_mae': float(val_mae),\n",
    "        'test_mae': float(test_mae),\n",
    "        'train_rmse': float(np.sqrt(mean_squared_error(y_train_denorm, y_pred_train_denorm))),\n",
    "        'val_rmse': float(val_rmse),\n",
    "        'test_rmse': float(test_rmse),\n",
    "        'epochs_trained': len(history.history['loss'])\n",
    "    },\n",
    "    'data_shapes': {\n",
    "        'train_samples': int(X_train_scaled.shape[0]),\n",
    "        'val_samples': int(X_val_scaled.shape[0]),\n",
    "        'test_samples': int(X_test_scaled.shape[0]),\n",
    "        'sequence_length': int(X_train_scaled.shape[1]),\n",
    "        'features_count': int(X_train_scaled.shape[2])\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(MODEL_PATH / 'model_metadata.json', 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "\n",
    "print(f\"Metadados salvos em: {MODEL_PATH / 'model_metadata.json'}\")\n",
    "\n",
    "# Salvar histórico de treinamento\n",
    "history_df = pd.DataFrame(history.history)\n",
    "history_df.to_csv(MODEL_PATH / 'training_history.csv', index=False)\n",
    "print(f\"Histórico salvo em: {MODEL_PATH / 'training_history.csv'}\")\n",
    "\n",
    "print(\"\\n=== MODELO LSTM TREINADO COM SUCESSO! ===\")\n",
    "print(f\"Arquivos gerados em: {MODEL_PATH}\")\n",
    "print(\"\\nPróximos passos:\")\n",
    "print(\"1. Implementar avaliação detalhada (notebooks/model_evaluation.ipynb)\")\n",
    "print(\"2. Integrar modelo na aplicação (app/features/forecast/infra/)\")\n",
    "print(\"3. Implementar classificação de eventos de chuva\")\n",
    "print(\"4. Testar diferentes arquiteturas se necessário\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca4d10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
