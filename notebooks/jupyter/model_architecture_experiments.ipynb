{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a34a95b3",
   "metadata": {},
   "source": [
    "# Experimentos de Arquitetura LSTM - Otimização de Hiperparâmetros\n",
    "\n",
    "Este notebook implementa experimentos sistemáticos para encontrar a melhor arquitetura LSTM para previsão meteorológica.\n",
    "\n",
    "## Objetivos:\n",
    "- Testar diferentes configurações de arquitetura LSTM\n",
    "- Grid search automatizado para hiperparâmetros\n",
    "- Comparação de performance entre arquiteturas\n",
    "- Análise de trade-offs entre complexidade e performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d5a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necessários\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import itertools\n",
    "import json\n",
    "from datetime import datetime\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Configurações\n",
    "plt.style.use('seaborn-v0_8')\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ea681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurações base\n",
    "BASE_CONFIG = {\n",
    "    'sequence_length': 24,\n",
    "    'forecast_horizon': 24,\n",
    "    'batch_size': 32,\n",
    "    'epochs': 50,  # Reduzido para experimentos\n",
    "    'patience': 10,\n",
    "    'validation_split': 0.2\n",
    "}\n",
    "\n",
    "# Paths\n",
    "DATA_PATH = Path('../data/processed')\n",
    "MODEL_PATH = Path('../data/modelos_treinados')\n",
    "EXPERIMENTS_PATH = MODEL_PATH / 'experiments'\n",
    "EXPERIMENTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Configuração base: {BASE_CONFIG}\")\n",
    "print(f\"Experimentos serão salvos em: {EXPERIMENTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e2d62",
   "metadata": {},
   "source": [
    "## 1. Definição de Arquiteturas para Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274a5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir diferentes arquiteturas para teste\n",
    "ARCHITECTURES = {\n",
    "    'simple_1_layer': {\n",
    "        'lstm_units': [64],\n",
    "        'dropout_rate': 0.1,\n",
    "        'learning_rate': 0.001,\n",
    "        'description': 'Arquitetura simples com 1 camada LSTM'\n",
    "    },\n",
    "    'simple_2_layers': {\n",
    "        'lstm_units': [128, 64],\n",
    "        'dropout_rate': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'description': 'Arquitetura com 2 camadas LSTM'\n",
    "    },\n",
    "    'simple_3_layers': {\n",
    "        'lstm_units': [256, 128, 64],\n",
    "        'dropout_rate': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'description': 'Arquitetura com 3 camadas LSTM'\n",
    "    },\n",
    "    'heavy_2_layers': {\n",
    "        'lstm_units': [256, 128],\n",
    "        'dropout_rate': 0.3,\n",
    "        'learning_rate': 0.0001,\n",
    "        'description': 'Arquitetura pesada com 2 camadas'\n",
    "    },\n",
    "    'light_3_layers': {\n",
    "        'lstm_units': [64, 32, 16],\n",
    "        'dropout_rate': 0.1,\n",
    "        'learning_rate': 0.01,\n",
    "        'description': 'Arquitetura leve com 3 camadas'\n",
    "    },\n",
    "    'production': {\n",
    "        'lstm_units': [128, 64, 32],\n",
    "        'dropout_rate': 0.2,\n",
    "        'learning_rate': 0.001,\n",
    "        'description': 'Arquitetura balanceada para produção'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Arquiteturas definidas:\")\n",
    "for name, config in ARCHITECTURES.items():\n",
    "    print(f\"  {name}: {config['lstm_units']} - {config['description']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd28ab",
   "metadata": {},
   "source": [
    "## 2. Grid Search Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd43b339",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Parâmetros para grid search\n",
    "GRID_SEARCH_PARAMS = {\n",
    "    'sequence_lengths': [12, 24, 48],\n",
    "    'learning_rates': [0.01, 0.001, 0.0001],\n",
    "    'batch_sizes': [16, 32, 64],\n",
    "    'dropout_rates': [0.1, 0.2, 0.3]\n",
    "}\n",
    "\n",
    "print(\"Parâmetros para Grid Search:\")\n",
    "for param, values in GRID_SEARCH_PARAMS.items():\n",
    "    print(f\"  {param}: {values}\")\n",
    "\n",
    "total_combinations = np.prod([len(v) for v in GRID_SEARCH_PARAMS.values()])\n",
    "print(f\"\\nTotal de combinações possíveis: {total_combinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f82614",
   "metadata": {},
   "source": [
    "## 3. Funções Utilitárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c0260",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def create_lstm_model(sequence_length, features_count, lstm_units, dropout_rate, learning_rate):\n",
    "    \"\"\"\n",
    "    Cria modelo LSTM com configuração específica\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Primeira camada LSTM\n",
    "    model.add(LSTM(\n",
    "        lstm_units[0],\n",
    "        return_sequences=len(lstm_units) > 1,\n",
    "        input_shape=(sequence_length, features_count),\n",
    "        name='lstm_1'\n",
    "    ))\n",
    "    model.add(Dropout(dropout_rate, name='dropout_1'))\n",
    "    \n",
    "    # Camadas LSTM adicionais\n",
    "    for i, units in enumerate(lstm_units[1:], 2):\n",
    "        return_sequences = i < len(lstm_units)\n",
    "        model.add(LSTM(\n",
    "            units,\n",
    "            return_sequences=return_sequences,\n",
    "            name=f'lstm_{i}'\n",
    "        ))\n",
    "        model.add(Dropout(dropout_rate, name=f'dropout_{i}'))\n",
    "    \n",
    "    # Camada densa final\n",
    "    model.add(Dense(50, activation='relu', name='dense_1'))\n",
    "    model.add(Dropout(dropout_rate, name='dropout_final'))\n",
    "    model.add(Dense(1, name='output'))\n",
    "    \n",
    "    # Compilar\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def prepare_sequences(data, feature_cols, target_col, sequence_length, forecast_horizon):\n",
    "    \"\"\"\n",
    "    Prepara sequências temporais para treinamento\n",
    "    \"\"\"\n",
    "    if 'timestamp' in data.columns:\n",
    "        data = data.sort_values('timestamp')\n",
    "    \n",
    "    features = data[feature_cols].values\n",
    "    target = data[target_col].values\n",
    "    \n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - sequence_length - forecast_horizon + 1):\n",
    "        X.append(features[i:(i + sequence_length)])\n",
    "        target_idx = i + sequence_length + forecast_horizon - 1\n",
    "        y.append(target[target_idx])\n",
    "    \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, target_scaler):\n",
    "    \"\"\"\n",
    "    Avalia modelo e retorna métricas\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    \n",
    "    # Desnormalizar\n",
    "    y_pred_denorm = target_scaler.inverse_transform(y_pred).flatten()\n",
    "    y_test_denorm = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Calcular métricas\n",
    "    mae = mean_absolute_error(y_test_denorm, y_pred_denorm)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test_denorm, y_pred_denorm))\n",
    "    \n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'mae_normalized': mean_absolute_error(y_test, y_pred.flatten()),\n",
    "        'rmse_normalized': np.sqrt(mean_squared_error(y_test, y_pred.flatten()))\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f1869",
   "metadata": {},
   "source": [
    "## 4. Carregamento de Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8aea68",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Carregar dados processados\n",
    "print(\"Carregando dados...\")\n",
    "train_data = pd.read_parquet(DATA_PATH / 'train_data.parquet')\n",
    "val_data = pd.read_parquet(DATA_PATH / 'validation_data.parquet')\n",
    "test_data = pd.read_parquet(DATA_PATH / 'test_data.parquet')\n",
    "\n",
    "print(f\"Train shape: {train_data.shape}\")\n",
    "print(f\"Validation shape: {val_data.shape}\")\n",
    "print(f\"Test shape: {test_data.shape}\")\n",
    "\n",
    "# Identificar features e target\n",
    "feature_columns = [col for col in train_data.columns if col not in ['timestamp']]\n",
    "target_column = None\n",
    "\n",
    "# Procurar coluna de precipitação\n",
    "for col in train_data.columns:\n",
    "    if 'precipitacao' in col.lower() or 'chuva' in col.lower():\n",
    "        target_column = col\n",
    "        break\n",
    "\n",
    "if target_column is None:\n",
    "    target_column = feature_columns[0]\n",
    "    print(f\"ATENÇÃO: Usando {target_column} como target\")\n",
    "\n",
    "# Remover target das features\n",
    "if target_column in feature_columns:\n",
    "    feature_columns.remove(target_column)\n",
    "\n",
    "print(f\"Features: {len(feature_columns)} colunas\")\n",
    "print(f\"Target: {target_column}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dc130",
   "metadata": {},
   "source": [
    "## 5. Experimento 1: Comparação de Arquiteturas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b08ff19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_architecture_experiment():\n",
    "    \"\"\"\n",
    "    Executa experimento comparando diferentes arquiteturas\n",
    "    \"\"\"\n",
    "    print(\"=== EXPERIMENTO 1: COMPARAÇÃO DE ARQUITETURAS ===\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for arch_name, arch_config in ARCHITECTURES.items():\n",
    "        print(f\"\\nTestando arquitetura: {arch_name}\")\n",
    "        print(f\"Configuração: {arch_config}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Preparar dados com configuração base\n",
    "            X_train, y_train = prepare_sequences(\n",
    "                train_data, feature_columns, target_column,\n",
    "                BASE_CONFIG['sequence_length'], BASE_CONFIG['forecast_horizon']\n",
    "            )\n",
    "            \n",
    "            X_val, y_val = prepare_sequences(\n",
    "                val_data, feature_columns, target_column,\n",
    "                BASE_CONFIG['sequence_length'], BASE_CONFIG['forecast_horizon']\n",
    "            )\n",
    "            \n",
    "            X_test, y_test = prepare_sequences(\n",
    "                test_data, feature_columns, target_column,\n",
    "                BASE_CONFIG['sequence_length'], BASE_CONFIG['forecast_horizon']\n",
    "            )\n",
    "            \n",
    "            # Normalizar dados\n",
    "            X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "            X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "            X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "            \n",
    "            feature_scaler = StandardScaler()\n",
    "            X_train_scaled = feature_scaler.fit_transform(X_train_reshaped).reshape(X_train.shape)\n",
    "            X_val_scaled = feature_scaler.transform(X_val_reshaped).reshape(X_val.shape)\n",
    "            X_test_scaled = feature_scaler.transform(X_test_reshaped).reshape(X_test.shape)\n",
    "            \n",
    "            target_scaler = StandardScaler()\n",
    "            y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "            y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "            y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Criar modelo\n",
    "            model = create_lstm_model(\n",
    "                BASE_CONFIG['sequence_length'],\n",
    "                len(feature_columns),\n",
    "                arch_config['lstm_units'],\n",
    "                arch_config['dropout_rate'],\n",
    "                arch_config['learning_rate']\n",
    "            )\n",
    "            \n",
    "            # Callbacks\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=BASE_CONFIG['patience'], \n",
    "                            restore_best_weights=True, verbose=0),\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, \n",
    "                                min_lr=1e-7, verbose=0)\n",
    "            ]\n",
    "            \n",
    "            # Treinar\n",
    "            history = model.fit(\n",
    "                X_train_scaled, y_train_scaled,\n",
    "                validation_data=(X_val_scaled, y_val_scaled),\n",
    "                epochs=BASE_CONFIG['epochs'],\n",
    "                batch_size=BASE_CONFIG['batch_size'],\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Avaliar\n",
    "            metrics = evaluate_model(model, X_test_scaled, y_test_scaled, target_scaler)\n",
    "            \n",
    "            # Calcular parâmetros do modelo\n",
    "            total_params = model.count_params()\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            result = {\n",
    "                'architecture': arch_name,\n",
    "                'lstm_units': arch_config['lstm_units'],\n",
    "                'dropout_rate': arch_config['dropout_rate'],\n",
    "                'learning_rate': arch_config['learning_rate'],\n",
    "                'total_params': total_params,\n",
    "                'training_time': training_time,\n",
    "                'epochs_trained': len(history.history['loss']),\n",
    "                'final_train_loss': history.history['loss'][-1],\n",
    "                'final_val_loss': history.history['val_loss'][-1],\n",
    "                'final_train_mae': history.history['mae'][-1],\n",
    "                'final_val_mae': history.history['val_mae'][-1],\n",
    "                **metrics\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"  ✓ Concluído em {training_time:.1f}s\")\n",
    "            print(f\"  ✓ MAE: {metrics['mae']:.4f}, RMSE: {metrics['rmse']:.4f}\")\n",
    "            print(f\"  ✓ Parâmetros: {total_params:,}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Erro: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Salvar resultados\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(EXPERIMENTS_PATH / 'architecture_comparison.csv', index=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Executar experimento\n",
    "architecture_results = run_architecture_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b3f621",
   "metadata": {},
   "source": [
    "## 6. Análise dos Resultados de Arquitetura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8670f468",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "if not architecture_results.empty:\n",
    "    print(\"\\n=== ANÁLISE DOS RESULTADOS DE ARQUITETURA ===\")\n",
    "    \n",
    "    # Ordenar por MAE\n",
    "    architecture_results_sorted = architecture_results.sort_values('mae')\n",
    "    \n",
    "    print(\"\\nTop 3 arquiteturas por MAE:\")\n",
    "    for i, (_, row) in enumerate(architecture_results_sorted.head(3).iterrows()):\n",
    "        print(f\"{i+1}. {row['architecture']}\")\n",
    "        print(f\"   MAE: {row['mae']:.4f}, RMSE: {row['rmse']:.4f}\")\n",
    "        print(f\"   Parâmetros: {row['total_params']:,}\")\n",
    "        print(f\"   Tempo: {row['training_time']:.1f}s\")\n",
    "    \n",
    "    # Visualizações\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # MAE vs RMSE\n",
    "    axes[0, 0].scatter(architecture_results['mae'], architecture_results['rmse'])\n",
    "    for i, row in architecture_results.iterrows():\n",
    "        axes[0, 0].annotate(row['architecture'], \n",
    "                           (row['mae'], row['rmse']), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    axes[0, 0].set_xlabel('MAE')\n",
    "    axes[0, 0].set_ylabel('RMSE')\n",
    "    axes[0, 0].set_title('MAE vs RMSE por Arquitetura')\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Parâmetros vs Performance\n",
    "    axes[0, 1].scatter(architecture_results['total_params'], architecture_results['mae'])\n",
    "    for i, row in architecture_results.iterrows():\n",
    "        axes[0, 1].annotate(row['architecture'], \n",
    "                           (row['total_params'], row['mae']), \n",
    "                           xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "    axes[0, 1].set_xlabel('Total Parameters')\n",
    "    axes[0, 1].set_ylabel('MAE')\n",
    "    axes[0, 1].set_title('Complexidade vs Performance')\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Tempo de treinamento\n",
    "    axes[1, 0].bar(architecture_results['architecture'], architecture_results['training_time'])\n",
    "    axes[1, 0].set_xlabel('Arquitetura')\n",
    "    axes[1, 0].set_ylabel('Tempo (s)')\n",
    "    axes[1, 0].set_title('Tempo de Treinamento')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Comparação de métricas\n",
    "    metrics_comparison = architecture_results[['architecture', 'mae', 'rmse']].set_index('architecture')\n",
    "    metrics_comparison.plot(kind='bar', ax=axes[1, 1])\n",
    "    axes[1, 1].set_xlabel('Arquitetura')\n",
    "    axes[1, 1].set_ylabel('Valor')\n",
    "    axes[1, 1].set_title('Comparação MAE vs RMSE')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENTS_PATH / 'architecture_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a69397",
   "metadata": {},
   "source": [
    "## 7. Experimento 2: Grid Search de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff34138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid_search_experiment(max_combinations=20):\n",
    "    \"\"\"\n",
    "    Executa grid search limitado para hiperparâmetros\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== EXPERIMENTO 2: GRID SEARCH (máximo {max_combinations} combinações) ===\")\n",
    "    \n",
    "    # Gerar todas as combinações\n",
    "    param_combinations = list(itertools.product(\n",
    "        GRID_SEARCH_PARAMS['sequence_lengths'],\n",
    "        GRID_SEARCH_PARAMS['learning_rates'],\n",
    "        GRID_SEARCH_PARAMS['batch_sizes'],\n",
    "        GRID_SEARCH_PARAMS['dropout_rates']\n",
    "    ))\n",
    "    \n",
    "    # Limitar número de combinações\n",
    "    if len(param_combinations) > max_combinations:\n",
    "        param_combinations = np.random.choice(\n",
    "            len(param_combinations), max_combinations, replace=False\n",
    "        )\n",
    "        param_combinations = [list(itertools.product(\n",
    "            GRID_SEARCH_PARAMS['sequence_lengths'],\n",
    "            GRID_SEARCH_PARAMS['learning_rates'],\n",
    "            GRID_SEARCH_PARAMS['batch_sizes'],\n",
    "            GRID_SEARCH_PARAMS['dropout_rates']\n",
    "        ))[i] for i in param_combinations]\n",
    "    \n",
    "    print(f\"Testando {len(param_combinations)} combinações de hiperparâmetros...\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, (seq_len, lr, batch_size, dropout) in enumerate(param_combinations):\n",
    "        print(f\"\\nCombinação {i+1}/{len(param_combinations)}\")\n",
    "        print(f\"seq_len={seq_len}, lr={lr}, batch_size={batch_size}, dropout={dropout}\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Preparar dados com sequence_length específico\n",
    "            X_train, y_train = prepare_sequences(\n",
    "                train_data, feature_columns, target_column,\n",
    "                seq_len, BASE_CONFIG['forecast_horizon']\n",
    "            )\n",
    "            \n",
    "            X_val, y_val = prepare_sequences(\n",
    "                val_data, feature_columns, target_column,\n",
    "                seq_len, BASE_CONFIG['forecast_horizon']\n",
    "            )\n",
    "            \n",
    "            X_test, y_test = prepare_sequences(\n",
    "                test_data, feature_columns, target_column,\n",
    "                seq_len, BASE_CONFIG['forecast_horizon']\n",
    "            )\n",
    "            \n",
    "            # Normalizar\n",
    "            X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])\n",
    "            X_val_reshaped = X_val.reshape(-1, X_val.shape[-1])\n",
    "            X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])\n",
    "            \n",
    "            feature_scaler = StandardScaler()\n",
    "            X_train_scaled = feature_scaler.fit_transform(X_train_reshaped).reshape(X_train.shape)\n",
    "            X_val_scaled = feature_scaler.transform(X_val_reshaped).reshape(X_val.shape)\n",
    "            X_test_scaled = feature_scaler.transform(X_test_reshaped).reshape(X_test.shape)\n",
    "            \n",
    "            target_scaler = StandardScaler()\n",
    "            y_train_scaled = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
    "            y_val_scaled = target_scaler.transform(y_val.reshape(-1, 1)).flatten()\n",
    "            y_test_scaled = target_scaler.transform(y_test.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Usar arquitetura padrão\n",
    "            model = create_lstm_model(\n",
    "                seq_len, len(feature_columns),\n",
    "                [128, 64], dropout, lr\n",
    "            )\n",
    "            \n",
    "            # Callbacks\n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_loss', patience=5, \n",
    "                            restore_best_weights=True, verbose=0)\n",
    "            ]\n",
    "            \n",
    "            # Treinar\n",
    "            history = model.fit(\n",
    "                X_train_scaled, y_train_scaled,\n",
    "                validation_data=(X_val_scaled, y_val_scaled),\n",
    "                epochs=30,  # Reduzido para grid search\n",
    "                batch_size=batch_size,\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Avaliar\n",
    "            metrics = evaluate_model(model, X_test_scaled, y_test_scaled, target_scaler)\n",
    "            \n",
    "            training_time = time.time() - start_time\n",
    "            \n",
    "            result = {\n",
    "                'sequence_length': seq_len,\n",
    "                'learning_rate': lr,\n",
    "                'batch_size': batch_size,\n",
    "                'dropout_rate': dropout,\n",
    "                'training_time': training_time,\n",
    "                'epochs_trained': len(history.history['loss']),\n",
    "                'final_val_loss': history.history['val_loss'][-1],\n",
    "                'final_val_mae': history.history['val_mae'][-1],\n",
    "                **metrics\n",
    "            }\n",
    "            \n",
    "            results.append(result)\n",
    "            \n",
    "            print(f\"  ✓ MAE: {metrics['mae']:.4f} em {training_time:.1f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Erro: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Salvar resultados\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(EXPERIMENTS_PATH / 'grid_search_results.csv', index=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Executar grid search\n",
    "grid_search_results = run_grid_search_experiment(max_combinations=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6abd910",
   "metadata": {},
   "source": [
    "## 8. Análise do Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7e959",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "if not grid_search_results.empty:\n",
    "    print(\"\\n=== ANÁLISE DO GRID SEARCH ===\")\n",
    "    \n",
    "    # Melhor combinação\n",
    "    best_combination = grid_search_results.loc[grid_search_results['mae'].idxmin()]\n",
    "    \n",
    "    print(\"\\nMelhor combinação de hiperparâmetros:\")\n",
    "    print(f\"  Sequence Length: {best_combination['sequence_length']}\")\n",
    "    print(f\"  Learning Rate: {best_combination['learning_rate']}\")\n",
    "    print(f\"  Batch Size: {best_combination['batch_size']}\")\n",
    "    print(f\"  Dropout Rate: {best_combination['dropout_rate']}\")\n",
    "    print(f\"  MAE: {best_combination['mae']:.4f}\")\n",
    "    print(f\"  RMSE: {best_combination['rmse']:.4f}\")\n",
    "    \n",
    "    # Análise por parâmetro\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Sequence Length\n",
    "    seq_len_analysis = grid_search_results.groupby('sequence_length')['mae'].agg(['mean', 'std'])\n",
    "    seq_len_analysis['mean'].plot(kind='bar', ax=axes[0, 0], yerr=seq_len_analysis['std'])\n",
    "    axes[0, 0].set_title('MAE por Sequence Length')\n",
    "    axes[0, 0].set_ylabel('MAE')\n",
    "    \n",
    "    # Learning Rate\n",
    "    lr_analysis = grid_search_results.groupby('learning_rate')['mae'].agg(['mean', 'std'])\n",
    "    lr_analysis['mean'].plot(kind='bar', ax=axes[0, 1], yerr=lr_analysis['std'])\n",
    "    axes[0, 1].set_title('MAE por Learning Rate')\n",
    "    axes[0, 1].set_ylabel('MAE')\n",
    "    axes[0, 1].set_xscale('log')\n",
    "    \n",
    "    # Batch Size\n",
    "    batch_analysis = grid_search_results.groupby('batch_size')['mae'].agg(['mean', 'std'])\n",
    "    batch_analysis['mean'].plot(kind='bar', ax=axes[1, 0], yerr=batch_analysis['std'])\n",
    "    axes[1, 0].set_title('MAE por Batch Size')\n",
    "    axes[1, 0].set_ylabel('MAE')\n",
    "    \n",
    "    # Dropout Rate\n",
    "    dropout_analysis = grid_search_results.groupby('dropout_rate')['mae'].agg(['mean', 'std'])\n",
    "    dropout_analysis['mean'].plot(kind='bar', ax=axes[1, 1], yerr=dropout_analysis['std'])\n",
    "    axes[1, 1].set_title('MAE por Dropout Rate')\n",
    "    axes[1, 1].set_ylabel('MAE')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EXPERIMENTS_PATH / 'grid_search_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215be492",
   "metadata": {},
   "source": [
    "## 9. Relatório Final dos Experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc698e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_experiment_report():\n",
    "    \"\"\"\n",
    "    Gera relatório final dos experimentos\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        'experiment_date': datetime.now().isoformat(),\n",
    "        'base_config': BASE_CONFIG,\n",
    "        'architectures_tested': len(ARCHITECTURES),\n",
    "        'grid_search_combinations': len(grid_search_results) if not grid_search_results.empty else 0,\n",
    "        'best_architecture': None,\n",
    "        'best_hyperparams': None,\n",
    "        'recommendations': []\n",
    "    }\n",
    "    \n",
    "    # Melhor arquitetura\n",
    "    if not architecture_results.empty:\n",
    "        best_arch = architecture_results.loc[architecture_results['mae'].idxmin()]\n",
    "        report['best_architecture'] = {\n",
    "            'name': best_arch['architecture'],\n",
    "            'mae': float(best_arch['mae']),\n",
    "            'rmse': float(best_arch['rmse']),\n",
    "            'params': int(best_arch['total_params']),\n",
    "            'config': {\n",
    "                'lstm_units': best_arch['lstm_units'],\n",
    "                'dropout_rate': float(best_arch['dropout_rate']),\n",
    "                'learning_rate': float(best_arch['learning_rate'])\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Melhores hiperparâmetros\n",
    "    if not grid_search_results.empty:\n",
    "        best_params = grid_search_results.loc[grid_search_results['mae'].idxmin()]\n",
    "        report['best_hyperparams'] = {\n",
    "            'sequence_length': int(best_params['sequence_length']),\n",
    "            'learning_rate': float(best_params['learning_rate']),\n",
    "            'batch_size': int(best_params['batch_size']),\n",
    "            'dropout_rate': float(best_params['dropout_rate']),\n",
    "            'mae': float(best_params['mae']),\n",
    "            'rmse': float(best_params['rmse'])\n",
    "        }\n",
    "    \n",
    "    # Recomendações\n",
    "    recommendations = []\n",
    "    \n",
    "    if not architecture_results.empty:\n",
    "        # Análise de complexidade vs performance\n",
    "        complexity_performance = architecture_results['total_params'] / architecture_results['mae']\n",
    "        best_efficiency = architecture_results.loc[complexity_performance.idxmax()]\n",
    "        \n",
    "        recommendations.append(f\"Arquitetura mais eficiente: {best_efficiency['architecture']}\")\n",
    "        recommendations.append(f\"Melhor performance: {architecture_results.loc[architecture_results['mae'].idxmin()]['architecture']}\")\n",
    "    \n",
    "    if not grid_search_results.empty:\n",
    "        # Análise de hiperparâmetros\n",
    "        if len(grid_search_results['sequence_length'].unique()) > 1:\n",
    "            best_seq_len = grid_search_results.groupby('sequence_length')['mae'].mean().idxmin()\n",
    "            recommendations.append(f\"Melhor sequence length: {best_seq_len}\")\n",
    "        \n",
    "        if len(grid_search_results['learning_rate'].unique()) > 1:\n",
    "            best_lr = grid_search_results.groupby('learning_rate')['mae'].mean().idxmin()\n",
    "            recommendations.append(f\"Melhor learning rate: {best_lr}\")\n",
    "    \n",
    "    report['recommendations'] = recommendations\n",
    "    \n",
    "    # Salvar relatório\n",
    "    with open(EXPERIMENTS_PATH / 'experiment_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Gerar relatório\n",
    "final_report = generate_experiment_report()\n",
    "\n",
    "print(\"\\n=== RELATÓRIO FINAL DOS EXPERIMENTOS ===\")\n",
    "print(f\"Data: {final_report['experiment_date']}\")\n",
    "print(f\"Arquiteturas testadas: {final_report['architectures_tested']}\")\n",
    "print(f\"Combinações de grid search: {final_report['grid_search_combinations']}\")\n",
    "\n",
    "if final_report['best_architecture']:\n",
    "    print(f\"\\nMelhor arquitetura: {final_report['best_architecture']['name']}\")\n",
    "    print(f\"  MAE: {final_report['best_architecture']['mae']:.4f}\")\n",
    "    print(f\"  RMSE: {final_report['best_architecture']['rmse']:.4f}\")\n",
    "\n",
    "if final_report['best_hyperparams']:\n",
    "    print(f\"\\nMelhores hiperparâmetros:\")\n",
    "    for param, value in final_report['best_hyperparams'].items():\n",
    "        if param not in ['mae', 'rmse']:\n",
    "            print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nRecomendações:\")\n",
    "for rec in final_report['recommendations']:\n",
    "    print(f\"  - {rec}\")\n",
    "\n",
    "print(f\"\\nArquivos gerados em: {EXPERIMENTS_PATH}\")\n",
    "print(\"  - architecture_comparison.csv\")\n",
    "print(\"  - grid_search_results.csv\")\n",
    "print(\"  - experiment_report.json\")\n",
    "print(\"  - architecture_analysis.png\")\n",
    "print(\"  - grid_search_analysis.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe3281c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
