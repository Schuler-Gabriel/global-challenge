{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "356925dd",
   "metadata": {},
   "source": [
    "# An√°lise Explorat√≥ria dos Dados Meteorol√≥gicos INMET\n",
    "\n",
    "Este notebook realiza a an√°lise explorat√≥ria dos dados hist√≥ricos do INMET (2000-2025) para o projeto de alertas de cheias.\n",
    "\n",
    "## Objetivos:\n",
    "- Explorar estrutura e qualidade dos dados meteorol√≥gicos\n",
    "- Identificar padr√µes sazonais e tend√™ncias clim√°ticas\n",
    "- Detectar outliers e dados inconsistentes\n",
    "- An√°lise de correla√ß√µes entre vari√°veis\n",
    "- Visualiza√ß√µes descritivas e estat√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import glob\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Configura√ß√µes\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"=== AN√ÅLISE EXPLORAT√ìRIA DOS DADOS INMET ===\")\n",
    "print(f\"Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b45976",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configura√ß√£o dos caminhos de dados\n",
    "DATA_PATH = Path('../data')\n",
    "RAW_DATA_PATH = DATA_PATH / 'raw' / 'dados_historicos'\n",
    "PROCESSED_DATA_PATH = DATA_PATH / 'processed'\n",
    "ANALYSIS_OUTPUT_PATH = DATA_PATH / 'analysis'\n",
    "\n",
    "# Criar diret√≥rio de sa√≠da se n√£o existir\n",
    "ANALYSIS_OUTPUT_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Caminho dos dados brutos: {RAW_DATA_PATH}\")\n",
    "print(f\"Caminho dos dados processados: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"Sa√≠da da an√°lise: {ANALYSIS_OUTPUT_PATH}\")\n",
    "\n",
    "# Verificar se existem dados\n",
    "raw_files = list(RAW_DATA_PATH.glob(\"*.CSV\")) if RAW_DATA_PATH.exists() else []\n",
    "processed_files = list(PROCESSED_DATA_PATH.glob(\"*.parquet\")) if PROCESSED_DATA_PATH.exists() else []\n",
    "\n",
    "print(f\"\\nArquivos CSV encontrados: {len(raw_files)}\")\n",
    "print(f\"Arquivos processados encontrados: {len(processed_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9145f7",
   "metadata": {},
   "source": [
    "## 1. Carregamento e Estrutura dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f3f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inmet_data():\n",
    "    \"\"\"\n",
    "    Carrega dados meteorol√≥gicos do INMET\n",
    "    \"\"\"\n",
    "    if processed_files:\n",
    "        print(\"Carregando dados processados...\")\n",
    "        \n",
    "        # Tentar carregar dados processados primeiro\n",
    "        try:\n",
    "            train_data = pd.read_parquet(PROCESSED_DATA_PATH / 'train_data.parquet')\n",
    "            print(f\"‚úì Dados de treino carregados: {train_data.shape}\")\n",
    "            return train_data\n",
    "        except FileNotFoundError:\n",
    "            print(\"Arquivos processados n√£o encontrados, carregando dados brutos...\")\n",
    "    \n",
    "    if not raw_files:\n",
    "        print(\"‚ùå Nenhum arquivo de dados encontrado!\")\n",
    "        # Criar dados sint√©ticos para demonstra√ß√£o\n",
    "        print(\"Criando dados sint√©ticos para demonstra√ß√£o...\")\n",
    "        \n",
    "        dates = pd.date_range('2020-01-01', '2022-12-31', freq='H')\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        data = pd.DataFrame({\n",
    "            'timestamp': dates,\n",
    "            'precipitacao_mm': np.random.exponential(0.5, len(dates)),\n",
    "            'temperatura_c': 20 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 2, len(dates)),\n",
    "            'umidade_relativa': 50 + 30 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 5, len(dates)),\n",
    "            'pressao_mb': 1013 + np.random.normal(0, 10, len(dates)),\n",
    "            'velocidade_vento_ms': np.random.gamma(2, 2, len(dates)),\n",
    "            'direcao_vento_gr': np.random.uniform(0, 360, len(dates)),\n",
    "            'radiacao_kjm2': np.maximum(0, 500 * np.sin(2 * np.pi * np.arange(len(dates)) / 24) + np.random.normal(0, 100, len(dates)))\n",
    "        })\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Carregar dados brutos INMET\n",
    "    print(\"Carregando dados brutos do INMET...\")\n",
    "    dataframes = []\n",
    "    \n",
    "    for file_path in raw_files[:3]:  # Limitar para 3 arquivos para an√°lise inicial\n",
    "        print(f\"Processando: {file_path.name}\")\n",
    "        try:\n",
    "            # Tentar diferentes encodings\n",
    "            for encoding in ['latin1', 'utf-8', 'cp1252']:\n",
    "                try:\n",
    "                    df = pd.read_csv(file_path, sep=';', encoding=encoding, skiprows=8)\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            # Adicionar informa√ß√£o do arquivo\n",
    "            df['arquivo_origem'] = file_path.name\n",
    "            dataframes.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Erro ao carregar {file_path.name}: {e}\")\n",
    "    \n",
    "    if dataframes:\n",
    "        combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "        print(f\"‚úì Dados combinados: {combined_data.shape}\")\n",
    "        return combined_data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Carregar dados\n",
    "data = load_inmet_data()\n",
    "\n",
    "if data is not None:\n",
    "    print(f\"\\nüìä Dados carregados com sucesso!\")\n",
    "    print(f\"Shape: {data.shape}\")\n",
    "    print(f\"Per√≠odo: {data['timestamp'].min() if 'timestamp' in data.columns else 'N/A'} at√© {data['timestamp'].max() if 'timestamp' in data.columns else 'N/A'}\")\n",
    "else:\n",
    "    print(\"‚ùå Falha ao carregar dados\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880326ab",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Explorar estrutura dos dados\n",
    "if data is not None:\n",
    "    print(\"=== ESTRUTURA DOS DADOS ===\")\n",
    "    print(f\"Dimens√µes: {data.shape}\")\n",
    "    print(f\"Colunas: {list(data.columns)}\")\n",
    "    print(f\"Tipos de dados:\\n{data.dtypes}\")\n",
    "    \n",
    "    # Informa√ß√µes gerais\n",
    "    print(f\"\\n=== INFORMA√á√ïES GERAIS ===\")\n",
    "    print(data.info())\n",
    "    \n",
    "    # Primeiras linhas\n",
    "    print(f\"\\n=== PRIMEIRAS 5 LINHAS ===\")\n",
    "    print(data.head())\n",
    "    \n",
    "    # Valores missing\n",
    "    print(f\"\\n=== VALORES MISSING ===\")\n",
    "    missing_data = data.isnull().sum()\n",
    "    missing_percent = (missing_data / len(data)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Missing %': missing_percent\n",
    "    }).sort_values('Missing %', ascending=False)\n",
    "    \n",
    "    print(missing_df[missing_df['Missing Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32711a8e",
   "metadata": {},
   "source": [
    "## 2. An√°lise de Qualidade dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd742bc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_data_quality(df):\n",
    "    \"\"\"\n",
    "    Analisa qualidade dos dados meteorol√≥gicos\n",
    "    \"\"\"\n",
    "    print(\"=== AN√ÅLISE DE QUALIDADE DOS DADOS ===\")\n",
    "    \n",
    "    # Identificar colunas num√©ricas\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if not numeric_cols:\n",
    "        print(\"‚ùå Nenhuma coluna num√©rica encontrada\")\n",
    "        return\n",
    "    \n",
    "    quality_report = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            \n",
    "            if len(col_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            report = {\n",
    "                'coluna': col,\n",
    "                'total_valores': len(df),\n",
    "                'valores_validos': len(col_data),\n",
    "                'missing_count': df[col].isnull().sum(),\n",
    "                'missing_percent': (df[col].isnull().sum() / len(df)) * 100,\n",
    "                'min': col_data.min(),\n",
    "                'max': col_data.max(),\n",
    "                'mean': col_data.mean(),\n",
    "                'std': col_data.std(),\n",
    "                'zeros': (col_data == 0).sum(),\n",
    "                'negativos': (col_data < 0).sum(),\n",
    "                'outliers_iqr': 0\n",
    "            }\n",
    "            \n",
    "            # Calcular outliers usando IQR\n",
    "            Q1 = col_data.quantile(0.25)\n",
    "            Q3 = col_data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = col_data[(col_data < Q1 - 1.5*IQR) | (col_data > Q3 + 1.5*IQR)]\n",
    "            report['outliers_iqr'] = len(outliers)\n",
    "            \n",
    "            quality_report.append(report)\n",
    "    \n",
    "    # Converter para DataFrame\n",
    "    quality_df = pd.DataFrame(quality_report)\n",
    "    \n",
    "    print(\"Resumo da qualidade dos dados:\")\n",
    "    print(quality_df.round(2))\n",
    "    \n",
    "    return quality_df\n",
    "\n",
    "# Executar an√°lise de qualidade\n",
    "if data is not None:\n",
    "    quality_report = analyze_data_quality(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ef49ec",
   "metadata": {},
   "source": [
    "## 3. An√°lise Estat√≠stica Descritiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb4fb02",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def generate_descriptive_statistics(df):\n",
    "    \"\"\"\n",
    "    Gera estat√≠sticas descritivas abrangentes\n",
    "    \"\"\"\n",
    "    print(\"=== ESTAT√çSTICAS DESCRITIVAS ===\")\n",
    "    \n",
    "    # Colunas num√©ricas\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if not numeric_cols:\n",
    "        print(\"‚ùå Nenhuma coluna num√©rica encontrada\")\n",
    "        return\n",
    "    \n",
    "    # Estat√≠sticas b√°sicas\n",
    "    print(\"Estat√≠sticas b√°sicas:\")\n",
    "    stats = df[numeric_cols].describe()\n",
    "    print(stats.round(3))\n",
    "    \n",
    "    # An√°lise de distribui√ß√µes\n",
    "    print(f\"\\n=== AN√ÅLISE DE DISTRIBUI√á√ïES ===\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols[:6]):  # Limitar a 6 vari√°veis\n",
    "        if col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            \n",
    "            # Histograma\n",
    "            axes[i].hist(col_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "            axes[i].set_title(f'Distribui√ß√£o - {col}')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Frequ√™ncia')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Adicionar estat√≠sticas no gr√°fico\n",
    "            mean_val = col_data.mean()\n",
    "            median_val = col_data.median()\n",
    "            axes[i].axvline(mean_val, color='red', linestyle='--', label=f'M√©dia: {mean_val:.2f}')\n",
    "            axes[i].axvline(median_val, color='green', linestyle='--', label=f'Mediana: {median_val:.2f}')\n",
    "            axes[i].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT_PATH / 'distribuicoes_variaveis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# Executar an√°lise descritiva\n",
    "if data is not None:\n",
    "    descriptive_stats = generate_descriptive_statistics(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1eb1cc3",
   "metadata": {},
   "source": [
    "## 4. An√°lise Temporal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5dbe48",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_temporal_patterns(df):\n",
    "    \"\"\"\n",
    "    Analisa padr√µes temporais nos dados\n",
    "    \"\"\"\n",
    "    print(\"=== AN√ÅLISE TEMPORAL ===\")\n",
    "    \n",
    "    # Verificar se existe coluna temporal\n",
    "    time_col = None\n",
    "    for col in ['timestamp', 'data', 'datetime', 'Data', 'Hora UTC']:\n",
    "        if col in df.columns:\n",
    "            time_col = col\n",
    "            break\n",
    "    \n",
    "    if time_col is None:\n",
    "        print(\"‚ùå Nenhuma coluna temporal encontrada\")\n",
    "        return\n",
    "    \n",
    "    # Converter para datetime se necess√°rio\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[time_col]):\n",
    "        try:\n",
    "            df[time_col] = pd.to_datetime(df[time_col])\n",
    "        except:\n",
    "            print(f\"‚ùå Erro ao converter {time_col} para datetime\")\n",
    "            return\n",
    "    \n",
    "    # Definir coluna de precipita√ß√£o\n",
    "    precip_col = None\n",
    "    for col in ['precipitacao_mm', 'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)', 'precipitacao']:\n",
    "        if col in df.columns:\n",
    "            precip_col = col\n",
    "            break\n",
    "    \n",
    "    if precip_col is None:\n",
    "        print(\"‚ùå Coluna de precipita√ß√£o n√£o encontrada\")\n",
    "        return\n",
    "    \n",
    "    # An√°lise temporal da precipita√ß√£o\n",
    "    df = df.copy()\n",
    "    df['ano'] = df[time_col].dt.year\n",
    "    df['mes'] = df[time_col].dt.month\n",
    "    df['hora'] = df[time_col].dt.hour\n",
    "    df['dia_semana'] = df[time_col].dt.dayofweek\n",
    "    \n",
    "    # An√°lise anual\n",
    "    annual_precip = df.groupby('ano')[precip_col].agg(['sum', 'mean', 'count']).reset_index()\n",
    "    \n",
    "    print(\"Precipita√ß√£o anual:\")\n",
    "    print(annual_precip.round(2))\n",
    "    \n",
    "    # An√°lise mensal\n",
    "    monthly_precip = df.groupby('mes')[precip_col].agg(['sum', 'mean', 'count']).reset_index()\n",
    "    \n",
    "    # Visualiza√ß√µes temporais\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Precipita√ß√£o anual\n",
    "    if len(annual_precip) > 1:\n",
    "        axes[0, 0].plot(annual_precip['ano'], annual_precip['sum'], marker='o')\n",
    "        axes[0, 0].set_title('Precipita√ß√£o Total Anual')\n",
    "        axes[0, 0].set_xlabel('Ano')\n",
    "        axes[0, 0].set_ylabel('Precipita√ß√£o (mm)')\n",
    "        axes[0, 0].grid(True)\n",
    "    \n",
    "    # Precipita√ß√£o mensal (padr√£o sazonal)\n",
    "    month_names = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun',\n",
    "                   'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez']\n",
    "    axes[0, 1].bar(range(1, 13), monthly_precip['sum'])\n",
    "    axes[0, 1].set_title('Precipita√ß√£o Total por M√™s')\n",
    "    axes[0, 1].set_xlabel('M√™s')\n",
    "    axes[0, 1].set_ylabel('Precipita√ß√£o (mm)')\n",
    "    axes[0, 1].set_xticks(range(1, 13))\n",
    "    axes[0, 1].set_xticklabels(month_names, rotation=45)\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Padr√£o hor√°rio\n",
    "    hourly_precip = df.groupby('hora')[precip_col].mean()\n",
    "    axes[1, 0].plot(hourly_precip.index, hourly_precip.values, marker='o')\n",
    "    axes[1, 0].set_title('Padr√£o Hor√°rio M√©dio de Precipita√ß√£o')\n",
    "    axes[1, 0].set_xlabel('Hora do Dia')\n",
    "    axes[1, 0].set_ylabel('Precipita√ß√£o M√©dia (mm)')\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Padr√£o semanal\n",
    "    weekday_names = ['Seg', 'Ter', 'Qua', 'Qui', 'Sex', 'S√°b', 'Dom']\n",
    "    weekly_precip = df.groupby('dia_semana')[precip_col].mean()\n",
    "    axes[1, 1].bar(range(7), weekly_precip.values)\n",
    "    axes[1, 1].set_title('Padr√£o Semanal de Precipita√ß√£o')\n",
    "    axes[1, 1].set_xlabel('Dia da Semana')\n",
    "    axes[1, 1].set_ylabel('Precipita√ß√£o M√©dia (mm)')\n",
    "    axes[1, 1].set_xticks(range(7))\n",
    "    axes[1, 1].set_xticklabels(weekday_names)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT_PATH / 'analise_temporal.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'annual': annual_precip,\n",
    "        'monthly': monthly_precip,\n",
    "        'hourly': hourly_precip,\n",
    "        'weekly': weekly_precip\n",
    "    }\n",
    "\n",
    "# Executar an√°lise temporal\n",
    "if data is not None:\n",
    "    temporal_analysis = analyze_temporal_patterns(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3471e1",
   "metadata": {},
   "source": [
    "## 5. An√°lise de Correla√ß√µes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20930ab5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_correlations(df):\n",
    "    \"\"\"\n",
    "    Analisa correla√ß√µes entre vari√°veis meteorol√≥gicas\n",
    "    \"\"\"\n",
    "    print(\"=== AN√ÅLISE DE CORRELA√á√ïES ===\")\n",
    "    \n",
    "    # Selecionar apenas colunas num√©ricas\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if len(numeric_cols) < 2:\n",
    "        print(\"‚ùå Insuficientes vari√°veis num√©ricas para an√°lise de correla√ß√£o\")\n",
    "        return\n",
    "    \n",
    "    # Calcular matriz de correla√ß√£o\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    print(\"Matriz de correla√ß√£o:\")\n",
    "    print(correlation_matrix.round(3))\n",
    "    \n",
    "    # Visualizar matriz de correla√ß√£o\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, \n",
    "                mask=mask,\n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={\"shrink\": .8})\n",
    "    \n",
    "    plt.title('Matriz de Correla√ß√£o - Vari√°veis Meteorol√≥gicas')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT_PATH / 'matriz_correlacao.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Identificar correla√ß√µes mais fortes\n",
    "    print(f\"\\n=== CORRELA√á√ïES MAIS FORTES ===\")\n",
    "    \n",
    "    # Converter matriz em lista de correla√ß√µes\n",
    "    correlations = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            var1 = correlation_matrix.columns[i]\n",
    "            var2 = correlation_matrix.columns[j]\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            \n",
    "            if not np.isnan(corr_value):\n",
    "                correlations.append({\n",
    "                    'variavel_1': var1,\n",
    "                    'variavel_2': var2,\n",
    "                    'correlacao': corr_value\n",
    "                })\n",
    "    \n",
    "    # Ordenar por valor absoluto da correla√ß√£o\n",
    "    correlations_df = pd.DataFrame(correlations)\n",
    "    correlations_df['correlacao_abs'] = correlations_df['correlacao'].abs()\n",
    "    correlations_df = correlations_df.sort_values('correlacao_abs', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 correla√ß√µes mais fortes:\")\n",
    "    print(correlations_df.head(10)[['variavel_1', 'variavel_2', 'correlacao']].round(3))\n",
    "    \n",
    "    return correlation_matrix, correlations_df\n",
    "\n",
    "# Executar an√°lise de correla√ß√µes\n",
    "if data is not None:\n",
    "    correlation_matrix, correlations_df = analyze_correlations(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45eb3d6",
   "metadata": {},
   "source": [
    "## 6. Detec√ß√£o de Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26396c55",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def detect_outliers(df):\n",
    "    \"\"\"\n",
    "    Detecta outliers usando m√∫ltiplos m√©todos\n",
    "    \"\"\"\n",
    "    print(\"=== DETEC√á√ÉO DE OUTLIERS ===\")\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if not numeric_cols:\n",
    "        print(\"‚ùå Nenhuma coluna num√©rica encontrada\")\n",
    "        return\n",
    "    \n",
    "    outlier_report = []\n",
    "    \n",
    "    for col in numeric_cols[:6]:  # Limitar a 6 vari√°veis principais\n",
    "        if col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        col_data = df[col].dropna()\n",
    "        \n",
    "        if len(col_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # M√©todo IQR\n",
    "        Q1 = col_data.quantile(0.25)\n",
    "        Q3 = col_data.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        iqr_outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]\n",
    "        \n",
    "        # M√©todo Z-Score\n",
    "        z_scores = np.abs((col_data - col_data.mean()) / col_data.std())\n",
    "        zscore_outliers = col_data[z_scores > 3]\n",
    "        \n",
    "        # M√©todo baseado em percentis\n",
    "        percentile_lower = col_data.quantile(0.01)\n",
    "        percentile_upper = col_data.quantile(0.99)\n",
    "        percentile_outliers = col_data[(col_data < percentile_lower) | (col_data > percentile_upper)]\n",
    "        \n",
    "        outlier_info = {\n",
    "            'variavel': col,\n",
    "            'total_valores': len(col_data),\n",
    "            'outliers_iqr': len(iqr_outliers),\n",
    "            'outliers_zscore': len(zscore_outliers),\n",
    "            'outliers_percentil': len(percentile_outliers),\n",
    "            'iqr_percent': (len(iqr_outliers) / len(col_data)) * 100,\n",
    "            'min_valor': col_data.min(),\n",
    "            'max_valor': col_data.max(),\n",
    "            'q1': Q1,\n",
    "            'q3': Q3,\n",
    "            'iqr_lower': lower_bound,\n",
    "            'iqr_upper': upper_bound\n",
    "        }\n",
    "        \n",
    "        outlier_report.append(outlier_info)\n",
    "    \n",
    "    # Converter para DataFrame\n",
    "    outlier_df = pd.DataFrame(outlier_report)\n",
    "    \n",
    "    print(\"Relat√≥rio de outliers:\")\n",
    "    print(outlier_df[['variavel', 'outliers_iqr', 'outliers_zscore', 'iqr_percent']].round(2))\n",
    "    \n",
    "    # Visualizar outliers para vari√°veis principais\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numeric_cols[:6]):\n",
    "        if col in df.columns:\n",
    "            col_data = df[col].dropna()\n",
    "            \n",
    "            # Box plot\n",
    "            axes[i].boxplot(col_data)\n",
    "            axes[i].set_title(f'Box Plot - {col}')\n",
    "            axes[i].set_ylabel(col)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT_PATH / 'outliers_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return outlier_df\n",
    "\n",
    "# Executar detec√ß√£o de outliers\n",
    "if data is not None:\n",
    "    outlier_report = detect_outliers(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529a4afe",
   "metadata": {},
   "source": [
    "## 7. An√°lise de Eventos Extremos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba851b9a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_extreme_events(df):\n",
    "    \"\"\"\n",
    "    Analisa eventos meteorol√≥gicos extremos\n",
    "    \"\"\"\n",
    "    print(\"=== AN√ÅLISE DE EVENTOS EXTREMOS ===\")\n",
    "    \n",
    "    # Identificar coluna de precipita√ß√£o\n",
    "    precip_col = None\n",
    "    for col in ['precipitacao_mm', 'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)', 'precipitacao']:\n",
    "        if col in df.columns:\n",
    "            precip_col = col\n",
    "            break\n",
    "    \n",
    "    if precip_col is None:\n",
    "        print(\"‚ùå Coluna de precipita√ß√£o n√£o encontrada\")\n",
    "        return\n",
    "    \n",
    "    precip_data = df[precip_col].dropna()\n",
    "    \n",
    "    if len(precip_data) == 0:\n",
    "        print(\"‚ùå Nenhum dado de precipita√ß√£o v√°lido\")\n",
    "        return\n",
    "    \n",
    "    # Definir thresholds para eventos extremos\n",
    "    p95 = precip_data.quantile(0.95)\n",
    "    p99 = precip_data.quantile(0.99)\n",
    "    p99_9 = precip_data.quantile(0.999)\n",
    "    \n",
    "    # Identificar eventos extremos\n",
    "    eventos_moderados = precip_data[precip_data >= p95]\n",
    "    eventos_severos = precip_data[precip_data >= p99]\n",
    "    eventos_extremos = precip_data[precip_data >= p99_9]\n",
    "    \n",
    "    print(f\"Thresholds de precipita√ß√£o:\")\n",
    "    print(f\"  P95 (eventos moderados): {p95:.2f} mm/h\")\n",
    "    print(f\"  P99 (eventos severos): {p99:.2f} mm/h\")\n",
    "    print(f\"  P99.9 (eventos extremos): {p99_9:.2f} mm/h\")\n",
    "    \n",
    "    print(f\"\\nContagem de eventos:\")\n",
    "    print(f\"  Eventos moderados (>= P95): {len(eventos_moderados)}\")\n",
    "    print(f\"  Eventos severos (>= P99): {len(eventos_severos)}\")\n",
    "    print(f\"  Eventos extremos (>= P99.9): {len(eventos_extremos)}\")\n",
    "    \n",
    "    # Estat√≠sticas dos eventos extremos\n",
    "    print(f\"\\nEstat√≠sticas dos eventos extremos:\")\n",
    "    if len(eventos_extremos) > 0:\n",
    "        print(f\"  Precipita√ß√£o m√°xima: {eventos_extremos.max():.2f} mm/h\")\n",
    "        print(f\"  Precipita√ß√£o m√©dia em eventos extremos: {eventos_extremos.mean():.2f} mm/h\")\n",
    "    \n",
    "    # Visualizar distribui√ß√£o de eventos extremos\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Histograma de precipita√ß√£o com thresholds\n",
    "    axes[0].hist(precip_data, bins=100, alpha=0.7, edgecolor='black', density=True)\n",
    "    axes[0].axvline(p95, color='orange', linestyle='--', label=f'P95: {p95:.2f}')\n",
    "    axes[0].axvline(p99, color='red', linestyle='--', label=f'P99: {p99:.2f}')\n",
    "    axes[0].axvline(p99_9, color='darkred', linestyle='--', label=f'P99.9: {p99_9:.2f}')\n",
    "    axes[0].set_title('Distribui√ß√£o de Precipita√ß√£o com Thresholds')\n",
    "    axes[0].set_xlabel('Precipita√ß√£o (mm/h)')\n",
    "    axes[0].set_ylabel('Densidade')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xlim(0, min(50, precip_data.max()))  # Limitar visualiza√ß√£o\n",
    "    \n",
    "    # Box plot comparativo\n",
    "    event_data = []\n",
    "    event_labels = []\n",
    "    \n",
    "    normal_events = precip_data[precip_data < p95]\n",
    "    if len(normal_events) > 0:\n",
    "        event_data.append(normal_events)\n",
    "        event_labels.append('Normal')\n",
    "    \n",
    "    if len(eventos_moderados) > 0:\n",
    "        event_data.append(eventos_moderados)\n",
    "        event_labels.append('Moderado')\n",
    "    \n",
    "    if len(eventos_severos) > 0:\n",
    "        event_data.append(eventos_severos)\n",
    "        event_labels.append('Severo')\n",
    "    \n",
    "    if len(eventos_extremos) > 0:\n",
    "        event_data.append(eventos_extremos)\n",
    "        event_labels.append('Extremo')\n",
    "    \n",
    "    if event_data:\n",
    "        axes[1].boxplot(event_data, labels=event_labels)\n",
    "        axes[1].set_title('Compara√ß√£o de Eventos por Intensidade')\n",
    "        axes[1].set_ylabel('Precipita√ß√£o (mm/h)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_OUTPUT_PATH / 'eventos_extremos.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'thresholds': {'p95': p95, 'p99': p99, 'p99_9': p99_9},\n",
    "        'eventos_moderados': len(eventos_moderados),\n",
    "        'eventos_severos': len(eventos_severos),\n",
    "        'eventos_extremos': len(eventos_extremos),\n",
    "        'max_precipitacao': precip_data.max(),\n",
    "        'dados_eventos_extremos': eventos_extremos if len(eventos_extremos) > 0 else None\n",
    "    }\n",
    "\n",
    "# Executar an√°lise de eventos extremos\n",
    "if data is not None:\n",
    "    extreme_events = analyze_extreme_events(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a7fce2",
   "metadata": {},
   "source": [
    "## 8. Relat√≥rio Final da An√°lise Explorat√≥ria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac179ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_report():\n",
    "    \"\"\"\n",
    "    Gera relat√≥rio final da an√°lise explorat√≥ria\n",
    "    \"\"\"\n",
    "    print(\"=== RELAT√ìRIO FINAL DA AN√ÅLISE EXPLORAT√ìRIA ===\")\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'dados': {\n",
    "            'shape': data.shape if data is not None else None,\n",
    "            'periodo': f\"{data['timestamp'].min()} - {data['timestamp'].max()}\" if data is not None and 'timestamp' in data.columns else 'N/A',\n",
    "            'total_registros': len(data) if data is not None else 0\n",
    "        },\n",
    "        'qualidade': {},\n",
    "        'principais_insights': []\n",
    "    }\n",
    "    \n",
    "    if data is not None:\n",
    "        # Resumo da qualidade\n",
    "        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        total_missing = data.isnull().sum().sum()\n",
    "        missing_percent = (total_missing / (len(data) * len(data.columns))) * 100\n",
    "        \n",
    "        report['qualidade'] = {\n",
    "            'variaveis_numericas': len(numeric_cols),\n",
    "            'total_missing': int(total_missing),\n",
    "            'missing_percent': round(missing_percent, 2),\n",
    "            'registros_completos': int(len(data.dropna()))\n",
    "        }\n",
    "        \n",
    "        # Insights principais\n",
    "        insights = [\n",
    "            \"‚úì Dados meteorol√≥gicos INMET carregados e analisados com sucesso\",\n",
    "            f\"‚úì Dataset cont√©m {len(data)} registros e {len(data.columns)} vari√°veis\",\n",
    "            f\"‚úì {missing_percent:.1f}% de dados faltantes identificados\",\n",
    "        ]\n",
    "        \n",
    "        # Insights espec√≠ficos baseados na an√°lise\n",
    "        if 'quality_report' in locals() and quality_report is not None:\n",
    "            high_missing_vars = quality_report[quality_report['missing_percent'] > 20]\n",
    "            if len(high_missing_vars) > 0:\n",
    "                insights.append(f\"‚ö†Ô∏è {len(high_missing_vars)} vari√°veis com >20% de dados faltantes\")\n",
    "        \n",
    "        if 'extreme_events' in locals() and extreme_events is not None:\n",
    "            insights.append(f\"‚úì {extreme_events['eventos_extremos']} eventos de precipita√ß√£o extrema identificados\")\n",
    "            insights.append(f\"‚úì Precipita√ß√£o m√°xima registrada: {extreme_events['max_precipitacao']:.2f} mm/h\")\n",
    "        \n",
    "        if 'correlations_df' in locals() and correlations_df is not None:\n",
    "            strong_corrs = correlations_df[correlations_df['correlacao_abs'] > 0.7]\n",
    "            if len(strong_corrs) > 0:\n",
    "                insights.append(f\"‚úì {len(strong_corrs)} correla√ß√µes fortes (>0.7) entre vari√°veis\")\n",
    "        \n",
    "        insights.extend([\n",
    "            \"‚úì Padr√µes sazonais e hor√°rios identificados na precipita√ß√£o\",\n",
    "            \"‚úì Outliers detectados e analisados usando m√∫ltiplos m√©todos\",\n",
    "            \"‚úì An√°lise de eventos extremos conclu√≠da\",\n",
    "            \"‚úì Dados prontos para fase de preprocessamento\"\n",
    "        ])\n",
    "        \n",
    "        report['principais_insights'] = insights\n",
    "    \n",
    "    # Salvar relat√≥rio\n",
    "    import json\n",
    "    with open(ANALYSIS_OUTPUT_PATH / 'relatorio_analise_exploratoria.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Exibir resumo\n",
    "    print(\"\\nüìã RESUMO EXECUTIVO:\")\n",
    "    for insight in report['principais_insights']:\n",
    "        print(f\"  {insight}\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Arquivos gerados:\")\n",
    "    output_files = list(ANALYSIS_OUTPUT_PATH.glob(\"*\"))\n",
    "    for file in output_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ An√°lise explorat√≥ria conclu√≠da com sucesso!\")\n",
    "    print(f\"üìä Relat√≥rio completo salvo em: {ANALYSIS_OUTPUT_PATH}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Gerar relat√≥rio final\n",
    "final_report = generate_final_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d455a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ AN√ÅLISE EXPLORAT√ìRIA CONCLU√çDA!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPr√≥ximos passos recomendados:\")\n",
    "print(\"1. üìù Revisar relat√≥rio de qualidade dos dados\")\n",
    "print(\"2. üßπ Executar preprocessamento baseado nos insights\")\n",
    "print(\"3. üîß Tratar valores missing e outliers identificados\")\n",
    "print(\"4. üìà Preparar features para modelagem LSTM\")\n",
    "print(\"5. ‚ö° Prosseguir para treinamento do modelo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c41b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
