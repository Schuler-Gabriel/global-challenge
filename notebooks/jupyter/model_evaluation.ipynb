{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02612aab",
   "metadata": {},
   "source": [
    "# Avalia√ß√£o do Modelo LSTM - Previs√£o Meteorol√≥gica\n",
    "\n",
    "Este notebook avalia a performance do modelo LSTM treinado para previs√£o de chuva.\n",
    "\n",
    "## Objetivos:\n",
    "- Carregar modelo treinado\n",
    "- Avaliar m√©tricas de performance\n",
    "- An√°lise de erros e casos extremos\n",
    "- Visualiza√ß√µes de resultados\n",
    "- Relat√≥rio final de avalia√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1eeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports necess√°rios\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=== AVALIA√á√ÉO DO MODELO LSTM ===\")\n",
    "print(f\"Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f42b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Configura√ß√£o dos caminhos\n",
    "DATA_PATH = Path('../data')\n",
    "PROCESSED_DATA_PATH = DATA_PATH / 'processed'\n",
    "MODELS_PATH = DATA_PATH / 'modelos_treinados'\n",
    "EVALUATION_PATH = DATA_PATH / 'evaluation'\n",
    "\n",
    "EVALUATION_PATH.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"Dados processados: {PROCESSED_DATA_PATH}\")\n",
    "print(f\"Modelos: {MODELS_PATH}\")\n",
    "print(f\"Avalia√ß√£o: {EVALUATION_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18883d6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_test_data():\n",
    "    \"\"\"\n",
    "    Carrega dados de teste\n",
    "    \"\"\"\n",
    "    print(\"=== CARREGAMENTO DOS DADOS DE TESTE ===\")\n",
    "    \n",
    "    try:\n",
    "        test_data = pd.read_parquet(PROCESSED_DATA_PATH / 'test_data.parquet')\n",
    "        print(f\"‚úì Dados de teste carregados: {test_data.shape}\")\n",
    "        return test_data\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå Dados de teste n√£o encontrados!\")\n",
    "        # Criar dados sint√©ticos para demonstra√ß√£o\n",
    "        dates = pd.date_range('2023-01-01', '2023-12-31', freq='H')\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        test_data = pd.DataFrame({\n",
    "            'timestamp': dates,\n",
    "            'precipitacao_mm': np.random.exponential(0.3, len(dates)),\n",
    "            'temperatura_c': 20 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 3, len(dates)),\n",
    "            'umidade_relativa': np.clip(50 + 30 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 8, len(dates)), 0, 100),\n",
    "            'pressao_mb': 1013 + np.random.normal(0, 15, len(dates)),\n",
    "            'velocidade_vento_ms': np.random.gamma(2, 1.5, len(dates)),\n",
    "            'direcao_vento_gr': np.random.uniform(0, 360, len(dates)),\n",
    "            'radiacao_kjm2': np.maximum(0, 800 * np.sin(2 * np.pi * np.arange(len(dates)) / 24) + np.random.normal(0, 150, len(dates)))\n",
    "        })\n",
    "        \n",
    "        print(\"üìä Dados sint√©ticos criados para demonstra√ß√£o\")\n",
    "        return test_data\n",
    "\n",
    "# Carregar dados de teste\n",
    "test_data = load_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a5a4c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_trained_model():\n",
    "    \"\"\"\n",
    "    Carrega modelo LSTM treinado\n",
    "    \"\"\"\n",
    "    print(\"=== CARREGAMENTO DO MODELO ===\")\n",
    "    \n",
    "    # Procurar modelos dispon√≠veis\n",
    "    if MODELS_PATH.exists():\n",
    "        model_files = list(MODELS_PATH.glob(\"*.h5\")) + list(MODELS_PATH.glob(\"*.keras\"))\n",
    "        \n",
    "        if model_files:\n",
    "            # Usar o modelo mais recente\n",
    "            latest_model = max(model_files, key=lambda x: x.stat().st_mtime)\n",
    "            print(f\"Carregando modelo: {latest_model.name}\")\n",
    "            \n",
    "            try:\n",
    "                model = tf.keras.models.load_model(latest_model)\n",
    "                print(f\"‚úì Modelo carregado com sucesso\")\n",
    "                print(f\"Arquitetura: {model.summary()}\")\n",
    "                return model\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Erro ao carregar modelo: {e}\")\n",
    "        else:\n",
    "            print(\"‚ùå Nenhum modelo encontrado!\")\n",
    "    else:\n",
    "        print(\"‚ùå Diret√≥rio de modelos n√£o existe!\")\n",
    "    \n",
    "    # Criar modelo dummy para demonstra√ß√£o\n",
    "    print(\"üîß Criando modelo dummy para demonstra√ß√£o...\")\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True, input_shape=(24, 8)),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='linear')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    print(\"‚úì Modelo dummy criado\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Carregar modelo\n",
    "model = load_trained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d8cee2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_sequences_for_prediction(data, sequence_length=24):\n",
    "    \"\"\"\n",
    "    Prepara sequ√™ncias para predi√ß√£o\n",
    "    \"\"\"\n",
    "    print(\"=== PREPARA√á√ÉO DE SEQU√äNCIAS ===\")\n",
    "    \n",
    "    # Selecionar features num√©ricas\n",
    "    feature_cols = ['precipitacao_mm', 'temperatura_c', 'umidade_relativa', 'pressao_mb',\n",
    "                   'velocidade_vento_ms', 'direcao_vento_gr', 'radiacao_kjm2']\n",
    "    \n",
    "    # Usar apenas colunas dispon√≠veis\n",
    "    available_cols = [col for col in feature_cols if col in data.columns]\n",
    "    \n",
    "    if len(available_cols) < 3:\n",
    "        print(\"‚ùå Insuficientes features num√©ricas\")\n",
    "        return None, None, None\n",
    "    \n",
    "    print(f\"Features utilizadas: {available_cols}\")\n",
    "    \n",
    "    # Extrair features\n",
    "    features = data[available_cols].values\n",
    "    \n",
    "    # Preencher com colunas extras se necess√°rio (para compatibilidade)\n",
    "    if features.shape[1] < 8:\n",
    "        padding = np.zeros((features.shape[0], 8 - features.shape[1]))\n",
    "        features = np.hstack([features, padding])\n",
    "    \n",
    "    # Criar sequ√™ncias\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(sequence_length, len(features)):\n",
    "        X.append(features[i-sequence_length:i])\n",
    "        y.append(features[i, 0])  # Predizer precipita√ß√£o (primeira coluna)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    print(f\"‚úì Sequ√™ncias criadas: X={X.shape}, y={y.shape}\")\n",
    "    \n",
    "    return X, y, available_cols\n",
    "\n",
    "# Preparar sequ√™ncias\n",
    "X_test, y_test, feature_names = prepare_sequences_for_prediction(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21600b9d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_regression_metrics(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Avalia m√©tricas de regress√£o\n",
    "    \"\"\"\n",
    "    print(\"=== M√âTRICAS DE REGRESS√ÉO ===\")\n",
    "    \n",
    "    if X_test is None or y_test is None:\n",
    "        print(\"‚ùå Dados de teste n√£o dispon√≠veis\")\n",
    "        return None\n",
    "    \n",
    "    # Fazer predi√ß√µes\n",
    "    print(\"Fazendo predi√ß√µes...\")\n",
    "    y_pred = model.predict(X_test, verbose=0)\n",
    "    y_pred = y_pred.flatten()\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # M√©tricas espec√≠ficas para precipita√ß√£o\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / np.maximum(y_test, 0.01))) * 100\n",
    "    \n",
    "    metrics = {\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R¬≤': r2,\n",
    "        'MAPE': mape\n",
    "    }\n",
    "    \n",
    "    print(\"M√©tricas de regress√£o:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"  {metric}: {value:.4f}\")\n",
    "    \n",
    "    # Verificar se atende crit√©rios\n",
    "    print(f\"\\nüìä Avalia√ß√£o dos crit√©rios:\")\n",
    "    print(f\"  MAE < 2.0 mm/h: {'‚úÖ' if mae < 2.0 else '‚ùå'} (atual: {mae:.3f})\")\n",
    "    print(f\"  RMSE < 3.0 mm/h: {'‚úÖ' if rmse < 3.0 else '‚ùå'} (atual: {rmse:.3f})\")\n",
    "    print(f\"  R¬≤ > 0.5: {'‚úÖ' if r2 > 0.5 else '‚ùå'} (atual: {r2:.3f})\")\n",
    "    \n",
    "    return metrics, y_pred\n",
    "\n",
    "# Avaliar m√©tricas de regress√£o\n",
    "if model is not None:\n",
    "    regression_metrics, predictions = evaluate_regression_metrics(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b3f46a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_metrics(y_true, y_pred, threshold=1.0):\n",
    "    \"\"\"\n",
    "    Avalia m√©tricas de classifica√ß√£o para eventos de chuva\n",
    "    \"\"\"\n",
    "    print(\"=== M√âTRICAS DE CLASSIFICA√á√ÉO ===\")\n",
    "    \n",
    "    # Converter para classifica√ß√£o bin√°ria (chuva/sem chuva)\n",
    "    y_true_class = (y_true >= threshold).astype(int)\n",
    "    y_pred_class = (y_pred >= threshold).astype(int)\n",
    "    \n",
    "    # M√©tricas de classifica√ß√£o\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    \n",
    "    accuracy = accuracy_score(y_true_class, y_pred_class)\n",
    "    precision = precision_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    recall = recall_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    f1 = f1_score(y_true_class, y_pred_class, zero_division=0)\n",
    "    \n",
    "    print(f\"M√©tricas de classifica√ß√£o (threshold={threshold} mm/h):\")\n",
    "    print(f\"  Accuracy: {accuracy:.3f}\")\n",
    "    print(f\"  Precision: {precision:.3f}\")\n",
    "    print(f\"  Recall: {recall:.3f}\")\n",
    "    print(f\"  F1-Score: {f1:.3f}\")\n",
    "    \n",
    "    # Verificar crit√©rio de accuracy > 75%\n",
    "    print(f\"\\nüìä Crit√©rio de classifica√ß√£o:\")\n",
    "    print(f\"  Accuracy > 75%: {'‚úÖ' if accuracy > 0.75 else '‚ùå'} (atual: {accuracy:.1%})\")\n",
    "    \n",
    "    # Matriz de confus√£o\n",
    "    cm = confusion_matrix(y_true_class, y_pred_class)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Sem Chuva', 'Com Chuva'],\n",
    "                yticklabels=['Sem Chuva', 'Com Chuva'])\n",
    "    plt.title('Matriz de Confus√£o - Eventos de Chuva')\n",
    "    plt.ylabel('Valores Reais')\n",
    "    plt.xlabel('Predi√ß√µes')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EVALUATION_PATH / 'confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "\n",
    "# Avaliar m√©tricas de classifica√ß√£o\n",
    "if 'predictions' in locals() and y_test is not None:\n",
    "    classification_metrics = evaluate_classification_metrics(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c97ba1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_prediction_errors(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Analisa erros de predi√ß√£o\n",
    "    \"\"\"\n",
    "    print(\"=== AN√ÅLISE DE ERROS ===\")\n",
    "    \n",
    "    # Calcular erros\n",
    "    errors = y_pred - y_true\n",
    "    abs_errors = np.abs(errors)\n",
    "    \n",
    "    # Estat√≠sticas dos erros\n",
    "    print(\"Estat√≠sticas dos erros:\")\n",
    "    print(f\"  Erro m√©dio: {np.mean(errors):.4f}\")\n",
    "    print(f\"  Erro absoluto m√©dio: {np.mean(abs_errors):.4f}\")\n",
    "    print(f\"  Desvio padr√£o dos erros: {np.std(errors):.4f}\")\n",
    "    print(f\"  Erro m√°ximo: {np.max(abs_errors):.4f}\")\n",
    "    \n",
    "    # Percentis dos erros\n",
    "    print(f\"\\nPercentis dos erros absolutos:\")\n",
    "    for p in [50, 75, 90, 95, 99]:\n",
    "        print(f\"  P{p}: {np.percentile(abs_errors, p):.4f}\")\n",
    "    \n",
    "    # Visualiza√ß√µes dos erros\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Scatter plot: predito vs real\n",
    "    axes[0, 0].scatter(y_true, y_pred, alpha=0.5)\n",
    "    axes[0, 0].plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)\n",
    "    axes[0, 0].set_xlabel('Valores Reais')\n",
    "    axes[0, 0].set_ylabel('Predi√ß√µes')\n",
    "    axes[0, 0].set_title('Predito vs Real')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribui√ß√£o dos erros\n",
    "    axes[0, 1].hist(errors, bins=50, alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].axvline(0, color='red', linestyle='--')\n",
    "    axes[0, 1].set_xlabel('Erro (Predito - Real)')\n",
    "    axes[0, 1].set_ylabel('Frequ√™ncia')\n",
    "    axes[0, 1].set_title('Distribui√ß√£o dos Erros')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Erros absolutos ao longo do tempo\n",
    "    axes[1, 0].plot(abs_errors[:1000])  # Primeiro 1000 pontos\n",
    "    axes[1, 0].set_xlabel('√çndice Temporal')\n",
    "    axes[1, 0].set_ylabel('Erro Absoluto')\n",
    "    axes[1, 0].set_title('Erros Absolutos ao Longo do Tempo')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-Q plot dos erros\n",
    "    from scipy import stats\n",
    "    stats.probplot(errors, dist=\"norm\", plot=axes[1, 1])\n",
    "    axes[1, 1].set_title('Q-Q Plot dos Erros')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EVALUATION_PATH / 'error_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'mean_error': np.mean(errors),\n",
    "        'mean_abs_error': np.mean(abs_errors),\n",
    "        'std_error': np.std(errors),\n",
    "        'max_abs_error': np.max(abs_errors),\n",
    "        'percentiles': {p: np.percentile(abs_errors, p) for p in [50, 75, 90, 95, 99]}\n",
    "    }\n",
    "\n",
    "# Analisar erros\n",
    "if 'predictions' in locals() and y_test is not None:\n",
    "    error_analysis = analyze_prediction_errors(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b472e3f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def analyze_extreme_events(y_true, y_pred, threshold_percentile=95):\n",
    "    \"\"\"\n",
    "    Analisa performance em eventos extremos\n",
    "    \"\"\"\n",
    "    print(\"=== AN√ÅLISE DE EVENTOS EXTREMOS ===\")\n",
    "    \n",
    "    # Definir threshold para eventos extremos\n",
    "    threshold = np.percentile(y_true, threshold_percentile)\n",
    "    \n",
    "    # Identificar eventos extremos\n",
    "    extreme_mask = y_true >= threshold\n",
    "    \n",
    "    if np.sum(extreme_mask) == 0:\n",
    "        print(\"‚ùå Nenhum evento extremo encontrado\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Threshold P{threshold_percentile}: {threshold:.3f} mm/h\")\n",
    "    print(f\"Eventos extremos: {np.sum(extreme_mask)} ({np.mean(extreme_mask)*100:.1f}%)\")\n",
    "    \n",
    "    # M√©tricas espec√≠ficas para eventos extremos\n",
    "    y_true_extreme = y_true[extreme_mask]\n",
    "    y_pred_extreme = y_pred[extreme_mask]\n",
    "    \n",
    "    mae_extreme = mean_absolute_error(y_true_extreme, y_pred_extreme)\n",
    "    rmse_extreme = np.sqrt(mean_squared_error(y_true_extreme, y_pred_extreme))\n",
    "    r2_extreme = r2_score(y_true_extreme, y_pred_extreme)\n",
    "    \n",
    "    print(f\"\\nM√©tricas para eventos extremos:\")\n",
    "    print(f\"  MAE: {mae_extreme:.4f}\")\n",
    "    print(f\"  RMSE: {rmse_extreme:.4f}\")\n",
    "    print(f\"  R¬≤: {r2_extreme:.4f}\")\n",
    "    \n",
    "    # An√°lise de detec√ß√£o de eventos extremos\n",
    "    y_pred_extreme_class = y_pred >= threshold\n",
    "    precision_extreme = np.sum(extreme_mask & y_pred_extreme_class) / np.sum(y_pred_extreme_class) if np.sum(y_pred_extreme_class) > 0 else 0\n",
    "    recall_extreme = np.sum(extreme_mask & y_pred_extreme_class) / np.sum(extreme_mask)\n",
    "    \n",
    "    print(f\"\\nDetec√ß√£o de eventos extremos:\")\n",
    "    print(f\"  Precision: {precision_extreme:.3f}\")\n",
    "    print(f\"  Recall: {recall_extreme:.3f}\")\n",
    "    \n",
    "    # Visualiza√ß√£o\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(y_true_extreme, y_pred_extreme, alpha=0.7, color='red')\n",
    "    plt.plot([y_true_extreme.min(), y_true_extreme.max()], \n",
    "             [y_true_extreme.min(), y_true_extreme.max()], 'k--')\n",
    "    plt.xlabel('Valores Reais')\n",
    "    plt.ylabel('Predi√ß√µes')\n",
    "    plt.title('Eventos Extremos - Predito vs Real')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    errors_extreme = y_pred_extreme - y_true_extreme\n",
    "    plt.hist(errors_extreme, bins=20, alpha=0.7, edgecolor='black')\n",
    "    plt.axvline(0, color='red', linestyle='--')\n",
    "    plt.xlabel('Erro')\n",
    "    plt.ylabel('Frequ√™ncia')\n",
    "    plt.title('Distribui√ß√£o dos Erros - Eventos Extremos')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(EVALUATION_PATH / 'extreme_events_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'threshold': threshold,\n",
    "        'count': np.sum(extreme_mask),\n",
    "        'mae': mae_extreme,\n",
    "        'rmse': rmse_extreme,\n",
    "        'r2': r2_extreme,\n",
    "        'precision': precision_extreme,\n",
    "        'recall': recall_extreme\n",
    "    }\n",
    "\n",
    "# Analisar eventos extremos\n",
    "if 'predictions' in locals() and y_test is not None:\n",
    "    extreme_events_analysis = analyze_extreme_events(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4245cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_evaluation_report():\n",
    "    \"\"\"\n",
    "    Gera relat√≥rio final de avalia√ß√£o\n",
    "    \"\"\"\n",
    "    print(\"=== RELAT√ìRIO FINAL DE AVALIA√á√ÉO ===\")\n",
    "    \n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'model_info': {\n",
    "            'architecture': 'LSTM',\n",
    "            'test_samples': len(y_test) if y_test is not None else 0\n",
    "        },\n",
    "        'regression_metrics': regression_metrics if 'regression_metrics' in locals() else {},\n",
    "        'classification_metrics': classification_metrics if 'classification_metrics' in locals() else {},\n",
    "        'error_analysis': error_analysis if 'error_analysis' in locals() else {},\n",
    "        'extreme_events': extreme_events_analysis if 'extreme_events_analysis' in locals() else {},\n",
    "        'criteria_evaluation': {}\n",
    "    }\n",
    "    \n",
    "    # Avaliar crit√©rios de sucesso\n",
    "    if 'regression_metrics' in locals() and regression_metrics:\n",
    "        mae_ok = regression_metrics['MAE'] < 2.0\n",
    "        rmse_ok = regression_metrics['RMSE'] < 3.0\n",
    "        r2_ok = regression_metrics['R¬≤'] > 0.5\n",
    "        \n",
    "        report['criteria_evaluation']['mae_criterion'] = {\n",
    "            'target': '< 2.0 mm/h',\n",
    "            'actual': regression_metrics['MAE'],\n",
    "            'passed': mae_ok\n",
    "        }\n",
    "        \n",
    "        report['criteria_evaluation']['rmse_criterion'] = {\n",
    "            'target': '< 3.0 mm/h',\n",
    "            'actual': regression_metrics['RMSE'],\n",
    "            'passed': rmse_ok\n",
    "        }\n",
    "        \n",
    "        report['criteria_evaluation']['r2_criterion'] = {\n",
    "            'target': '> 0.5',\n",
    "            'actual': regression_metrics['R¬≤'],\n",
    "            'passed': r2_ok\n",
    "        }\n",
    "    \n",
    "    if 'classification_metrics' in locals() and classification_metrics:\n",
    "        accuracy_ok = classification_metrics['accuracy'] > 0.75\n",
    "        \n",
    "        report['criteria_evaluation']['accuracy_criterion'] = {\n",
    "            'target': '> 75%',\n",
    "            'actual': classification_metrics['accuracy'],\n",
    "            'passed': accuracy_ok\n",
    "        }\n",
    "    \n",
    "    # Salvar relat√≥rio\n",
    "    import json\n",
    "    with open(EVALUATION_PATH / 'evaluation_report.json', 'w') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Exibir resumo\n",
    "    print(\"\\nüìã RESUMO DA AVALIA√á√ÉO:\")\n",
    "    \n",
    "    if 'regression_metrics' in locals():\n",
    "        print(\"‚úì M√©tricas de regress√£o calculadas\")\n",
    "    \n",
    "    if 'classification_metrics' in locals():\n",
    "        print(\"‚úì M√©tricas de classifica√ß√£o calculadas\")\n",
    "    \n",
    "    if 'error_analysis' in locals():\n",
    "        print(\"‚úì An√°lise de erros conclu√≠da\")\n",
    "    \n",
    "    if 'extreme_events_analysis' in locals():\n",
    "        print(\"‚úì An√°lise de eventos extremos conclu√≠da\")\n",
    "    \n",
    "    print(f\"\\nüìÅ Arquivos gerados:\")\n",
    "    output_files = list(EVALUATION_PATH.glob(\"*\"))\n",
    "    for file in output_files:\n",
    "        print(f\"  - {file.name}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Avalia√ß√£o conclu√≠da!\")\n",
    "    print(f\"üìä Relat√≥rio completo salvo em: {EVALUATION_PATH}\")\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Gerar relat√≥rio final\n",
    "final_report = generate_evaluation_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd8d2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ AVALIA√á√ÉO DO MODELO CONCLU√çDA!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPr√≥ximos passos recomendados:\")\n",
    "print(\"1. üìä Revisar m√©tricas de performance\")\n",
    "print(\"2. üîß Ajustar arquitetura se necess√°rio\")\n",
    "print(\"3. üìà Otimizar hiperpar√¢metros\")\n",
    "print(\"4. üöÄ Preparar para deploy em produ√ß√£o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd72f47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
