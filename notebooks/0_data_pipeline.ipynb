{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# üîÑ Pipeline de Dados Atmosf√©ricos\n",
        "\n",
        "Este notebook executa o pipeline completo de coleta e processamento dos dados atmosf√©ricos para o modelo h√≠brido LSTM.\n",
        "\n",
        "## Pipeline:\n",
        "1. **Coleta de Dados**: Open-Meteo APIs (Historical Forecast + Weather)\n",
        "2. **Feature Engineering**: Cria√ß√£o de 149 vari√°veis atmosf√©ricas\n",
        "3. **Preprocessamento**: Normaliza√ß√£o e sequ√™ncias temporais\n",
        "4. **Valida√ß√£o**: Qualidade e consist√™ncia dos dados\n",
        "\n",
        "## APIs Utilizadas:\n",
        "- **Historical Forecast API**: 149 vari√°veis, 2022-2025 (fonte principal)\n",
        "- **Historical Weather API**: 25 vari√°veis, 2000-2024 (extens√£o temporal)\n",
        "- **INMET**: Valida√ß√£o local Porto Alegre\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ INICIANDO PIPELINE DE PROCESSAMENTO DE DADOS\n",
            "üìÖ 2025-06-06 14:58:45\n",
            "üìä VERIFICANDO DADOS EXISTENTES\n",
            "========================================\n",
            "‚úÖ Open-Meteo Historical Forecast: 14 arquivo(s) (0 CSV, 14 JSON)\n",
            "‚úÖ Open-Meteo Historical Weather: 75 arquivo(s) (50 CSV, 25 JSON)\n",
            "‚úÖ INMET: 0 arquivo(s) (0 CSV, 0 JSON)\n",
            "\n",
            "‚úÖ Dados brutos encontrados!\n",
            "\n",
            "üîÑ Executando processamento dos dados existentes\n",
            "==================================================\n",
            "‚úÖ Processamento - SUCESSO\n",
            "üì§ Output:\n",
            "üöÄ INICIANDO PROCESSAMENTO DOS DADOS EXISTENTES\n",
            "==================================================\n",
            "üìÖ 2025-06-06 14:58:46\n",
            "‚úÖ Diret√≥rios configurados\n",
            "\n",
            "üîÑ Processando Open-Meteo Historical Forecast...\n",
            "üìÅ Encontrados 14 arquivos JSON\n",
            "‚úÖ Carregado: chunk_20221231_to_20230331.json - (2184, 49)\n",
            "‚úÖ Carregado: chunk_20230930_to_20231229.json - (2184, 49)\n",
            "‚úÖ Carregado: chunk_20240330_to_20240628.json - (2184, 49)\n",
            "‚úÖ Carregado: chunk_20221001_to_20221230.json - (2184, 49)\n",
            "‚úÖ Carregado: chunk_20220101_to_20220401.json - (2184, 49)\n",
            "‚úÖ Salvo: /Users/gabrielschuler/Developer/Fiap/Challenge/data/processed/openmeteo_historical_forecast.csv\n",
            "üìä Shape final: (10920, 49)\n",
            "üóìÔ∏è Per√≠odo: 2022-01-01 00:00:00 at√© 2024-06-28 23:00:00\n",
            "\n",
            "üîÑ Processando Open-Meteo Historical Weather...\n",
            "üìÅ Encontrados 25 arquivos (hor√°rias)\n",
            "‚úÖ Carregado: open_meteo_hourly_2007.csv - (8760, 22)\n",
            "‚úÖ Carregado: open_meteo_hourly_2013.csv - (8760, 22)\n",
            "‚úÖ Carregado: open_meteo_hourly_2012.csv - (8784, 22)\n",
            "‚úÖ Carregado: open_meteo_hourly_2006.csv - (8760, 22)\n",
            "‚úÖ Carregado: open_meteo_hourly_2010.csv - (8760, 10)\n",
            "‚úÖ Carregado: open_meteo_hourly_2004.csv - (8784, 22)\n",
            "‚úÖ Carregado: open_meteo_hourly_2005.csv - (8760, 22)\n",
            "‚úÖ Carregado: open_meteo_hourly_2011.csv - (8760, 10)\n",
            "‚úÖ Carregado: open_meteo_hourly_2015.csv - (8760, 22)\n",
            "‚úÖ Carregado: open_meteo_hourly_2001.csv - (8760, 22)\n",
            "‚úÖ Salvo: /Users/gabrielschuler/Developer/Fiap/Challenge/data/processed/openmeteo_historical_weather.csv\n",
            "üìä Shape final: (87648, 22)\n",
            "\n",
            "üîÑ Processando dados INMET...\n",
            "üìÅ Encontrados 30 arquivos INMET\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE_01-01-2020_A_31-12-2020.CSV - (8784, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8784/8784 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE_01-01-2006_A_31-12-2006.CSV - (8760, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8760/8760 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE_01-01-2002_A_31-12-2002.CSV - (8760, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8760/8760 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE_01-01-2012_A_31-12-2012.CSV - (8784, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8784/8784 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE_01-01-2016_A_31-12-2016.CSV - (8784, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8784/8784 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE - JARDIM BOTANICO_01-01-2022_A_31-12-2022.CSV - (8760, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8760/8760 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE_01-01-2021_A_31-12-2021.CSV - (8760, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8760/8760 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE - JARDIM BOTANICO_01-01-2025_A_30-04-2025.CSV - (2880, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 2880/2880 registros\n",
            "‚úÖ Carregado: INMET_S_RS_B807_PORTO ALEGRE- BELEM NOVO_01-01-2023_A_31-12-2023.CSV - (8760, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8760/8760 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE_01-01-2007_A_31-12-2007.CSV - (8760, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8760/8760 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE_01-01-2003_A_31-12-2003.CSV - (8760, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8760/8760 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE_01-01-2013_A_31-12-2013.CSV - (8760, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8760/8760 registros\n",
            "‚úÖ Carregado: INMET_S_RS_B807_PORTO ALEGRE- BELEM NOVO_01-01-2025_A_30-04-2025.CSV - (2880, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 2880/2880 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE_01-01-2017_A_31-12-2017.CSV - (8760, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8760/8760 registros\n",
            "‚úÖ Carregado: INMET_S_RS_A801_PORTO ALEGRE - JARDIM BOTANICO_01-01-2023_A_31-12-2023.CSV - (8760, 20)\n",
            "‚úÖ Data processada - per√≠odo v√°lido: 8760/8760 registros\n",
            "üìä Processados 15 arquivos com sucesso\n",
            "üîÑ Removidas 78846 duplicatas\n",
            "‚úÖ Salvo: /Users/gabrielschuler/Developer/Fiap/Challenge/data/processed/dados_inmet_processados.csv\n",
            "üìä Shape final: (40866, 9)\n",
            "üìã Colunas finais: ['datetime', 'temperatura', 'precipitacao', 'umidade', 'pressao', 'vento_velocidade', 'vento_direcao', 'ano_arquivo', 'fonte_arquivo']\n",
            "üóìÔ∏è Per√≠odo: 2002-01-01 00:00:00 at√© 2025-04-30 23:00:00\n",
            "üìÖ Registros com data v√°lida: 40866/40866\n",
            "\n",
            "üìà Qualidade dos dados:\n",
            "  ‚Ä¢ temperatura: 40592/40866 (99.3%) valores v√°lidos\n",
            "  ‚Ä¢ precipitacao: 40589/40866 (99.3%) valores v√°lidos\n",
            "  ‚Ä¢ umidade: 40591/40866 (99.3%) valores v√°lidos\n",
            "  ‚Ä¢ pressao: 40591/40866 (99.3%) valores v√°lidos\n",
            "  ‚Ä¢ vento_velocidade: 40060/40866 (98.0%) valores v√°lidos\n",
            "  ‚Ä¢ vento_direcao: 40061/40866 (98.0%) valores v√°lidos\n",
            "\n",
            "üßÆ Criando features atmosf√©ricas...\n",
            "üå°Ô∏è Colunas de temperatura encontradas: ['temperature_850hPa', 'temperature_500hPa', 'temperature_700hPa', 'temperature_1000hPa']\n",
            "‚úÖ Gradiente t√©rmico 850-500hPa criado\n",
            "‚úÖ Features temporais criadas\n",
            "‚úÖ Features salvas: /Users/gabrielschuler/Developer/Fiap/Challenge/data/processed/atmospheric_features_149vars.csv\n",
            "üìä Total de features: 54\n",
            "\n",
            "üåç Criando features de superf√≠cie...\n",
            "‚úÖ Features temporais de superf√≠cie criadas\n",
            "‚úÖ √çndice de calor criado\n",
            "‚úÖ Features de superf√≠cie salvas: /Users/gabrielschuler/Developer/Fiap/Challenge/data/processed/surface_features_25vars.csv\n",
            "üìä Total de features: 27\n",
            "\n",
            "üìã Criando relat√≥rio resumo...\n",
            "‚úÖ Relat√≥rio salvo: /Users/gabrielschuler/Developer/Fiap/Challenge/data/analysis/processing_report.json\n",
            "\n",
            "üéØ PROCESSAMENTO CONCLU√çDO!\n",
            "==============================\n",
            "‚úÖ Dados processados e salvos em data/processed/\n",
            "üìä Total de features dispon√≠veis: 49\n",
            "üìã Relat√≥rio: data/analysis/processing_report.json\n",
            "\n",
            "üìà Fontes de dados processadas: 3\n",
            "   ‚Ä¢ openmeteo_forecast: 10,920 registros, 49 features\n",
            "   ‚Ä¢ openmeteo_weather: 87,648 registros, 22 features\n",
            "   ‚Ä¢ inmet: 40,866 registros, 9 features\n",
            "\n",
            "üîó Pr√≥ximos passos:\n",
            "   1. Execute: notebooks/1_exploratory_analysis.ipynb\n",
            "   2. Execute: notebooks/2_model_training.ipynb\n",
            "\n",
            "\n",
            "üìã VERIFICANDO DADOS PROCESSADOS\n",
            "========================================\n",
            "‚úÖ openmeteo_historical_forecast.csv: (10920, 49) - 2.57MB\n",
            "‚úÖ openmeteo_historical_weather.csv: (87648, 22) - 10.42MB\n",
            "‚úÖ dados_inmet_processados.csv: (40866, 9) - 4.68MB\n",
            "‚úÖ atmospheric_features_149vars.csv: (10920, 54) - 2.76MB\n",
            "‚úÖ surface_features_25vars.csv: (87648, 27) - 12.21MB\n",
            "\n",
            "üéØ PIPELINE CONCLU√çDO!\n",
            "==============================\n",
            "‚úÖ 5/5 arquivos processados\n",
            "üìä Dados prontos para an√°lise e treinamento\n",
            "\n",
            "üîó Pr√≥ximos passos:\n",
            "   1. ‚úÖ Execute: notebooks/1_exploratory_analysis.ipynb\n",
            "   2. ‚úÖ Execute: notebooks/2_model_training.ipynb\n",
            "   3. ‚úÖ Execute: notebooks/3_model_evaluation.ipynb\n",
            "\n",
            "üìù Log completo salvo em: logs/\n",
            "üìä Para an√°lise manual, verifique: data/processed/\n"
          ]
        }
      ],
      "source": [
        "# Pipeline de processamento simplificado dos dados existentes\n",
        "import subprocess\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "def check_data_exists():\n",
        "    \"\"\"Verifica se os dados brutos existem\"\"\"\n",
        "    print(\"üìä VERIFICANDO DADOS EXISTENTES\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    data_paths = {\n",
        "        \"Open-Meteo Historical Forecast\": \"../data/raw/Open-Meteo Historical Forecast/\",\n",
        "        \"Open-Meteo Historical Weather\": \"../data/raw/Open-Meteo Historical Weather/\", \n",
        "        \"INMET\": \"../data/raw/INMET/\"\n",
        "    }\n",
        "    \n",
        "    data_summary = {}\n",
        "    for name, path in data_paths.items():\n",
        "        if Path(path).exists():\n",
        "            # Contar todos os arquivos (CSV e JSON)\n",
        "            csv_files = list(Path(path).glob(\"*.csv\"))\n",
        "            json_files = list(Path(path).glob(\"*.json\"))\n",
        "            total_files = len(csv_files) + len(json_files)\n",
        "            \n",
        "            data_summary[name] = {\n",
        "                \"exists\": True,\n",
        "                \"files\": total_files,\n",
        "                \"csv_files\": len(csv_files),\n",
        "                \"json_files\": len(json_files),\n",
        "                \"path\": path\n",
        "            }\n",
        "            print(f\"‚úÖ {name}: {total_files} arquivo(s) ({len(csv_files)} CSV, {len(json_files)} JSON)\")\n",
        "        else:\n",
        "            data_summary[name] = {\n",
        "                \"exists\": False,\n",
        "                \"files\": 0,\n",
        "                \"path\": path\n",
        "            }\n",
        "            print(f\"‚ùå {name}: N√£o encontrado\")\n",
        "    \n",
        "    return data_summary\n",
        "\n",
        "def run_processing_script():\n",
        "    \"\"\"Executa o script de processamento personalizado\"\"\"\n",
        "    print(\"\\nüîÑ Executando processamento dos dados existentes\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    script_path = Path(\"../scripts/process_existing_data.py\")\n",
        "    \n",
        "    try:\n",
        "        result = subprocess.run([sys.executable, str(script_path)], \n",
        "                              capture_output=True, text=True, timeout=600)\n",
        "        \n",
        "        if result.returncode == 0:\n",
        "            print(\"‚úÖ Processamento - SUCESSO\")\n",
        "            if result.stdout:\n",
        "                print(\"üì§ Output:\")\n",
        "                print(result.stdout)\n",
        "        else:\n",
        "            print(\"‚ùå Processamento - ERRO\")\n",
        "            print(\"üí• Error output:\")\n",
        "            print(result.stderr)\n",
        "            \n",
        "        return result.returncode == 0\n",
        "            \n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"‚è∞ Processamento - TIMEOUT (>10min)\")\n",
        "        return False\n",
        "    except FileNotFoundError:\n",
        "        print(f\"üìÅ Script n√£o encontrado: {script_path}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"üí• Erro executando processamento: {e}\")\n",
        "        return False\n",
        "\n",
        "def check_processed_data():\n",
        "    \"\"\"Verifica se os dados foram processados com sucesso\"\"\"\n",
        "    print(\"\\nüìã VERIFICANDO DADOS PROCESSADOS\")\n",
        "    print(\"=\" * 40)\n",
        "    \n",
        "    expected_files = [\n",
        "        \"../data/processed/openmeteo_historical_forecast.csv\",\n",
        "        \"../data/processed/openmeteo_historical_weather.csv\", \n",
        "        \"../data/processed/dados_inmet_processados.csv\",\n",
        "        \"../data/processed/atmospheric_features_149vars.csv\",\n",
        "        \"../data/processed/surface_features_25vars.csv\"\n",
        "    ]\n",
        "    \n",
        "    processed_summary = {}\n",
        "    \n",
        "    for file_path in expected_files:\n",
        "        path = Path(file_path)\n",
        "        if path.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(path)\n",
        "                processed_summary[path.name] = {\n",
        "                    \"exists\": True,\n",
        "                    \"shape\": df.shape,\n",
        "                    \"size_mb\": round(path.stat().st_size / 1024 / 1024, 2)\n",
        "                }\n",
        "                print(f\"‚úÖ {path.name}: {df.shape} - {processed_summary[path.name]['size_mb']}MB\")\n",
        "            except Exception as e:\n",
        "                processed_summary[path.name] = {\n",
        "                    \"exists\": True,\n",
        "                    \"error\": str(e)\n",
        "                }\n",
        "                print(f\"‚ö†Ô∏è {path.name}: Erro ao ler - {e}\")\n",
        "        else:\n",
        "            processed_summary[path.name] = {\"exists\": False}\n",
        "            print(f\"‚ùå {path.name}: N√£o encontrado\")\n",
        "    \n",
        "    return processed_summary\n",
        "\n",
        "print(\"üöÄ INICIANDO PIPELINE DE PROCESSAMENTO DE DADOS\")\n",
        "print(f\"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "# 1. Verificar dados brutos\n",
        "data_summary = check_data_exists()\n",
        "data_available = any(ds[\"exists\"] for ds in data_summary.values())\n",
        "\n",
        "if data_available:\n",
        "    print(f\"\\n‚úÖ Dados brutos encontrados!\")\n",
        "    \n",
        "    # 2. Executar processamento\n",
        "    processing_success = run_processing_script()\n",
        "    \n",
        "    if processing_success:\n",
        "        # 3. Verificar resultados\n",
        "        processed_summary = check_processed_data()\n",
        "        \n",
        "        # 4. Relat√≥rio final\n",
        "        processed_files = sum(1 for ps in processed_summary.values() if ps.get(\"exists\", False))\n",
        "        \n",
        "        print(f\"\\nüéØ PIPELINE CONCLU√çDO!\")\n",
        "        print(\"=\" * 30)\n",
        "        print(f\"‚úÖ {processed_files}/{len(processed_summary)} arquivos processados\")\n",
        "        print(f\"üìä Dados prontos para an√°lise e treinamento\")\n",
        "        \n",
        "        if processed_files >= 2:  # Pelo menos dados b√°sicos\n",
        "            print(\"\\nüîó Pr√≥ximos passos:\")\n",
        "            print(\"   1. ‚úÖ Execute: notebooks/1_exploratory_analysis.ipynb\")\n",
        "            print(\"   2. ‚úÖ Execute: notebooks/2_model_training.ipynb\") \n",
        "            print(\"   3. ‚úÖ Execute: notebooks/3_model_evaluation.ipynb\")\n",
        "        else:\n",
        "            print(\"\\n‚ö†Ô∏è Dados insuficientes para treinamento completo\")\n",
        "            print(\"   Mas voc√™ pode executar a an√°lise explorat√≥ria\")\n",
        "    else:\n",
        "        print(\"\\n‚ùå Falha no processamento dos dados\")\n",
        "        print(\"üìã Verifique os logs acima para detalhes\")\n",
        "        \n",
        "else:\n",
        "    print(\"\\n‚ùå Dados brutos n√£o encontrados!\")\n",
        "    print(\"üìÅ Certifique-se de que existem dados em:\")\n",
        "    for name, ds in data_summary.items():\n",
        "        if not ds[\"exists\"]:\n",
        "            print(f\"   - {ds['path']}\")\n",
        "\n",
        "print(f\"\\nüìù Log completo salvo em: logs/\")\n",
        "print(f\"üìä Para an√°lise manual, verifique: data/processed/\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
