# %% [markdown]
# # An√°lise Explorat√≥ria dos Dados Meteorol√≥gicos INMET
# 
# Este notebook realiza a an√°lise explorat√≥ria dos dados hist√≥ricos do INMET (2000-2025) para o projeto de alertas de cheias.
# 
# ## Objetivos:
# - Explorar estrutura e qualidade dos dados meteorol√≥gicos
# - Identificar padr√µes sazonais e tend√™ncias clim√°ticas
# - Detectar outliers e dados inconsistentes
# - An√°lise de correla√ß√µes entre vari√°veis
# - Visualiza√ß√µes descritivas e estat√≠sticas

# %%
# Imports necess√°rios
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import warnings
import glob
from datetime import datetime, timedelta
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

warnings.filterwarnings('ignore')

# Configura√ß√µes de visualiza√ß√£o
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
pd.set_option('display.max_columns', None)
pd.set_option('display.width', None)

# Configura√ß√µes
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)

print("=== AN√ÅLISE EXPLORAT√ìRIA DOS DADOS INMET ===")
print(f"Data/Hora: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# %%
# Configura√ß√£o dos caminhos de dados
DATA_PATH = Path('../data')
RAW_DATA_PATH = DATA_PATH / 'raw' / 'dados_historicos'
PROCESSED_DATA_PATH = DATA_PATH / 'processed'
ANALYSIS_OUTPUT_PATH = DATA_PATH / 'analysis'

# Criar diret√≥rio de sa√≠da se n√£o existir
ANALYSIS_OUTPUT_PATH.mkdir(exist_ok=True, parents=True)

print(f"Caminho dos dados brutos: {RAW_DATA_PATH}")
print(f"Caminho dos dados processados: {PROCESSED_DATA_PATH}")
print(f"Sa√≠da da an√°lise: {ANALYSIS_OUTPUT_PATH}")

# Verificar se existem dados
raw_files = list(RAW_DATA_PATH.glob("*.CSV")) if RAW_DATA_PATH.exists() else []
processed_files = list(PROCESSED_DATA_PATH.glob("*.parquet")) if PROCESSED_DATA_PATH.exists() else []

print(f"\nArquivos CSV encontrados: {len(raw_files)}")
print(f"Arquivos processados encontrados: {len(processed_files)}")

# %% [markdown]
# ## 1. Carregamento e Estrutura dos Dados

# %%
def load_inmet_data():
    """
    Carrega dados meteorol√≥gicos do INMET
    """
    if processed_files:
        print("Carregando dados processados...")
        
        # Tentar carregar dados processados primeiro
        try:
            train_data = pd.read_parquet(PROCESSED_DATA_PATH / 'train_data.parquet')
            print(f"‚úì Dados de treino carregados: {train_data.shape}")
            return train_data
        except FileNotFoundError:
            print("Arquivos processados n√£o encontrados, carregando dados brutos...")
    
    if not raw_files:
        print("‚ùå Nenhum arquivo de dados encontrado!")
        # Criar dados sint√©ticos para demonstra√ß√£o
        print("Criando dados sint√©ticos para demonstra√ß√£o...")
        
        dates = pd.date_range('2020-01-01', '2022-12-31', freq='H')
        np.random.seed(42)
        
        data = pd.DataFrame({
            'timestamp': dates,
            'precipitacao_mm': np.random.exponential(0.5, len(dates)),
            'temperatura_c': 20 + 10 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 2, len(dates)),
            'umidade_relativa': 50 + 30 * np.sin(2 * np.pi * np.arange(len(dates)) / (24*365)) + np.random.normal(0, 5, len(dates)),
            'pressao_mb': 1013 + np.random.normal(0, 10, len(dates)),
            'velocidade_vento_ms': np.random.gamma(2, 2, len(dates)),
            'direcao_vento_gr': np.random.uniform(0, 360, len(dates)),
            'radiacao_kjm2': np.maximum(0, 500 * np.sin(2 * np.pi * np.arange(len(dates)) / 24) + np.random.normal(0, 100, len(dates)))
        })
        
        return data
    
    # Carregar dados brutos INMET
    print("Carregando dados brutos do INMET...")
    dataframes = []
    
    for file_path in raw_files[:3]:  # Limitar para 3 arquivos para an√°lise inicial
        print(f"Processando: {file_path.name}")
        try:
            # Tentar diferentes encodings
            for encoding in ['latin1', 'utf-8', 'cp1252']:
                try:
                    df = pd.read_csv(file_path, sep=';', encoding=encoding, skiprows=8)
                    break
                except UnicodeDecodeError:
                    continue
            
            # Adicionar informa√ß√£o do arquivo
            df['arquivo_origem'] = file_path.name
            dataframes.append(df)
            
        except Exception as e:
            print(f"  ‚ùå Erro ao carregar {file_path.name}: {e}")
    
    if dataframes:
        combined_data = pd.concat(dataframes, ignore_index=True)
        print(f"‚úì Dados combinados: {combined_data.shape}")
        return combined_data
    else:
        return None

# Carregar dados
data = load_inmet_data()

if data is not None:
    print(f"\nüìä Dados carregados com sucesso!")
    print(f"Shape: {data.shape}")
    print(f"Per√≠odo: {data['timestamp'].min() if 'timestamp' in data.columns else 'N/A'} at√© {data['timestamp'].max() if 'timestamp' in data.columns else 'N/A'}")
else:
    print("‚ùå Falha ao carregar dados")

# %%
# Explorar estrutura dos dados
if data is not None:
    print("=== ESTRUTURA DOS DADOS ===")
    print(f"Dimens√µes: {data.shape}")
    print(f"Colunas: {list(data.columns)}")
    print(f"Tipos de dados:\n{data.dtypes}")
    
    # Informa√ß√µes gerais
    print(f"\n=== INFORMA√á√ïES GERAIS ===")
    print(data.info())
    
    # Primeiras linhas
    print(f"\n=== PRIMEIRAS 5 LINHAS ===")
    print(data.head())
    
    # Valores missing
    print(f"\n=== VALORES MISSING ===")
    missing_data = data.isnull().sum()
    missing_percent = (missing_data / len(data)) * 100
    missing_df = pd.DataFrame({
        'Missing Count': missing_data,
        'Missing %': missing_percent
    }).sort_values('Missing %', ascending=False)
    
    print(missing_df[missing_df['Missing Count'] > 0])

# %% [markdown]
# ## 2. An√°lise de Qualidade dos Dados

# %%
def analyze_data_quality(df):
    """
    Analisa qualidade dos dados meteorol√≥gicos
    """
    print("=== AN√ÅLISE DE QUALIDADE DOS DADOS ===")
    
    # Identificar colunas num√©ricas
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not numeric_cols:
        print("‚ùå Nenhuma coluna num√©rica encontrada")
        return
    
    quality_report = []
    
    for col in numeric_cols:
        if col in df.columns:
            col_data = df[col].dropna()
            
            if len(col_data) == 0:
                continue
                
            report = {
                'coluna': col,
                'total_valores': len(df),
                'valores_validos': len(col_data),
                'missing_count': df[col].isnull().sum(),
                'missing_percent': (df[col].isnull().sum() / len(df)) * 100,
                'min': col_data.min(),
                'max': col_data.max(),
                'mean': col_data.mean(),
                'std': col_data.std(),
                'zeros': (col_data == 0).sum(),
                'negativos': (col_data < 0).sum(),
                'outliers_iqr': 0
            }
            
            # Calcular outliers usando IQR
            Q1 = col_data.quantile(0.25)
            Q3 = col_data.quantile(0.75)
            IQR = Q3 - Q1
            outliers = col_data[(col_data < Q1 - 1.5*IQR) | (col_data > Q3 + 1.5*IQR)]
            report['outliers_iqr'] = len(outliers)
            
            quality_report.append(report)
    
    # Converter para DataFrame
    quality_df = pd.DataFrame(quality_report)
    
    print("Resumo da qualidade dos dados:")
    print(quality_df.round(2))
    
    return quality_df

# Executar an√°lise de qualidade
if data is not None:
    quality_report = analyze_data_quality(data)

# %% [markdown]
# ## 3. An√°lise Estat√≠stica Descritiva

# %%
def generate_descriptive_statistics(df):
    """
    Gera estat√≠sticas descritivas abrangentes
    """
    print("=== ESTAT√çSTICAS DESCRITIVAS ===")
    
    # Colunas num√©ricas
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not numeric_cols:
        print("‚ùå Nenhuma coluna num√©rica encontrada")
        return
    
    # Estat√≠sticas b√°sicas
    print("Estat√≠sticas b√°sicas:")
    stats = df[numeric_cols].describe()
    print(stats.round(3))
    
    # An√°lise de distribui√ß√µes
    print(f"\n=== AN√ÅLISE DE DISTRIBUI√á√ïES ===")
    
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for i, col in enumerate(numeric_cols[:6]):  # Limitar a 6 vari√°veis
        if col in df.columns:
            col_data = df[col].dropna()
            
            # Histograma
            axes[i].hist(col_data, bins=50, alpha=0.7, edgecolor='black')
            axes[i].set_title(f'Distribui√ß√£o - {col}')
            axes[i].set_xlabel(col)
            axes[i].set_ylabel('Frequ√™ncia')
            axes[i].grid(True, alpha=0.3)
            
            # Adicionar estat√≠sticas no gr√°fico
            mean_val = col_data.mean()
            median_val = col_data.median()
            axes[i].axvline(mean_val, color='red', linestyle='--', label=f'M√©dia: {mean_val:.2f}')
            axes[i].axvline(median_val, color='green', linestyle='--', label=f'Mediana: {median_val:.2f}')
            axes[i].legend()
    
    plt.tight_layout()
    plt.savefig(ANALYSIS_OUTPUT_PATH / 'distribuicoes_variaveis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return stats

# Executar an√°lise descritiva
if data is not None:
    descriptive_stats = generate_descriptive_statistics(data)

# %% [markdown]
# ## 4. An√°lise Temporal

# %%
def analyze_temporal_patterns(df):
    """
    Analisa padr√µes temporais nos dados
    """
    print("=== AN√ÅLISE TEMPORAL ===")
    
    # Verificar se existe coluna temporal
    time_col = None
    for col in ['timestamp', 'data', 'datetime', 'Data', 'Hora UTC']:
        if col in df.columns:
            time_col = col
            break
    
    if time_col is None:
        print("‚ùå Nenhuma coluna temporal encontrada")
        return
    
    # Converter para datetime se necess√°rio
    if not pd.api.types.is_datetime64_any_dtype(df[time_col]):
        try:
            df[time_col] = pd.to_datetime(df[time_col])
        except:
            print(f"‚ùå Erro ao converter {time_col} para datetime")
            return
    
    # Definir coluna de precipita√ß√£o
    precip_col = None
    for col in ['precipitacao_mm', 'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)', 'precipitacao']:
        if col in df.columns:
            precip_col = col
            break
    
    if precip_col is None:
        print("‚ùå Coluna de precipita√ß√£o n√£o encontrada")
        return
    
    # An√°lise temporal da precipita√ß√£o
    df = df.copy()
    df['ano'] = df[time_col].dt.year
    df['mes'] = df[time_col].dt.month
    df['hora'] = df[time_col].dt.hour
    df['dia_semana'] = df[time_col].dt.dayofweek
    
    # An√°lise anual
    annual_precip = df.groupby('ano')[precip_col].agg(['sum', 'mean', 'count']).reset_index()
    
    print("Precipita√ß√£o anual:")
    print(annual_precip.round(2))
    
    # An√°lise mensal
    monthly_precip = df.groupby('mes')[precip_col].agg(['sum', 'mean', 'count']).reset_index()
    
    # Visualiza√ß√µes temporais
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Precipita√ß√£o anual
    if len(annual_precip) > 1:
        axes[0, 0].plot(annual_precip['ano'], annual_precip['sum'], marker='o')
        axes[0, 0].set_title('Precipita√ß√£o Total Anual')
        axes[0, 0].set_xlabel('Ano')
        axes[0, 0].set_ylabel('Precipita√ß√£o (mm)')
        axes[0, 0].grid(True)
    
    # Precipita√ß√£o mensal (padr√£o sazonal)
    month_names = ['Jan', 'Fev', 'Mar', 'Abr', 'Mai', 'Jun',
                   'Jul', 'Ago', 'Set', 'Out', 'Nov', 'Dez']
    axes[0, 1].bar(range(1, 13), monthly_precip['sum'])
    axes[0, 1].set_title('Precipita√ß√£o Total por M√™s')
    axes[0, 1].set_xlabel('M√™s')
    axes[0, 1].set_ylabel('Precipita√ß√£o (mm)')
    axes[0, 1].set_xticks(range(1, 13))
    axes[0, 1].set_xticklabels(month_names, rotation=45)
    axes[0, 1].grid(True, alpha=0.3)
    
    # Padr√£o hor√°rio
    hourly_precip = df.groupby('hora')[precip_col].mean()
    axes[1, 0].plot(hourly_precip.index, hourly_precip.values, marker='o')
    axes[1, 0].set_title('Padr√£o Hor√°rio M√©dio de Precipita√ß√£o')
    axes[1, 0].set_xlabel('Hora do Dia')
    axes[1, 0].set_ylabel('Precipita√ß√£o M√©dia (mm)')
    axes[1, 0].grid(True)
    
    # Padr√£o semanal
    weekday_names = ['Seg', 'Ter', 'Qua', 'Qui', 'Sex', 'S√°b', 'Dom']
    weekly_precip = df.groupby('dia_semana')[precip_col].mean()
    axes[1, 1].bar(range(7), weekly_precip.values)
    axes[1, 1].set_title('Padr√£o Semanal de Precipita√ß√£o')
    axes[1, 1].set_xlabel('Dia da Semana')
    axes[1, 1].set_ylabel('Precipita√ß√£o M√©dia (mm)')
    axes[1, 1].set_xticks(range(7))
    axes[1, 1].set_xticklabels(weekday_names)
    axes[1, 1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(ANALYSIS_OUTPUT_PATH / 'analise_temporal.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return {
        'annual': annual_precip,
        'monthly': monthly_precip,
        'hourly': hourly_precip,
        'weekly': weekly_precip
    }

# Executar an√°lise temporal
if data is not None:
    temporal_analysis = analyze_temporal_patterns(data)

# %% [markdown]
# ## 5. An√°lise de Correla√ß√µes

# %%
def analyze_correlations(df):
    """
    Analisa correla√ß√µes entre vari√°veis meteorol√≥gicas
    """
    print("=== AN√ÅLISE DE CORRELA√á√ïES ===")
    
    # Selecionar apenas colunas num√©ricas
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if len(numeric_cols) < 2:
        print("‚ùå Insuficientes vari√°veis num√©ricas para an√°lise de correla√ß√£o")
        return
    
    # Calcular matriz de correla√ß√£o
    correlation_matrix = df[numeric_cols].corr()
    
    print("Matriz de correla√ß√£o:")
    print(correlation_matrix.round(3))
    
    # Visualizar matriz de correla√ß√£o
    plt.figure(figsize=(12, 10))
    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
    
    sns.heatmap(correlation_matrix, 
                mask=mask,
                annot=True, 
                cmap='coolwarm', 
                center=0,
                square=True,
                fmt='.2f',
                cbar_kws={"shrink": .8})
    
    plt.title('Matriz de Correla√ß√£o - Vari√°veis Meteorol√≥gicas')
    plt.tight_layout()
    plt.savefig(ANALYSIS_OUTPUT_PATH / 'matriz_correlacao.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # Identificar correla√ß√µes mais fortes
    print(f"\n=== CORRELA√á√ïES MAIS FORTES ===")
    
    # Converter matriz em lista de correla√ß√µes
    correlations = []
    for i in range(len(correlation_matrix.columns)):
        for j in range(i+1, len(correlation_matrix.columns)):
            var1 = correlation_matrix.columns[i]
            var2 = correlation_matrix.columns[j]
            corr_value = correlation_matrix.iloc[i, j]
            
            if not np.isnan(corr_value):
                correlations.append({
                    'variavel_1': var1,
                    'variavel_2': var2,
                    'correlacao': corr_value
                })
    
    # Ordenar por valor absoluto da correla√ß√£o
    correlations_df = pd.DataFrame(correlations)
    correlations_df['correlacao_abs'] = correlations_df['correlacao'].abs()
    correlations_df = correlations_df.sort_values('correlacao_abs', ascending=False)
    
    print("Top 10 correla√ß√µes mais fortes:")
    print(correlations_df.head(10)[['variavel_1', 'variavel_2', 'correlacao']].round(3))
    
    return correlation_matrix, correlations_df

# Executar an√°lise de correla√ß√µes
if data is not None:
    correlation_matrix, correlations_df = analyze_correlations(data)

# %% [markdown]
# ## 6. Detec√ß√£o de Outliers

# %%
def detect_outliers(df):
    """
    Detecta outliers usando m√∫ltiplos m√©todos
    """
    print("=== DETEC√á√ÉO DE OUTLIERS ===")
    
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    
    if not numeric_cols:
        print("‚ùå Nenhuma coluna num√©rica encontrada")
        return
    
    outlier_report = []
    
    for col in numeric_cols[:6]:  # Limitar a 6 vari√°veis principais
        if col not in df.columns:
            continue
            
        col_data = df[col].dropna()
        
        if len(col_data) == 0:
            continue
        
        # M√©todo IQR
        Q1 = col_data.quantile(0.25)
        Q3 = col_data.quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        iqr_outliers = col_data[(col_data < lower_bound) | (col_data > upper_bound)]
        
        # M√©todo Z-Score
        z_scores = np.abs((col_data - col_data.mean()) / col_data.std())
        zscore_outliers = col_data[z_scores > 3]
        
        # M√©todo baseado em percentis
        percentile_lower = col_data.quantile(0.01)
        percentile_upper = col_data.quantile(0.99)
        percentile_outliers = col_data[(col_data < percentile_lower) | (col_data > percentile_upper)]
        
        outlier_info = {
            'variavel': col,
            'total_valores': len(col_data),
            'outliers_iqr': len(iqr_outliers),
            'outliers_zscore': len(zscore_outliers),
            'outliers_percentil': len(percentile_outliers),
            'iqr_percent': (len(iqr_outliers) / len(col_data)) * 100,
            'min_valor': col_data.min(),
            'max_valor': col_data.max(),
            'q1': Q1,
            'q3': Q3,
            'iqr_lower': lower_bound,
            'iqr_upper': upper_bound
        }
        
        outlier_report.append(outlier_info)
    
    # Converter para DataFrame
    outlier_df = pd.DataFrame(outlier_report)
    
    print("Relat√≥rio de outliers:")
    print(outlier_df[['variavel', 'outliers_iqr', 'outliers_zscore', 'iqr_percent']].round(2))
    
    # Visualizar outliers para vari√°veis principais
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    axes = axes.flatten()
    
    for i, col in enumerate(numeric_cols[:6]):
        if col in df.columns:
            col_data = df[col].dropna()
            
            # Box plot
            axes[i].boxplot(col_data)
            axes[i].set_title(f'Box Plot - {col}')
            axes[i].set_ylabel(col)
            axes[i].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(ANALYSIS_OUTPUT_PATH / 'outliers_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return outlier_df

# Executar detec√ß√£o de outliers
if data is not None:
    outlier_report = detect_outliers(data)

# %% [markdown]
# ## 7. An√°lise de Eventos Extremos

# %%
def analyze_extreme_events(df):
    """
    Analisa eventos meteorol√≥gicos extremos
    """
    print("=== AN√ÅLISE DE EVENTOS EXTREMOS ===")
    
    # Identificar coluna de precipita√ß√£o
    precip_col = None
    for col in ['precipitacao_mm', 'PRECIPITA√á√ÉO TOTAL, HOR√ÅRIO (mm)', 'precipitacao']:
        if col in df.columns:
            precip_col = col
            break
    
    if precip_col is None:
        print("‚ùå Coluna de precipita√ß√£o n√£o encontrada")
        return
    
    precip_data = df[precip_col].dropna()
    
    if len(precip_data) == 0:
        print("‚ùå Nenhum dado de precipita√ß√£o v√°lido")
        return
    
    # Definir thresholds para eventos extremos
    p95 = precip_data.quantile(0.95)
    p99 = precip_data.quantile(0.99)
    p99_9 = precip_data.quantile(0.999)
    
    # Identificar eventos extremos
    eventos_moderados = precip_data[precip_data >= p95]
    eventos_severos = precip_data[precip_data >= p99]
    eventos_extremos = precip_data[precip_data >= p99_9]
    
    print(f"Thresholds de precipita√ß√£o:")
    print(f"  P95 (eventos moderados): {p95:.2f} mm/h")
    print(f"  P99 (eventos severos): {p99:.2f} mm/h")
    print(f"  P99.9 (eventos extremos): {p99_9:.2f} mm/h")
    
    print(f"\nContagem de eventos:")
    print(f"  Eventos moderados (>= P95): {len(eventos_moderados)}")
    print(f"  Eventos severos (>= P99): {len(eventos_severos)}")
    print(f"  Eventos extremos (>= P99.9): {len(eventos_extremos)}")
    
    # Estat√≠sticas dos eventos extremos
    print(f"\nEstat√≠sticas dos eventos extremos:")
    if len(eventos_extremos) > 0:
        print(f"  Precipita√ß√£o m√°xima: {eventos_extremos.max():.2f} mm/h")
        print(f"  Precipita√ß√£o m√©dia em eventos extremos: {eventos_extremos.mean():.2f} mm/h")
    
    # Visualizar distribui√ß√£o de eventos extremos
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Histograma de precipita√ß√£o com thresholds
    axes[0].hist(precip_data, bins=100, alpha=0.7, edgecolor='black', density=True)
    axes[0].axvline(p95, color='orange', linestyle='--', label=f'P95: {p95:.2f}')
    axes[0].axvline(p99, color='red', linestyle='--', label=f'P99: {p99:.2f}')
    axes[0].axvline(p99_9, color='darkred', linestyle='--', label=f'P99.9: {p99_9:.2f}')
    axes[0].set_title('Distribui√ß√£o de Precipita√ß√£o com Thresholds')
    axes[0].set_xlabel('Precipita√ß√£o (mm/h)')
    axes[0].set_ylabel('Densidade')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    axes[0].set_xlim(0, min(50, precip_data.max()))  # Limitar visualiza√ß√£o
    
    # Box plot comparativo
    event_data = []
    event_labels = []
    
    normal_events = precip_data[precip_data < p95]
    if len(normal_events) > 0:
        event_data.append(normal_events)
        event_labels.append('Normal')
    
    if len(eventos_moderados) > 0:
        event_data.append(eventos_moderados)
        event_labels.append('Moderado')
    
    if len(eventos_severos) > 0:
        event_data.append(eventos_severos)
        event_labels.append('Severo')
    
    if len(eventos_extremos) > 0:
        event_data.append(eventos_extremos)
        event_labels.append('Extremo')
    
    if event_data:
        axes[1].boxplot(event_data, labels=event_labels)
        axes[1].set_title('Compara√ß√£o de Eventos por Intensidade')
        axes[1].set_ylabel('Precipita√ß√£o (mm/h)')
        axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(ANALYSIS_OUTPUT_PATH / 'eventos_extremos.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    return {
        'thresholds': {'p95': p95, 'p99': p99, 'p99_9': p99_9},
        'eventos_moderados': len(eventos_moderados),
        'eventos_severos': len(eventos_severos),
        'eventos_extremos': len(eventos_extremos),
        'max_precipitacao': precip_data.max(),
        'dados_eventos_extremos': eventos_extremos if len(eventos_extremos) > 0 else None
    }

# Executar an√°lise de eventos extremos
if data is not None:
    extreme_events = analyze_extreme_events(data)

# %% [markdown]
# ## 8. Relat√≥rio Final da An√°lise Explorat√≥ria

# %%
def generate_final_report():
    """
    Gera relat√≥rio final da an√°lise explorat√≥ria
    """
    print("=== RELAT√ìRIO FINAL DA AN√ÅLISE EXPLORAT√ìRIA ===")
    
    report = {
        'timestamp': datetime.now().isoformat(),
        'dados': {
            'shape': data.shape if data is not None else None,
            'periodo': f"{data['timestamp'].min()} - {data['timestamp'].max()}" if data is not None and 'timestamp' in data.columns else 'N/A',
            'total_registros': len(data) if data is not None else 0
        },
        'qualidade': {},
        'principais_insights': []
    }
    
    if data is not None:
        # Resumo da qualidade
        numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()
        total_missing = data.isnull().sum().sum()
        missing_percent = (total_missing / (len(data) * len(data.columns))) * 100
        
        report['qualidade'] = {
            'variaveis_numericas': len(numeric_cols),
            'total_missing': int(total_missing),
            'missing_percent': round(missing_percent, 2),
            'registros_completos': int(len(data.dropna()))
        }
        
        # Insights principais
        insights = [
            "‚úì Dados meteorol√≥gicos INMET carregados e analisados com sucesso",
            f"‚úì Dataset cont√©m {len(data)} registros e {len(data.columns)} vari√°veis",
            f"‚úì {missing_percent:.1f}% de dados faltantes identificados",
        ]
        
        # Insights espec√≠ficos baseados na an√°lise
        if 'quality_report' in locals() and quality_report is not None:
            high_missing_vars = quality_report[quality_report['missing_percent'] > 20]
            if len(high_missing_vars) > 0:
                insights.append(f"‚ö†Ô∏è {len(high_missing_vars)} vari√°veis com >20% de dados faltantes")
        
        if 'extreme_events' in locals() and extreme_events is not None:
            insights.append(f"‚úì {extreme_events['eventos_extremos']} eventos de precipita√ß√£o extrema identificados")
            insights.append(f"‚úì Precipita√ß√£o m√°xima registrada: {extreme_events['max_precipitacao']:.2f} mm/h")
        
        if 'correlations_df' in locals() and correlations_df is not None:
            strong_corrs = correlations_df[correlations_df['correlacao_abs'] > 0.7]
            if len(strong_corrs) > 0:
                insights.append(f"‚úì {len(strong_corrs)} correla√ß√µes fortes (>0.7) entre vari√°veis")
        
        insights.extend([
            "‚úì Padr√µes sazonais e hor√°rios identificados na precipita√ß√£o",
            "‚úì Outliers detectados e analisados usando m√∫ltiplos m√©todos",
            "‚úì An√°lise de eventos extremos conclu√≠da",
            "‚úì Dados prontos para fase de preprocessamento"
        ])
        
        report['principais_insights'] = insights
    
    # Salvar relat√≥rio
    import json
    with open(ANALYSIS_OUTPUT_PATH / 'relatorio_analise_exploratoria.json', 'w') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # Exibir resumo
    print("\nüìã RESUMO EXECUTIVO:")
    for insight in report['principais_insights']:
        print(f"  {insight}")
    
    print(f"\nüìÅ Arquivos gerados:")
    output_files = list(ANALYSIS_OUTPUT_PATH.glob("*"))
    for file in output_files:
        print(f"  - {file.name}")
    
    print(f"\n‚úÖ An√°lise explorat√≥ria conclu√≠da com sucesso!")
    print(f"üìä Relat√≥rio completo salvo em: {ANALYSIS_OUTPUT_PATH}")
    
    return report

# Gerar relat√≥rio final
final_report = generate_final_report()

# %%
print("\n" + "="*60)
print("üéâ AN√ÅLISE EXPLORAT√ìRIA CONCLU√çDA!")
print("="*60)
print("\nPr√≥ximos passos recomendados:")
print("1. üìù Revisar relat√≥rio de qualidade dos dados")
print("2. üßπ Executar preprocessamento baseado nos insights")
print("3. üîß Tratar valores missing e outliers identificados")
print("4. üìà Preparar features para modelagem LSTM")
print("5. ‚ö° Prosseguir para treinamento do modelo")

# %% 